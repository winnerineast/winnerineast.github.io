{"singlePage": [], "startSite": "", "filingNum": "", "onePageListNum": 15, "commentLabelColor": "#006b75", "yearColorList": ["#bc4c00", "#0969da", "#1f883d", "#A333D0"], "i18n": "CN", "themeMode": "manual", "dayTheme": "light", "nightTheme": "dark", "urlMode": "pinyin", "script": "", "style": "", "head": "", "indexScript": "", "indexStyle": "", "bottomText": "", "showPostSource": 1, "iconList": {}, "UTC": 8, "rssSplit": "sentence", "exlink": {}, "needComment": 1, "allHead": "", "title": "Winnerineast Blog", "subTitle": "Humachine Studio", "avatarUrl": "https://github.githubassets.com/favicons/favicon.svg", "GMEEK_VERSION": "last", "postListJson": {"P1": {"htmlDir": "docs/post/\u3010-lun-wen-tui-jian-\u3011DeepResearch.html", "labels": ["paper"], "postTitle": "\u3010\u8bba\u6587\u63a8\u8350\u3011DeepResearch", "postUrl": "post/%E3%80%90-lun-wen-tui-jian-%E3%80%91DeepResearch.html", "postSourceUrl": "https://github.com/winnerineast/winnerineast.github.io/issues/1", "commentNum": 0, "wordCount": 901, "description": "\uff08\u597d\u4e00\u4e9b\u7684\u7efc\u8ff0\uff09[https://arxiv.org/abs/2506.18096](https://t.co/KqENoLuAiv) \n\uff08\u8fc7\u4e8e\u6c42\u5168\u53cd\u800c\u7f3a\u4e4f\u6d1e\u89c1\u7684\u7efc\u8ff0\uff09 [https://arxiv.org/pdf/2506.12594](https://t.co/xeWo94wXZq) \n\uff08\u8bc4\u4f30\u65b9\u6cd5\uff0c\u4ece\u62a5\u544a\u8d28\u91cf\u548c\u5f15\u7528\u7cbe\u5ea6\u4e24\u4e2a\u89d2\u5ea6\u8bc4\u4f30\uff09[https://arxiv.org/pdf/2506.11763](https://t.co/vKR0NO4HLO)\n\n\uff08\u63a8\u8350\uff0c\u5e26UI\uff0c\u5b57\u8282\u5f00\u6e90\u7684\u57fa\u4e8eLangGraph\u7684DeepFlow\uff09[https://github.com/bytedance/deer-flow\u2026](https://t.co/EtqKMqlKBf) \n\uff08\u63a8\u8350\uff0c\u5e26UI\uff0cGemini\u57fa\u4e8eLangGraph\u7684Deep Search\u5b9e\u73b0\uff0c\u6bd4\u8f83\u7b80\u5355\uff09 [https://github.com/google-gemini/gemini-fullstack-langgraph-quickstart\u2026](https://t.co/VTe064Fsx2) \n\uff08\u6ca1\u6709UI\uff0c\u4f46\u662f\u529f\u80fd\u6bd4\u8f83\u4e30\u5bcc\uff09[https://github.com/foreveryh/mentis/tree/main/super_agents/deep_research\u2026](https://t.co/9TJy0juGLG)\n[](https://t.co/VTe064Fsx2)\n\uff08\u57fa\u4e8esmolagents\u6846\u67b6\uff0c\u6bd4\u8f83\u7b80\u5355\uff09[https://github.com/huggingface/smolagents/tree/main/examples/open_deep_research\u2026](https://t.co/9wXwCsbxeL) \uff08\u57fa\u4e8eOpenAI Agents \u5e93\u5b9e\u73b0\uff0c\u6bd4\u8f83\u7b80\u5355\uff09 [https://huggingface.co/spaces/mallocode200/Deep_Research_Assistant/blob/main/research_manager.py](https://t.co/BaoHko5WT9)\u3002", "top": 0, "createdAt": 1752315239, "style": "", "script": "", "head": "", "ogImage": "https://github.githubassets.com/favicons/favicon.svg", "createdDate": "2025-07-12", "dateLabelColor": "#0969da"}, "P2": {"htmlDir": "docs/post/AI-zhi-hui-yu-jian-suo-de-dian-feng-rong-he-\uff1aRAG-jie-suo-xia-yi-dai-xin-xi-sheng-cheng-xin-jia-gou.html", "labels": ["AI"], "postTitle": "AI\u667a\u6167\u4e0e\u68c0\u7d22\u7684\u5dc5\u5cf0\u878d\u5408\uff1aRAG\u89e3\u9501\u4e0b\u4e00\u4ee3\u4fe1\u606f\u751f\u6210\u65b0\u67b6\u6784", "postUrl": "post/AI-zhi-hui-yu-jian-suo-de-dian-feng-rong-he-%EF%BC%9ARAG-jie-suo-xia-yi-dai-xin-xi-sheng-cheng-xin-jia-gou.html", "postSourceUrl": "https://github.com/winnerineast/winnerineast.github.io/issues/2", "commentNum": 0, "wordCount": 1894, "description": "**GPTDAOCN-e/acc** @GPTDAOCN [2024-12-11](https://x.com/GPTDAOCN/status/1866947666690838557)\n\nAI\u667a\u6167\u4e0e\u68c0\u7d22\u7684\u5dc5\u5cf0\u878d\u5408\uff1aRAG\u89e3\u9501\u4e0b\u4e00\u4ee3\u4fe1\u606f\u751f\u6210\u65b0\u67b6\u6784\n\n\u8fd9\u5f20\u56fe\u5c55\u793a\u4e86'\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08Retrieval-Augmented Generation\uff0cRAG\uff09'\u6280\u672f\u7684\u4e0d\u540c\u67b6\u6784\uff0c\u7528\u6765\u63cf\u8ff0\u5982\u4f55\u7ed3\u5408\u4fe1\u606f\u68c0\u7d22\u548c\u751f\u6210\u5f0fAI\u6a21\u578b\uff08\u5982GPT\uff09\u6765\u66f4\u597d\u5730\u56de\u7b54\u95ee\u9898\u6216\u751f\u6210\u5185\u5bb9\u3002", "top": 0, "createdAt": 1752321646, "style": "", "script": "", "head": "", "ogImage": "https://github.githubassets.com/favicons/favicon.svg", "createdDate": "2025-07-12", "dateLabelColor": "#0969da"}, "P3": {"htmlDir": "docs/post/Building effective agents.html", "labels": ["AI"], "postTitle": "Building effective agents", "postUrl": "post/Building%20effective%20agents.html", "postSourceUrl": "https://github.com/winnerineast/winnerineast.github.io/issues/3", "commentNum": 0, "wordCount": 18822, "description": "Over the past year, we've worked with dozens of teams building large language model (LLM) agents across industries. Consistently, the most successful implementations weren't using complex frameworks or specialized libraries. Instead, they were building with simple, composable patterns.\n\nIn this post, we share what we\u2019ve learned from working with our customers and building agents ourselves, and give practical advice for developers on building effective agents.\n\n## What are agents?\n\n'Agent' can be defined in several ways. Some customers define agents as fully autonomous systems that operate independently over extended periods, using various tools to accomplish complex tasks. Others use the term to describe more prescriptive implementations that follow predefined workflows. At Anthropic, we categorize all these variations as **agentic systems**, but draw an important architectural distinction between **workflows** and **agents**:\n\n- **Workflows** are systems where LLMs and tools are orchestrated through predefined code paths.\n- **Agents**, on the other hand, are systems where LLMs dynamically direct their own processes and tool usage, maintaining control over how they accomplish tasks.\n\nBelow, we will explore both types of agentic systems in detail. In Appendix 1 (\u201cAgents in Practice\u201d), we describe two domains where customers have found particular value in using these kinds of systems.\n\n## When (and when not) to use agents\n\nWhen building applications with LLMs, we recommend finding the simplest solution possible, and only increasing complexity when needed. This might mean not building agentic systems at all. Agentic systems often trade latency and cost for better task performance, and you should consider when this tradeoff makes sense.\n\nWhen more complexity is warranted, workflows offer predictability and consistency for well-defined tasks, whereas agents are the better option when flexibility and model-driven decision-making are needed at scale. For many applications, however, optimizing single LLM calls with retrieval and in-context examples is usually enough.\n\n## When and how to use frameworks\n\nThere are many frameworks that make agentic systems easier to implement, including:\n\n- [LangGraph](https://langchain-ai.github.io/langgraph/) from LangChain;\n- Amazon Bedrock's [AI Agent framework](https://aws.amazon.com/bedrock/agents/);\n- [Rivet](https://rivet.ironcladapp.com/), a drag and drop GUI LLM workflow builder; and\n- [Vellum](https://www.vellum.ai/), another GUI tool for building and testing complex workflows.\n\nThese frameworks make it easy to get started by simplifying standard low-level tasks like calling LLMs, defining and parsing tools, and chaining calls together. However, they often create extra layers of abstraction that can obscure the underlying prompts \u200b\u200band responses, making them harder to debug. They can also make it tempting to add complexity when a simpler setup would suffice.\n\nWe suggest that developers start by using LLM APIs directly: many patterns can be implemented in a few lines of code. If you do use a framework, ensure you understand the underlying code. Incorrect assumptions about what's under the hood are a common source of customer error.\n\nSee our [cookbook](https://github.com/anthropics/anthropic-cookbook/tree/main/patterns/agents) for some sample implementations.\n\n## Building blocks, workflows, and agents\n\nIn this section, we\u2019ll explore the common patterns for agentic systems we\u2019ve seen in production. We'll start with our foundational building block\u2014the augmented LLM\u2014and progressively increase complexity, from simple compositional workflows to autonomous agents.\n\n### Building block: The augmented LLM\n\nThe basic building block of agentic systems is an LLM enhanced with augmentations such as retrieval, tools, and memory. Our current models can actively use these capabilities\u2014generating their own search queries, selecting appropriate tools, and determining what information to retain.\n\nWe recommend focusing on two key aspects of the implementation: tailoring these capabilities to your specific use case and ensuring they provide an easy, well-documented interface for your LLM. While there are many ways to implement these augmentations, one approach is through our recently released [Model Context Protocol](https://www.anthropic.com/news/model-context-protocol), which allows developers to integrate with a growing ecosystem of third-party tools with a simple [client implementation](https://modelcontextprotocol.io/tutorials/building-a-client#building-mcp-clients).\n\nFor the remainder of this post, we'll assume each LLM call has access to these augmented capabilities.\n\n### Workflow: Prompt chaining\n\nPrompt chaining decomposes a task into a sequence of steps, where each LLM call processes the output of the previous one. You can add programmatic checks (see 'gate\u201d in the diagram below) on any intermediate steps to ensure that the process is still on track.\n\n**When to use this workflow:** This workflow is ideal for situations where the task can be easily and cleanly decomposed into fixed subtasks. The main goal is to trade off latency for higher accuracy, by making each LLM call an easier task.\n\n**Examples where prompt chaining is useful:**\n\n- Generating Marketing copy, then translating it into a different language.\n- Writing an outline of a document, checking that the outline meets certain criteria, then writing the document based on the outline.\n\n### Workflow: Routing\n\nRouting classifies an input and directs it to a specialized followup task. This workflow allows for separation of concerns, and building more specialized prompts. Without this workflow, optimizing for one kind of input can hurt performance on other inputs.\n\n**When to use this workflow:** Routing works well for complex tasks where there are distinct categories that are better handled separately, and where classification can be handled accurately, either by an LLM or a more traditional classification model/algorithm.\n\n**Examples where routing is useful:**\n\n- Directing different types of customer service queries (general questions, refund requests, technical support) into different downstream processes, prompts, and tools.\n- Routing easy/common questions to smaller models like Claude 3.5 Haiku and hard/unusual questions to more capable models like Claude 3.5 Sonnet to optimize cost and speed.\n\n### Workflow: Parallelization\n\nLLMs can sometimes work simultaneously on a task and have their outputs aggregated programmatically. This workflow, parallelization, manifests in two key variations:\n\n- **Sectioning**: Breaking a task into independent subtasks run in parallel.\n- **Voting:** Running the same task multiple times to get diverse outputs.\n\n**When to use this workflow:** Parallelization is effective when the divided subtasks can be parallelized for speed, or when multiple perspectives or attempts are needed for higher confidence results. For complex tasks with multiple considerations, LLMs generally perform better when each consideration is handled by a separate LLM call, allowing focused attention on each specific aspect.\n\n**Examples where parallelization is useful:**\n\n- **Sectioning**:\n- Implementing guardrails where one model instance processes user queries while another screens them for inappropriate content or requests. This tends to perform better than having the same LLM call handle both guardrails and the core response.\n- Automating evals for evaluating LLM performance, where each LLM call evaluates a different aspect of the model\u2019s performance on a given prompt.\n- **Voting**:\n- Reviewing a piece of code for vulnerabilities, where several different prompts review and flag the code if they find a problem.\n- Evaluating whether a given piece of content is inappropriate, with multiple prompts evaluating different aspects or requiring different vote thresholds to balance false positives and negatives.\n\n### Workflow: Orchestrator-workers\n\nIn the orchestrator-workers workflow, a central LLM dynamically breaks down tasks, delegates them to worker LLMs, and synthesizes their results.\n\n**When to use this workflow:** This workflow is well-suited for complex tasks where you can\u2019t predict the subtasks needed (in coding, for example, the number of files that need to be changed and the nature of the change in each file likely depend on the task). Whereas it\u2019s topographically similar, the key difference from parallelization is its flexibility\u2014subtasks aren't pre-defined, but determined by the orchestrator based on the specific input.\n\n**Example where orchestrator-workers is useful:**\n\n- Coding products that make complex changes to multiple files each time.\n- Search tasks that involve gathering and analyzing information from multiple sources for possible relevant information.\n\n### Workflow: Evaluator-optimizer\n\nIn the evaluator-optimizer workflow, one LLM call generates a response while another provides evaluation and feedback in a loop.\n\n**When to use this workflow:** This workflow is particularly effective when we have clear evaluation criteria, and when iterative refinement provides measurable value. The two signs of good fit are, first, that LLM responses can be demonstrably improved when a human articulates their feedback; and second, that the LLM can provide such feedback. This is analogous to the iterative writing process a human writer might go through when producing a polished document.\n\n**Examples where evaluator-optimizer is useful:**\n\n- Literary translation where there are nuances that the translator LLM might not capture initially, but where an evaluator LLM can provide useful critiques.\n- Complex search tasks that require multiple rounds of searching and analysis to gather comprehensive information, where the evaluator decides whether further searches are warranted.\n\n### Agents\n\nAgents are emerging in production as LLMs mature in key capabilities\u2014understanding complex inputs, engaging in reasoning and planning, using tools reliably, and recovering from errors. Agents begin their work with either a command from, or interactive discussion with, the human user. Once the task is clear, agents plan and operate independently, potentially returning to the human for further information or judgement. During execution, it's crucial for the agents to gain \u201cground truth\u201d from the environment at each step (such as tool call results or code execution) to assess its progress. Agents can then pause for human feedback at checkpoints or when encountering blockers. The task often terminates upon completion, but it\u2019s also common to include stopping conditions (such as a maximum number of iterations) to maintain control.\n\nAgents can handle sophisticated tasks, but their implementation is often straightforward. They are typically just LLMs using tools based on environmental feedback in a loop. It is therefore crucial to design toolsets and their documentation clearly and thoughtfully. We expand on best practices for tool development in Appendix 2 ('Prompt Engineering your Tools').\n\n**When to use agents:** Agents can be used for open-ended problems where it\u2019s difficult or impossible to predict the required number of steps, and where you can\u2019t hardcode a fixed path. The LLM will potentially operate for many turns, and you must have some level of trust in its decision-making. Agents' autonomy makes them ideal for scaling tasks in trusted environments.\n\nThe autonomous nature of agents means higher costs, and the potential for compounding errors. We recommend extensive testing in sandboxed environments, along with the appropriate guardrails.\n\n**Examples where agents are useful:**\n\nThe following examples are from our own implementations:\n\n- A coding Agent to resolve [SWE-bench tasks](https://www.anthropic.com/research/swe-bench-sonnet), which involve edits to many files based on a task description;\n- Our [\u201ccomputer use\u201d reference implementation](https://github.com/anthropics/anthropic-quickstarts/tree/main/computer-use-demo), where Claude uses a computer to accomplish tasks.\n\n## Combining and customizing these patterns\n\nThese building blocks aren't prescriptive. They're common patterns that developers can shape and combine to fit different use cases. The key to success, as with any LLM features, is measuring performance and iterating on implementations. To repeat: you should consider adding complexity *only* when it demonstrably improves outcomes.\n\n## Summary\n\nSuccess in the LLM space isn't about building the most sophisticated system. It's about building the *right* system for your needs. Start with simple prompts, optimize them with comprehensive evaluation, and add multi-step agentic systems only when simpler solutions fall short.\n\nWhen implementing agents, we try to follow three core principles:\n\n1. Maintain **simplicity** in your agent's design.\n2. Prioritize **transparency** by explicitly showing the agent\u2019s planning steps.\n3. Carefully craft your agent-computer interface (ACI) through thorough tool **documentation and testing**.\n\nFrameworks can help you get started quickly, but don't hesitate to reduce abstraction layers and build with basic components as you move to production. By following these principles, you can create agents that are not only powerful but also reliable, maintainable, and trusted by their users.\n\n### Acknowledgements\n\nWritten by Erik Schluntz and Barry Zhang. This work draws upon our experiences building agents at Anthropic and the valuable insights shared by our customers, for which we're deeply grateful.\n\n## Appendix 1: Agents in practice\n\nOur work with customers has revealed two particularly promising applications for AI agents that demonstrate the practical value of the patterns discussed above. Both applications illustrate how agents add the most value for tasks that require both conversation and action, have clear success criteria, enable feedback loops, and integrate meaningful human oversight.\n\n### A. Customer support\n\nCustomer support combines familiar chatbot interfaces with enhanced capabilities through tool integration. This is a natural fit for more open-ended agents because:\n\n- Support interactions naturally follow a conversation flow while requiring access to external information and actions;\n- Tools can be integrated to pull customer data, order history, and knowledge base articles;\n- Actions such as issuing refunds or updating tickets can be handled programmatically; and\n- Success can be clearly measured through user-defined resolutions.\n\nSeveral companies have demonstrated the viability of this approach through usage-based pricing models that charge only for successful resolutions, showing confidence in their agents' effectiveness.\n\n### B. Coding agents\n\nThe software development space has shown remarkable potential for LLM features, with capabilities evolving from code completion to autonomous problem-solving. Agents are particularly effective because:\n\n- Code solutions are verifiable through automated tests;\n- Agents can iterate on solutions using test results as feedback;\n- The problem space is well-defined and structured; and\n- Output quality can be measured objectively.\n\nIn our own implementation, agents can now solve real GitHub issues in the [SWE-bench Verified](https://www.anthropic.com/research/swe-bench-sonnet) benchmark based on the pull request description alone. However, whereas automated testing helps verify functionality, human review remains crucial for ensuring solutions align with broader system requirements.\n\n## Appendix 2: Prompt engineering your tools\n\nNo matter which agentic system you're building, tools will likely be an important part of your agent. [Tools](https://www.anthropic.com/news/tool-use-ga) enable Claude to interact with external services and APIs by specifying their exact structure and definition in our API. When Claude responds, it will include a [tool use block](https://docs.anthropic.com/en/docs/build-with-claude/tool-use#example-api-response-with-a-tool-use-content-block) in the API response if it plans to invoke a tool. Tool definitions and specifications should be given just as much prompt engineering attention as your overall prompts. In this brief appendix, we describe how to prompt engineer your tools.\n\nThere are often several ways to specify the same action. For instance, you can specify a file edit by writing a diff, or by rewriting the entire file. For structured output, you can return code inside markdown or inside JSON. In software engineering, differences like these are cosmetic and can be converted losslessly from one to the other. However, some formats are much more difficult for an LLM to write than others. Writing a diff requires knowing how many lines are changing in the chunk header before the new code is written. Writing code inside JSON (compared to markdown) requires extra escaping of newlines and quotes.\n\nOur suggestions for deciding on tool formats are the following:\n\n- Give the model enough tokens to 'think' before it writes itself into a corner.\n- Keep the format close to what the model has seen naturally occurring in text on the internet.\n- Make sure there's no formatting 'overhead' such as having to keep an accurate count of thousands of lines of code, or string-escaping any code it writes.\n\nOne rule of thumb is to think about how much effort goes into human-computer interfaces (HCI), and plan to invest just as much effort in creating good *agent*\\-computer interfaces (ACI). Here are some thoughts on how to do so:\n\n- Put yourself in the model's shoes. Is it obvious how to use this tool, based on the description and parameters, or would you need to think carefully about it? If so, then it\u2019s probably also true for the model. A good tool definition often includes example usage, edge cases, input format requirements, and clear boundaries from other tools.\n- How can you change parameter names or descriptions to make things more obvious? Think of this as writing a great docstring for a junior developer on your team. This is especially important when using many similar tools.\n- Test how the model uses your tools: Run many example inputs in our [workbench](https://console.anthropic.com/workbench) to see what mistakes the model makes, and iterate.\n- [Poka-yoke](https://en.wikipedia.org/wiki/Poka-yoke) your tools. Change the arguments so that it is harder to make mistakes.\n\nWhile building our agent for [SWE-bench](https://www.anthropic.com/research/swe-bench-sonnet), we actually spent more time optimizing our tools than the overall prompt. For example, we found that the model would make mistakes with tools using relative filepaths after the agent had moved out of the root directory. To fix this, we changed the tool to always require absolute filepaths\u2014and we found that the model used this method flawlessly.\u3002", "top": 0, "createdAt": 1752321675, "style": "", "script": "", "head": "", "ogImage": "https://github.githubassets.com/favicons/favicon.svg", "createdDate": "2025-07-12", "dateLabelColor": "#0969da"}, "P4": {"htmlDir": "docs/post/Deep Research,  WebDancer,  WebSailor.html", "labels": ["AI", "Source Code"], "postTitle": "Deep Research,  WebDancer,  WebSailor", "postUrl": "post/Deep%20Research%2C%20%20WebDancer%2C%20%20WebSailor.html", "postSourceUrl": "https://github.com/winnerineast/winnerineast.github.io/issues/4", "commentNum": 0, "wordCount": 748, "description": "\n- \u95ee\uff1a\u201c\u4e00\u90e8\u77e5\u540d\u7535\u89c6\u5267\uff1a\u5973\u4e8c 1993 \u5e74\u5165\u884c\uff1b\u5973\u4e00\u73b0\u4efb\u4e08\u592b\u662f\u6d59\u6c5f\u6e56\u5dde\u4eba\uff1b\u7537\u4e00\u516d\u5e74\u540e\u767b\u4e0a\u6625\u665a\u3002", "top": 0, "createdAt": 1752321703, "style": "", "script": "", "head": "", "ogImage": "https://github.githubassets.com/favicons/favicon.svg", "createdDate": "2025-07-12", "dateLabelColor": "#0969da"}, "P5": {"htmlDir": "docs/post/MUVERA--rang-duo-xiang-liang-jian-suo-yu-dan-xiang-liang-sou-suo-yi-yang-kuai.html", "labels": ["paper", "AI", "Source Code"], "postTitle": "MUVERA-\u8ba9\u591a\u5411\u91cf\u68c0\u7d22\u4e0e\u5355\u5411\u91cf\u641c\u7d22\u4e00\u6837\u5feb", "postUrl": "post/MUVERA--rang-duo-xiang-liang-jian-suo-yu-dan-xiang-liang-sou-suo-yi-yang-kuai.html", "postSourceUrl": "https://github.com/winnerineast/winnerineast.github.io/issues/5", "commentNum": 0, "wordCount": 786, "description": "\u505aRAG\u7684\u670b\u53cb\u4e00\u5b9a\u8981\u770b\u770b Google \u8fd9\u4e2a\u65b0\u8bba\u6587\u2014\u2014MUVERA\uff1a\u8ba9\u591a\u5411\u91cf\u68c0\u7d22\u4e0e\u5355\u5411\u91cf\u641c\u7d22\u4e00\u6837\u5feb \u5927\u5bb6\u5728RAG\u7684\u68c0\u7d22\u5185\u5bb9\u8fc7\u7a0b\u90fd\u4f1a\u9047\u5230\u8fd9\u79cd\u60c5\u51b5\uff0c\u5982\u679c\u7528\u4f20\u7edf\u641c\u7d22\uff08\u4f8b\u5982ElasticSearch\uff09\uff0c\u6587\u6863 = 1 \u4e2a\u5411\u91cf \u2192 \u5feb\u901f\u4f46\u4e0d\u51c6\u786e\u3002", "top": 0, "createdAt": 1752321737, "style": "", "script": "", "head": "", "ogImage": "https://github.githubassets.com/favicons/favicon.svg", "createdDate": "2025-07-12", "dateLabelColor": "#0969da"}, "P6": {"htmlDir": "docs/post/Optimizing ColPali for Retrieval at Scale, 13x Faster Results.html", "labels": ["AI", "Source Code"], "postTitle": "Optimizing ColPali for Retrieval at Scale, 13x Faster Results", "postUrl": "post/Optimizing%20ColPali%20for%20Retrieval%20at%20Scale%2C%2013x%20Faster%20Results.html", "postSourceUrl": "https://github.com/winnerineast/winnerineast.github.io/issues/6", "commentNum": 0, "wordCount": 6181, "description": "\nColPali is a fascinating leap in document retrieval. Its precision in handling visually rich PDFs is phenomenal, but scaling it to handle real-world datasets comes with its share of computational challenges.\n\nHere\u2019s how we solved these challenges to make ColPali 13x faster without sacrificing the precision it\u2019s known for.\n\n## [](https://qdrant.tech/blog/colpali-qdrant-optimization/#the-scaling-dilemma)The Scaling Dilemma\n\nColPali generates\u00a0**1,030 vectors for just one page of a PDF.**\u00a0While this is manageable for small-scale tasks, in a real-world production setting where you may need to store hundreds od thousands of PDFs, the challenge of scaling becomes significant.\n\nConsider this scenario:\n\n- **Dataset Size:**\u00a020,000 PDF pages.\n- **Number of Vectors:**\u00a0Each page generates ~1,000 vectors of 128 dimensions.\n\nThe total number of comparisons is calculated as:\n\n1,000\u22c51,000\u22c520,000\u22c5128=2.56\u00d71012\u00a0comparisons!\n\nThat\u2019s trillions of comparisons needed to build the index. Even advanced indexing algorithms like\u00a0**HNSW**\u00a0struggle with this scale, as computational costs grow quadratically with amount of multivectors per page.\n\nWe turned to a hybrid optimization strategy combining\u00a0**pooling**\u00a0(to reduce computational overhead) and\u00a0**reranking**\u00a0(to preserve accuracy).\n\nBefore we go any deeper, watch our\u00a0[Webinar video](https://www.youtube.com/live/_h6SN1WwnLs?si=n8gwiIjJ5dnfucXC)\u00a0for the full demo walkthrough.\n\nFor those eager to explore, the\u00a0[codebase is available here](https://github.com/qdrant/demo-colpali-optimized).\n\n## [](https://qdrant.tech/blog/colpali-qdrant-optimization/#two-stage-retrieval-process)Two-Stage Retrieval Process\n\n### [](https://qdrant.tech/blog/colpali-qdrant-optimization/#pooling)Pooling\n\nPooling is well-known in machine learning as a way to compress data while keeping important information. For ColPali, we reduced 1,030 vectors per page to just 38 vectors by pooling rows in the document\u2019s 32x32 grid.\n\n![](https://qdrant.tech/blog/colpali-optimization/rows.png)\n\nMax and mean pooling are the two most popular types, so we decided to test both approaches on the rows of the grid. Likewise, we could apply pooling on columns, which we plan to explore in the future.\n\n- **Mean Pooling:**\u00a0Averages values across rows.\n- **Max Pooling:**\u00a0Selects the maximum value for each feature.\n\n32 vectors represent the pooled rows, while 6 vectors encode contextual information derived from ColPali\u2019s special tokens (e.g.,\u00a0for the beginning of the sequence, and task-specific instructions like \u201cDescribe the image\u201d).\n\nFor our experiments, we chose to preserve these 6 additional vectors.\n\n### [](https://qdrant.tech/blog/colpali-qdrant-optimization/#the-colpali-as-a-reranker-experiment)The \u201cColPali as a Reranker\u201d Experiment\n\nPooling drastically reduces retrieval costs, but there\u2019s a risk of losing fine-grained precision. To address this, we implemented a\u00a0**two-stage retrieval system**, where embeddings generated with ColPali were max/mean pooled by grid rows to create lightweight vectors for the initial retrieval stage, followed by reranking with the original high-resolution embeddings:\n\n1. **Pooled Retrieval:**\u00a0Quickly retrieves the top 200 candidates using lightweight pooled embeddings.\n2. **Full Reranking:**\u00a0Refines these candidates using the original, high-resolution embeddings, delivering the final top 20 results.\n\n### [](https://qdrant.tech/blog/colpali-qdrant-optimization/#implementation)Implementation\n\nWe created a custom dataset with over 20,000 unique PDF pages by merging:\n\n- **ViDoRe Benchmark:**\u00a0Designed for PDF documents retrieval evaluation.\n- **UFO Dataset:**\u00a0Visually rich documents paired with synthetic queries\u00a0[generated by Daniel van Strien](https://huggingface.co/datasets/davanstrien/ufo-ColPali).\n- **DocVQA Dataset:**\u00a0A large set of document-derived Q&A pairs.\n\nEach document was processed into 32x32 grids, generating both full-resolution and pooled embeddings.\u00a0**Full-resolution**\u00a0embeddings consisted of 1,030 vectors per page, while\u00a0**pooled embeddings**\u00a0included mean and max pooling variants.\n\nAll embeddings were were stored and kept in RAM to avoid caching effects during retrieval speed experiments.\n\n### [](https://qdrant.tech/blog/colpali-qdrant-optimization/#experiment-setup)Experiment Setup\n\nWe evaluated retrieval quality with 1,000 queries. First, pooled embeddings retrieved the top 200 candidates. Then, full-resolution embeddings reranked them to produce the final top 20 results.\n\nTo measure performance, we used:\n\n- **NDCG@20:**\u00a0Measures ranking quality (how well the top results align with expectations).\n- **Recall@20:**\u00a0Measures the overlap between this method and the original ColPali retrieval.\n\n## [](https://qdrant.tech/blog/colpali-qdrant-optimization/#results)Results\n\nThe experiment showed promising improvements in speed and accuracy. Retrieval time improved\u00a0**13x**\u00a0compared to using full-resolution embeddings alone.\n\n### [](https://qdrant.tech/blog/colpali-qdrant-optimization/#metrics)Metrics\n\n|Pooling Type|NDCG@20|Recall@20|\n|---|---|---|\n|**Mean**|0.952|0.917|\n|**Max**|0.759|0.656|\n\nMean pooling preserved nearly identical quality to the original ColPali, with NDCG@20 = 0.952 and Recall@20 = 0.917. Max pooling did not perform well enough to be considered viable since it sacrificed significant accuracy without delivering a meaningful speed advantage.\n\n## [](https://qdrant.tech/blog/colpali-qdrant-optimization/#whats-next)What\u2019s Next?\n\nFuture experiments could push these results even further:\n\n- Investigating column-wise pooling for additional compression.\n- Testing half-precision (float16) vectors to balance memory use and speed.\n- Skipping special multivectors during prefetch to streamline retrieval.\n- Combining quantization with oversampling for even faster search.\n\n### [](https://qdrant.tech/blog/colpali-qdrant-optimization/#try-it-yourself)Try It Yourself\n\nCurious to see this in action? Explore the full codebase and experiment with ColPali optimizations:\n\n- **Demo Notebook:**\u00a0[GitHub Repository](https://github.com/qdrant/demo-colpali-optimized)\n- **Webinar Walkthrough:**\u00a0[Watch Here](https://www.youtube.com/live/_h6SN1WwnLs?si=n8gwiIjJ5dnfucXC)\u3002", "top": 0, "createdAt": 1752321794, "style": "", "script": "", "head": "", "ogImage": "https://github.githubassets.com/favicons/favicon.svg", "createdDate": "2025-07-12", "dateLabelColor": "#0969da"}, "P7": {"htmlDir": "docs/post/RAG -shi-zhan-fen-xiang-  - 50-60GB PDF -wen-jian-, 6000 -pian-yi-xue-lun-wen-, -neng-zuo- RAG -ma.html", "labels": ["paper", "AI", "Source Code"], "postTitle": "RAG \u5b9e\u6218\u5206\u4eab  - 50-60GB PDF \u6587\u4ef6, 6000 \u7bc7\u533b\u5b66\u8bba\u6587, \u80fd\u505a RAG \u5417", "postUrl": "post/RAG%20-shi-zhan-fen-xiang-%20%20-%2050-60GB%20PDF%20-wen-jian-%2C%206000%20-pian-yi-xue-lun-wen-%2C%20-neng-zuo-%20RAG%20-ma.html", "postSourceUrl": "https://github.com/winnerineast/winnerineast.github.io/issues/7", "commentNum": 0, "wordCount": 1430, "description": "\u4f5c\u8005 Scott \u662f\u5fae\u8f6f\u5f00\u53d1\u8005\u793e\u533a VP, \u8fd9\u5e94\u8be5\u4e5f\u662f\u4ed6\u5728\u5f00\u53d1\u8005\u4e2d\u9047\u5230\u7684\u5b9e\u9645\u95ee\u9898, \u8fd9\u4e2a\u95ee\u9898\u7684\u8ba8\u8bba\u975e\u5e38\u6fc0\u70c8\u3002", "top": 0, "createdAt": 1752321822, "style": "", "script": "", "head": "", "ogImage": "https://github.githubassets.com/favicons/favicon.svg", "createdDate": "2025-07-12", "dateLabelColor": "#0969da"}, "P8": {"htmlDir": "docs/post/Reliable Agentic RAG with LLM Trustworthiness Estimates.html", "labels": ["paper", "AI", "Source Code"], "postTitle": "Reliable Agentic RAG with LLM Trustworthiness Estimates", "postUrl": "post/Reliable%20Agentic%20RAG%20with%20LLM%20Trustworthiness%20Estimates.html", "postSourceUrl": "https://github.com/winnerineast/winnerineast.github.io/issues/8", "commentNum": 0, "wordCount": 14009, "description": "September 12, 2024\n\n- ![Chris Mauck](https://cleanlab.ai/_next/static/images/chris-2be9b6d6c9460c74034608339ff17c6d.jpg)Chris Mauck\n- ![Jonas Mueller](https://cleanlab.ai/_next/static/images/jonas-038f0156ce880eb9cff38bb91618ea1b.jpg)Jonas Mueller\n\nThis article demonstrates an agentic system to ensure reliable answers in Retrieval-Augmented Generation, while also ensuring that\u00a0**latency and compute costs do not exceed the processing needed to accurately respond to complex queries**. Our system relies on\u00a0_trustworthiness scores_\u00a0for LLM outputs, in order to dynamically adjust retrieval strategies until sufficient context has been retrieved to generate a trustworthy RAG answer.\n\n![Diagram of Agentic RAG with trustworthiness scores](https://cleanlab.ai/_next/static/images/RAG_diagram-cb17a3bb114e546c10eb3e8ddc68a415.png)\n\nBased on the trustworthiness score for a candidate response, the RAG Agent can choose more complex retrieval plans or approve the response for production.\n\n## Introduction\n\nRetrieval-Augmented Generation (RAG) combines the strengths of large language models (LLMs) with powerful retrieval systems to generate more accurate responses grounded in knowledge databases. Simple RAG systems retrieve relevant information to a query via semantic search based on vector embeddings of query and database contents, but this strategy fails for more complex queries.\n\n_Agentic RAG_\u00a0considers various Retrieval strategies as tools available to an LLM orchestrator that can iteratively decide which tools to call next based on what it\u2019s seen thus far. This Agent can plan, execute, and refine multi-step retrieval processes, but it is critical to ensure latency and compute costs do not exceed what is required to produce a good answer for a user\u2019s query. Despite advancements from LLMs \u2192 RAG \u2192 Agentic RAG with sophisticated Retrieval strategies, AI-generated responses still suffer from hallucinations today, producing incorrect or nonsensical information with unwarranted confidence.\n\nThis blog outlines an Agentic RAG system that can produce trustworthy answers even for complex queries, in a manner that keeps latency/costs in check. Our system relies on the\u00a0[Trustworthy Language Model](https://cleanlab.ai/blog/trustworthy-language-model/)\u00a0to score the trustworthiness of a candidate response (based on the query and currently retrieved context). When the current response is deemed untrustworthy, the Agent is tasked with orchestrating a better Retrieval strategy to improve the context. This system starts with cheaper Retrieval strategies, and dynamically tries strategies with greater runtime/costs\u00a0**only for complex queries where they are necessary to produce a trustworthy RAG answer**.\n\n## Trustworthy Language Model (TLM)\n\nFor a given user query, the RAG system will retrieve relevant context, which is then fed into a LLM to produce the response.\u00a0_But how do we know when the response is untrustworthy_? For instance, here is question incorrectly answered by ChatGPT with no indication it should not be trusted.\n\n![ChatGPT giving an incorrect answer](https://cleanlab.ai/_next/static/images/gpt-367eda0e9cc624b7c3497ea417db378b.png)\n\nTLM automates this determination, by producing a trustworthiness score (between 0-1) for responses from any LLM. For the above prompt & ChatGPT response:\n\n```text\ntlm.get_trustworthiness_score(prompt, response) = 0.413\n```\n\nindicating this response should not be trusted.\n\nThese scores\u00a0[have been found](https://cleanlab.ai/blog/trustworthy-language-model/)\u00a0to detect hallucinations and LLM errors with greater precision/recall than alternative approaches like asking the LLM to evaluate its own output or relying on token-probabilities. TLM internally estimates aleatoric and epistemic uncertainty of the LLM by combining techniques including self-reflection, probabilistic prediction, and observed consistency. You can\u00a0[present](https://help.cleanlab.ai/tutorials/tlm_rag/)\u00a0TLM trustworthiness scores to users of your RAG system to automatically help them understand which responses warrant further scrutiny.\n\n## Utilizing the trustworthiness score in Agentic RAG\n\nA user\u2019s query is processed by our RAG system as follows: the Retrieval Planner Agent orchestrates a series of Retrieval strategies in order to discover relevant context, that when appended to the query, leads to an LLM response with sufficiently high trustworthiness score. The Agent is encouraged to start with faster/cheaper Retrieval strategies and only escalate to more complex Retrieval when a high trustworthiness score has not yet been achieved. As soon as a trustworthy LLM response is produced, it is returned to the user.\n\nThis high-level system can work with many types of Retrieval Planner Agent implementations (e.g. via frameworks like LangGraph and tool-use algorithms like OpenAI Function Calling), as well as all sorts of Retrieval strategies. The goal is to minimize the runtime and costs required to process most queries, while still being able to produce trustworthy responses for complex queries that necessitate more compute.\n\n### Potential Retrieval Strategies\n\nAs a concrete example, our Retrieval Planner Agent might choose from the following Retrieval strategies, increasing in time and compute complexity:\n\n1. No Retrieval\n\n- Complexity:\u00a0_None_\n- The query is answerable with general knowledge the LLM already knows.\n\n2. Semantic Search (vector embedding similarity)\n\n- Complexity:\u00a0_Low_\n- Vector database (Pinecone, Qdrant, Weaviate, etc.) is searched using top similarities in space of embeddings (Sentence Transformers, Voyage, etc.)\n\n3. [Hybrid Search (vector + keyword search) with Reciprocal Rank Fusion](https://www.assembled.com/blog/better-rag-results-with-reciprocal-rank-fusion-and-hybrid-search)\n\n- Complexity:\u00a0_Low/Medium_\n- Knowledge database is searched via a combination of vector similarity and classical keyword search like BM25, with results rankings from different searches aggregated via the RRF method.\n\n4. [Re-Ranking](https://adasci.org/a-hands-on-guide-to-enhance-rag-with-re-ranking/)\u00a0retrieved results\n\n- Complexity:\u00a0_Medium_\n- A specialized re-ranker model is applied to the retrieved results from either vector or keyword search that more accurately estimates which ones are relevant to the query.\n\n5. [Query Expansion](https://docs.llamaindex.ai/en/stable/examples/query_transformations/query_transform_cookbook/)\n\n- Complexity:\u00a0_Medium/High_\n- User query is rewritten into possibly multiple queries before (possibly multiple steps of) Retrieval. This includes entity recognition, separate keyword searches, and methods like\u00a0[Hypothetical Document Embeddings (Hyde)](https://arxiv.org/abs/2212.10496)\u00a0or\u00a0[Step-Back Prompting](https://arxiv.org/abs/2310.06117).\n\n6. Chunk/Document Expansion ([Multi-Hop RAG](https://cobusgreyling.medium.com/multihop-rag-1c695794eeda),\u00a0[GraphRAG](https://neo4j.com/blog/graphrag-manifesto/))\n\n- Complexity:\u00a0_Medium/High_\n- Returned chunks from the vector database search are expanded by referring to the original document from which they came and traversing related documents or Knowledge Graphs like Neo4j. This may be required to find additional information that is needed for the retrieved context to be useful.\n\n## Examples of our Trustworthy Agentic RAG in action\n\nTo make things more concrete, we consider a RAG application intended to answer questions based on Nvidia\u2019s product documentation.\n\n### Simple Query\n\nLet\u2019s first consider a simple query that a user may pose:\n\n> **Query:**\u00a0_Which component of a computer is responsible for graphics rendering?_\n\nOn the first pass, our RAG Agent chooses the least complex retrieval plan: do not retrieve anything. The response and associated trustworthiness score are:\n\n> **Response:**\u00a0The component of a computer responsible for graphics rendering is the Graphics Processing Unit (GPU).\n> \n> **Trustworthiness Score:**\u00a00.986\n\nIn this case, the RAG Agent determines the response to be trustworthy and approves the response to be used downstream.\n\n### Complex Query\n\nNow for a more complex query:\n\n> **Query:**\u00a0_How many NVIDIA CUDA Cores does the RTX 4090 have?_\n\nOur RAG Agent might again initially choose to not retrieve anything, yielding:\n\n> **Response:**\u00a0As of my last update in October 2022, NVIDIA has not released an RTX 4090 graphics card. Therefore, there are no official specifications.\n> \n> **Trustworthiness Score:**\u00a00.725\n\nIt appears that the LLM used does not have access to this knowledge. This is quantified by the trustworthiness score, enabling our system to determine this is not a reliable response and then to automatically employ a more complex retrieval plan: vector embedding search. On the second pass, the context, response, and trustworthiness score are:\n\n> **Retrieved Context:**\n> \n> GeForce RTX 4090 GPU Engine Specs: NVIDIA CUDA Cores 16384 Shader Cores Ada Lovelace 83 TFLOPS Ray Tracing Cores 3rd Generation 191 TFLOPS Tensor Cores (AI) 4th Generation 1321 AI TOPS Boost Clock (GHz) 2.52 Base Clock (GHz) 2.23\n> \n> **Response:**\u00a0The NVIDIA GeForce RTX 4090 features 16,384 CUDA cores.\n> \n> **Trustworthiness Score:**\u00a00.988\n\nNice! The RAG Agent was able to dynamically up the complexity of the retrieval plan and now has a satisfactory answer with a high trustworthiness score.\n\n### More Complex Query\n\nLet\u2019s consider a more complex query:\n\n> **Query:**\u00a0_What were the key design challenges faced by NVIDIA in developing the RTX 4090, and how were they overcome?_\n\nOur RAG Agent might again initially chooses to not retrieve anything, yielding:\n\n> **Response:**\u00a0The NVIDIA RTX 4090 was developed to push the boundaries of GPU performance, requiring significant advancements in architecture, thermal management, and power efficiency.\n> \n> **Trustworthiness Score:**\u00a00.635\n\nTLM identifies this response is generic and not confidently containing the right information. In the next pass, our RAG Agent might opt for vector similarity search again, yielding:\n\n> **Retrieved Context:**\u00a0NVIDIA\u2019s RTX 4090 was anticipated to be a major leap in GPU technology. There were discussions about improving overall architecture and enhancing ray tracing capabilities. However, specific details about the challenges during its development are scarce. Some sources suggest that NVIDIA focused on maximizing performance for gaming and AI applications, but there is little concrete information on the exact hurdles faced.\n> \n> **Response:**\u00a0The NVIDIA RTX 4090 focused on improving performance and ray tracing capabilities, but specific details about the challenges encountered during development are not well-documented. It is believed that NVIDIA prioritized enhancements in gaming and AI, but the exact difficulties they faced remain unclear.\n> \n> **Trustworthiness Score:**\u00a00.805\n\nIn this pass, our RAG Agent retrieved unsatisfactory information. The corresponding moderate trustworthiness score reflects the lack of specific context. Our RAG Agent thus considers another Retrieval pass with a more complex Hybrid search strategy, yielding:\n\n> **Retrieved Context:**\u00a0The RTX 4090\u2019s development saw NVIDIA addressing several engineering hurdles. The thermal challenges were exacerbated by the sheer density of the CUDA cores, necessitating an innovative cooling solution. NVIDIA introduced a new fan design and a larger vapor chamber to dissipate heat more effectively. On the power side, the 4090 required a new power connector standard, the PCIe Gen 5 16-pin, which allowed for higher wattage delivery, ensuring stable operation under load. For AI integration, NVIDIA refined its Tensor Core technology to balance power and performance, ensuring that the 4090 could handle advanced AI workloads without bottlenecking other processes.\n> \n> **Response:**\u00a0NVIDIA\u2019s RTX 4090 development involved overcoming several key challenges, particularly around thermal management and power delivery. The GPU\u2019s high density of 16,384 CUDA cores required an innovative cooling system, leading to the introduction of a new fan design and larger vapor chamber. Additionally, the need for higher power delivery led to the adoption of the PCIe Gen 5 16-pin connector. To integrate AI features effectively, NVIDIA refined its Tensor Core technology, balancing power and performance to handle advanced AI workloads efficiently.\n> \n> **Trustworthiness Score:**\u00a00.975\n\nWith the expanded context, the RAG Agent produces a highly detailed and specific response that earns a high enough trustworthiness score to return to the user. That said, this higher quality answer did require additional time/costs to produce.\n\nThe same process continues for queries with greater levels of complexity. Our RAG Agent continues escalating Retrieval strategies until the trustworthiness score indicates a reliable response, or the maximum time limit is reached. If no Retrieval strategy yields a trustworthy LLM response, then our system responds that it cannot handle this question without additional clarifications/information.\n\n## Wrap Up\n\nIntegrating the Trustworthy Language Model enables Agentic RAG systems that can ensure accurate answers to complex queries while bounding latency/costs for regular queries. You can adopt this approach to navigate the delicate balance between speed, cost, and accuracy across diverse RAG applications\u2014from customer service to specialized fields like finance, law, and medicine.\n\nWhile traditional RAG systems generate responses of unknown quality based on predefined steps to process every query,\u00a0**the future of AI lies in systems that assess response trustworthiness and adapt processing plans to each query\u2019s complexity**. Agentic RAG with the TLM offers a promising step toward this future of\u00a0_reliable_\u00a0AI.\n\n## Next Steps\n\n1. Get started with the\u00a0[TLM tutorials](https://help.cleanlab.ai/tutorials/tlm/).\n2. Try it instantly via the\u00a0[TLM Playground](https://tlm.cleanlab.ai/).\n3. Read\u00a0[benchmarks](https://cleanlab.ai/blog/trustworthy-language-model/)\u00a0measuring the effectiveness of LLM trustworthiness scores.\u3002", "top": 0, "createdAt": 1752321863, "style": "", "script": "", "head": "", "ogImage": "https://github.githubassets.com/favicons/favicon.svg", "createdDate": "2025-07-12", "dateLabelColor": "#0969da"}, "P9": {"htmlDir": "docs/post/What is \ud835\uddd4\ud835\uddf4\ud835\uddf2\ud835\uddfb\ud835\ude01\ud835\uddf6\ud835\uddf0 \ud835\udde5\ud835\uddd4\ud835\uddda.html", "labels": ["paper", "AI", "Source Code"], "postTitle": "What is \ud835\uddd4\ud835\uddf4\ud835\uddf2\ud835\uddfb\ud835\ude01\ud835\uddf6\ud835\uddf0 \ud835\udde5\ud835\uddd4\ud835\uddda", "postUrl": "post/What%20is%20%F0%9D%97%94%F0%9D%97%B4%F0%9D%97%B2%F0%9D%97%BB%F0%9D%98%81%F0%9D%97%B6%F0%9D%97%B0%20%F0%9D%97%A5%F0%9D%97%94%F0%9D%97%9A.html", "postSourceUrl": "https://github.com/winnerineast/winnerineast.github.io/issues/9", "commentNum": 0, "wordCount": 2102, "description": "\nIn real world applications, simple naive RAG systems are rarely used nowadays. To provide correct answers to a user query, we are always adding some agency to the RAG system. However, it is important to \ud835\uddfb\ud835\uddfc\ud835\ude01 \ud835\uddf4\ud835\uddf2\ud835\ude01 \ud835\uddf9\ud835\uddfc\ud835\ude00\ud835\ude01 \ud835\uddf6\ud835\uddfb \ud835\ude01\ud835\uddf5\ud835\uddf2 \ud835\uddef\ud835\ude02\ud835\ude07\ud835\ude07 \ud835\uddee\ud835\uddfb\ud835\uddf1 \ud835\ude01\ud835\uddf2\ud835\uddff\ud835\uddfa\ud835\uddf6\ud835\uddfb\ud835\uddfc\ud835\uddf9\ud835\uddfc\ud835\uddf4\ud835\ude06 and understand that there is \ud835\uddfb\ud835\uddfc \ud835\ude00\ud835\uddf6\ud835\uddfb\ud835\uddf4\ud835\uddf9\ud835\uddf2 \ud835\uddef\ud835\uddf9\ud835\ude02\ud835\uddf2\ud835\uddfd\ud835\uddff\ud835\uddf6\ud835\uddfb\ud835\ude01 to add the mentioned agency to your RAG system and you should adapt to your use case. My advice is to not get stuck on terminology and think about engineering flows. Let\u2019s explore some of the moving pieces in Agentic RAG: \ud835\udfed. Analysis of the user query: we pass the original user query to a LLM based Agent for analysis. This is where: \n- The original query can be rewritten, sometimes multiple times to create either a single or multiple queries to be passed down the pipeline. \n- The agent decides if additional data sources are required to answer the query. \n\ud835\udfee. If additional data is required, the Retrieval step is triggered. In Agentic RAG case, we could have a single or multiple agents responsible for figuring out what data sources should be tapped into, few examples: \n- Real time user data. This is a pretty cool concept as we might have some real time information like current location available for the user. \n- Internal documents that a user might be interested in. \n- Data available on the web. \n\ud835\udfef. If there is no need for additional data, we try to compose the answer (or multiple answers) straight via an LLM. \n\ud835\udff0. The answer (or answers) get analyzed, summarized and evaluated for correctness and relevance: \n- If the Agent decides that the answer is good enough, it gets returned to the user. \n- If the Agent decides that the answer needs improvement, we try to rewrite the usr query and repeat the generation loop. \n\nThe real power of Agentic RAG lies in its ability to perform additional routing pre and post generation, handle multiple distinct data sources for retrieval if it is needed and recover from failures in generating correct answers. What are your thoughts on Agentic RAG? Let me know in the comments! \n\n![\u56fe\u50cf](https://pbs.twimg.com/media/Gc_Wr6JXcAAMPEG?format=jpg&name=small)\n\n\u3002", "top": 0, "createdAt": 1752321884, "style": "", "script": "", "head": "", "ogImage": "https://github.githubassets.com/favicons/favicon.svg", "createdDate": "2025-07-12", "dateLabelColor": "#0969da"}, "P10": {"htmlDir": "docs/post/yi-kuan-PDF-zhuan-JSON,Markdown-de-gong-ju-\uff1aDocling.html", "labels": ["AI", "Source Code"], "postTitle": "\u4e00\u6b3ePDF\u8f6cJSON,Markdown\u7684\u5de5\u5177\uff1aDocling", "postUrl": "post/yi-kuan-PDF-zhuan-JSON%2CMarkdown-de-gong-ju-%EF%BC%9ADocling.html", "postSourceUrl": "https://github.com/winnerineast/winnerineast.github.io/issues/10", "commentNum": 0, "wordCount": 115, "description": "\u652f\u6301\u8be6\u7ec6\u9875\u9762\u5e03\u5c40\u548c\u9605\u8bfb\u987a\u5e8f\u7406\u89e3\u3001\u8868\u683c\u7ed3\u6784\u6062\u590d\uff0c\u5143\u6570\u636e\u63d0\u53d6\uff0c\u652f\u6301OCR\u529f\u80fd\uff0c\u53ef\u7528\u4e8e\u626b\u63cf\u7684PDF \n\ngithub\uff1a[https://github.com/DS4SD/docling](https://t.co/AOYqyN8JN5)\u3002", "top": 0, "createdAt": 1752321905, "style": "", "script": "", "head": "", "ogImage": "https://github.githubassets.com/favicons/favicon.svg", "createdDate": "2025-07-12", "dateLabelColor": "#0969da"}, "P11": {"htmlDir": "docs/post/wu-da-zhu-liu-Agentic-kuang-jia-you-lie-dui-bi-fen-xi.html", "labels": ["AI", "Source Code"], "postTitle": "\u4e94\u5927\u4e3b\u6d41Agentic\u6846\u67b6\u4f18\u52a3\u5bf9\u6bd4\u5206\u6790", "postUrl": "post/wu-da-zhu-liu-Agentic-kuang-jia-you-lie-dui-bi-fen-xi.html", "postSourceUrl": "https://github.com/winnerineast/winnerineast.github.io/issues/11", "commentNum": 0, "wordCount": 1080, "description": "\n\u300c\u5fae\u8f6f\u3001OpenAI \u7b49\u79d1\u6280\u5de8\u5934\u63a8\u51fa\u7684\u4e94\u5927\u591a AI \u4ee3\u7406\u6846\u67b6\u5404\u5177\u7279\u8272\uff0cAutoGen \u9002\u5408\u5f00\u53d1\u3001CrewAI \u6613\u4e0a\u624b\u3001LangGraph \u6700\u7075\u6d3b\u3001Swarm \u6700\u7b80\u5355\u3001Magnetic-One \u8f83\u5168\u80fd\uff0c\u5f00\u53d1\u8005\u8be5\u600e\u4e48\u9009\u5462\uff1f\u300d 1. AutoGen\n\n[@pyautogen](https://x.com/pyautogen)\n\n- \u4f18\u52bf: - \u6700\u65e9\u4e14\u6700\u6d41\u884c\u7684\u6846\u67b6 - \u7279\u522b\u9002\u5408\u8f6f\u4ef6\u5f00\u53d1\u4efb\u52a1 - \u6709\u5fae\u8f6f\u5f3a\u5927\u7684\u793e\u533a\u652f\u6301 - \u57fa\u4e8e\u7528\u6237\u4ee3\u7406\u548c\u52a9\u624b\u4ee3\u7406\u7684\u4ea4\u4e92\u6a21\u5f0f - \u5c40\u9650: - \u5bf9\u975e\u7a0b\u5e8f\u5458\u4e0d\u591f\u53cb\u597d - \u8bbe\u7f6e\u590d\u6742,\u7279\u522b\u662f\u4f7f\u7528\u672c\u5730LLM\u65f6 - \u5728\u975e\u8f6f\u4ef6\u5f00\u53d1\u4efb\u52a1\u4e0a\u8868\u73b0\u4e00\u822c 2. CrewAI\n\n[@crewAIInc](https://x.com/crewAIInc)\n\n- \u4f18\u52bf: - \u975e\u5e38\u76f4\u89c2,\u4e3b\u8981\u4f9d\u8d56\u63d0\u793a\u8bcd\u7f16\u5199 - \u5bb9\u6613\u521b\u5efa\u548c\u6dfb\u52a0\u65b0\u4ee3\u7406 - \u5bf9\u975e\u6280\u672f\u7528\u6237\u53cb\u597d - \u4e0e\u5927\u591a\u6570LLM\u63d0\u4f9b\u5546\u517c\u5bb9 - \u5c40\u9650: - \u7075\u6d3b\u6027\u548c\u81ea\u5b9a\u4e49\u6027\u6709\u9650 - \u4e3b\u8981\u9002\u5408\u57fa\u7840\u7528\u4f8b - \u4ee3\u7406\u4e4b\u95f4\u7684\u4ea4\u4e92\u5b58\u5728\u4e00\u4e9bbug - \u793e\u533a\u652f\u6301\u6709\u9650 3. LangGraph\n\n[@LangChainAI](https://x.com/LangChainAI)\n\n- \u4f18\u52bf: - \u5efa\u7acb\u5728LangChain\u4e4b\u4e0a,\u57fa\u4e8e\u6709\u5411\u5faa\u73af\u56fe - \u975e\u5e38\u7075\u6d3b\u548c\u53ef\u5b9a\u5236 - \u6709\u826f\u597d\u7684\u793e\u533a\u652f\u6301 - \u53ef\u4e0e\u5f00\u6e90LLM\u548c\u5404\u79cdAPI\u914d\u5408\u4f7f\u7528 - \u5c40\u9650: - \u6587\u6863\u4e0d\u591f\u5b8c\u5584 - \u5bf9\u975e\u7a0b\u5e8f\u5458\u4e0d\u591f\u53cb\u597d - \u9700\u8981\u8f83\u5f3a\u7684\u7f16\u7a0b\u6280\u80fd 4. OpenAI Swarm\n\n[@OpenAIDevs](https://x.com/OpenAIDevs)\n\n- \u4f18\u52bf: - \u6700\u5bb9\u6613\u4e0a\u624b\u7684\u6846\u67b6 - \u7b80\u5316\u4e86\u4ee3\u7406\u521b\u5efa\u548c\u5207\u6362 - \u9002\u5408\u5feb\u901fdemo - \u5c40\u9650: - \u4ec5\u652f\u6301OpenAI API - \u4e0d\u9002\u5408\u751f\u4ea7\u90e8\u7f72 - \u7075\u6d3b\u6027\u4e0d\u8db3 - \u793e\u533a\u652f\u6301\u8584\u5f31 5. Magnetic-One (\n\n[@OpenAtMicrosoft](https://x.com/OpenAtMicrosoft)\n\n) - \u4f18\u52bf: - \u9002\u5408\u975e\u7a0b\u5e8f\u5458 - \u9884\u88c55\u4e2a\u4ee3\u7406(\u542b1\u4e2a\u7ba1\u7406\u4ee3\u7406) - \u5efa\u7acb\u5728AutoGen\u4e4b\u4e0a - \u5c40\u9650: - \u5f00\u6e90LLM\u652f\u6301\u590d\u6742 - \u7075\u6d3b\u6027\u4e0d\u8db3 - \u6587\u6863\u548c\u793e\u533a\u652f\u6301\u51e0\u4e4e\u6ca1\u6709 \u4f5c\u8005\u7684\u6700\u7ec8\u5efa\u8bae: - \u8f6f\u4ef6\u5f00\u53d1\u4efb\u52a1\u9009\u62e9 AutoGen - \u65b0\u624b\u5165\u95e8\u9009\u62e9 OpenAI Swarm \u6216 CrewAI - \u590d\u6742\u4efb\u52a1\u9009\u62e9 LangGraph - \u5f00\u6e90LLM\u4f7f\u7528\u63a8\u8350 LangGraph - \u793e\u533a\u652f\u6301\u6700\u597d\u7684\u662f AutoGen - \u5feb\u901f\u542f\u52a8\u9009\u62e9 CrewAI - \u6210\u672c\u6548\u76ca\u8003\u8651 Magnetic-One\u3002", "top": 0, "createdAt": 1752321931, "style": "", "script": "", "head": "", "ogImage": "https://github.githubassets.com/favicons/favicon.svg", "createdDate": "2025-07-12", "dateLabelColor": "#0969da"}, "P12": {"htmlDir": "docs/post/quan-mian-jie-ma-MLOps - -cong-gou-xiang-dao-luo-di-de-ji-qi-xue-xi-zhi-lv.html", "labels": ["AI", "Source Code"], "postTitle": "\u5168\u9762\u89e3\u7801MLOps - \u4ece\u6784\u60f3\u5230\u843d\u5730\u7684\u673a\u5668\u5b66\u4e60\u4e4b\u65c5", "postUrl": "post/quan-mian-jie-ma-MLOps%20-%20-cong-gou-xiang-dao-luo-di-de-ji-qi-xue-xi-zhi-lv.html", "postSourceUrl": "https://github.com/winnerineast/winnerineast.github.io/issues/12", "commentNum": 0, "wordCount": 724, "description": "<img width='1818' height='1892' alt='Image' src='https://github.com/user-attachments/assets/792fc6af-5295-4faa-8570-fb3f33be16d6' />\n\n\u8fd9\u5f20\u56fe\u5c55\u793a\u4e86\u4e00\u4e2a\u7aef\u5230\u7aef\u7684MLOps\uff08\u673a\u5668\u5b66\u4e60\u8fd0\u7ef4\uff09\u67b6\u6784\uff0c\u5206\u4e3a\u51e0\u4e2a\u5173\u952e\u6b65\u9aa4\u548c\u533a\u57df\u3002", "top": 0, "createdAt": 1752321994, "style": "", "script": "", "head": "", "ogImage": "https://github.githubassets.com/favicons/favicon.svg", "createdDate": "2025-07-12", "dateLabelColor": "#0969da"}, "P13": {"htmlDir": "docs/post/ru-he-da-zao-shu-yu-ni-de-ding-zhi-hua-da-mo-xing-liao-tian-ji-qi-ren.html", "labels": ["AI", "Source Code"], "postTitle": "\u5982\u4f55\u6253\u9020\u5c5e\u4e8e\u4f60\u7684\u5b9a\u5236\u5316\u5927\u6a21\u578b\u804a\u5929\u673a\u5668\u4eba", "postUrl": "post/ru-he-da-zao-shu-yu-ni-de-ding-zhi-hua-da-mo-xing-liao-tian-ji-qi-ren.html", "postSourceUrl": "https://github.com/winnerineast/winnerineast.github.io/issues/13", "commentNum": 0, "wordCount": 782, "description": "\n1. \u6587\u6863\u62c6\u89e3\uff1a - \u9996\u5148\uff0c\u4f60\u5f97\u6709\u4e00\u4e9b\u6587\u672c\u8d44\u6599\uff08Documents\uff09\u3002", "top": 0, "createdAt": 1752322036, "style": "", "script": "", "head": "", "ogImage": "https://github.githubassets.com/favicons/favicon.svg", "createdDate": "2025-07-12", "dateLabelColor": "#0969da"}, "P14": {"htmlDir": "docs/post/mo-xing-zhu-ce-biao-de-quan-jing-yun-zuo.html", "labels": ["AI", "Source Code"], "postTitle": "\u6a21\u578b\u6ce8\u518c\u8868\u7684\u5168\u666f\u8fd0\u4f5c", "postUrl": "post/mo-xing-zhu-ce-biao-de-quan-jing-yun-zuo.html", "postSourceUrl": "https://github.com/winnerineast/winnerineast.github.io/issues/14", "commentNum": 0, "wordCount": 457, "description": "\n\u63ed\u5f00MLOps\u7684\u6838\u5fc3\u5965\u79d8\uff1a\u6a21\u578b\u6ce8\u518c\u8868\u7684\u5168\u666f\u8fd0\u4f5c\uff01 \u8fd9\u5f20\u56fe\u5c55\u793a\u4e86\u5728MLOps\uff08\u673a\u5668\u5b66\u4e60\u8fd0\u7ef4\uff09\u4e2d\uff0c\u6a21\u578b\u6ce8\u518c\u8868\u5982\u4f55\u4f5c\u4e3a\u6838\u5fc3\u73af\u8282\uff0c\u534f\u8c03\u548c\u7ba1\u7406\u6574\u4e2a\u673a\u5668\u5b66\u4e60\u6d41\u6c34\u7ebf\u3002", "top": 0, "createdAt": 1752322065, "style": "", "script": "", "head": "", "ogImage": "https://github.githubassets.com/favicons/favicon.svg", "createdDate": "2025-07-12", "dateLabelColor": "#0969da"}, "P15": {"htmlDir": "docs/post/xian-dai- LLM -li-lun-dao-gong-cheng-shi-jian- -cong-yu-xun-lian-dao-bu-shu.html", "labels": ["AI"], "postTitle": "\u73b0\u4ee3 LLM \u7406\u8bba\u5230\u5de5\u7a0b\u5b9e\u8df5 \u4ece\u9884\u8bad\u7ec3\u5230\u90e8\u7f72", "postUrl": "post/xian-dai-%20LLM%20-li-lun-dao-gong-cheng-shi-jian-%20-cong-yu-xun-lian-dao-bu-shu.html", "postSourceUrl": "https://github.com/winnerineast/winnerineast.github.io/issues/15", "commentNum": 0, "wordCount": 1277, "description": "\n\u8bb2\u5e08\n\n[@yanndubs](https://x.com/yanndubs)\n\n(\n\n[@OpenAI](https://x.com/OpenAI)\n\n| PhD\n\n[@StanfordAILab](https://x.com/StanfordAILab)\n\n) \u8fd9\u95e8\u8bfe\u7a0b\u975e\u5e38\u7cfb\u7edf\u5730\u4ecb\u7ecd\u4e86\u6784\u5efa\u73b0\u4ee3 LLM \u7684\u5b8c\u6574\u6280\u672f\u6808, \u4ece\u57fa\u7840\u6982\u5ff5\u5230\u5b9e\u8df5\u7ec6\u8282\u90fd\u6709\u6d89\u53ca\u3002", "top": 0, "createdAt": 1752322095, "style": "", "script": "", "head": "", "ogImage": "https://github.githubassets.com/favicons/favicon.svg", "createdDate": "2025-07-12", "dateLabelColor": "#0969da"}, "P16": {"htmlDir": "docs/post/jie-gou- AI -yuan-sheng-ying-yong- - -qi-ye-ruan-jian-de-wei-lai-tu-jing.html", "labels": ["AI"], "postTitle": "\u89e3\u6784 AI \u539f\u751f\u5e94\u7528 - \u4f01\u4e1a\u8f6f\u4ef6\u7684\u672a\u6765\u56fe\u666f", "postUrl": "post/jie-gou-%20AI%20-yuan-sheng-ying-yong-%20-%20-qi-ye-ruan-jian-de-wei-lai-tu-jing.html", "postSourceUrl": "https://github.com/winnerineast/winnerineast.github.io/issues/16", "commentNum": 0, "wordCount": 47297, "description": "\n<img width='2000' height='1125' alt='Image' src='https://github.com/user-attachments/assets/2a7cad0a-f31b-4ff1-b153-7c14a45da4ba' />\n\n\u300c\u6765\u81ea Sapphire Ventures \u7684\u91cd\u8981\u6587\u7ae0\uff0c\u901a\u8fc7\u8bbe\u8ba1\u3001\u6570\u636e\u3001\u4e13\u4e1a\u9886\u57df\u3001\u52a8\u6001\u7279\u6027\u3001\u5546\u4e1a\u5206\u53d1\u4e94\u5927\u7ef4\u5ea6\uff0c\u63a2\u8ba8 AI \u5982\u4f55\u4ece'\u529f\u80fd\u9644\u52a0'\u8f6c\u5411'\u6838\u5fc3\u9a71\u52a8'\uff0c\u91cd\u5851\u4e0b\u4e00\u4ee3\u4f01\u4e1a\u8f6f\u4ef6\u7684\u53d1\u5c55\u65b9\u5411\u300d \u300cAI \u539f\u751f\u5e94\u7528\u7684\u5b9a\u4e49\u300d - AI \u662f\u5e94\u7528\u7684\u6838\u5fc3\u4f53\u9a8c\uff0c\u800c\u4e0d\u53ea\u662f\u9644\u52a0\u529f\u80fd - \u5efa\u7acb\u5728\u57fa\u7840 AI \u80fd\u529b\u4e4b\u4e0a - \u80fd\u7a81\u7834\u4f20\u7edf\u7684\u901f\u5ea6\u3001\u89c4\u6a21\u548c\u6210\u672c\u9650\u5236 - \u80fd\u6301\u7eed\u6539\u8fdb\u548c\u4f18\u5316 - \u5305\u542b\u4e00\u5b9a\u7a0b\u5ea6\u7684\u4e13\u6709 AI \u6280\u672f \u300c\u5e02\u573a\u73b0\u72b6\u300d - 2024\u5e74 AI \u539f\u751f\u5e94\u7528\u7684\u878d\u8d44\u8fbe\u523085\u4ebf\u7f8e\u5143 - \u5df2\u6709\u81f3\u5c1147\u5bb6 AI \u539f\u751f\u5e94\u7528\u516c\u53f8\u5e74\u6536\u5165\u8d85\u8fc72500\u4e07\u7f8e\u5143 - \u9884\u8ba1\u660e\u5e74\u5c06\u6709\u540c\u7b49\u6570\u91cf\u516c\u53f8\u5e74\u6536\u5165\u8d85\u8fc75000\u4e07\u7f8e\u5143 \u300c\u8bc4\u4f30\u6846\u67b6(5\u4e2a\u7ef4\u5ea6)\u300d A. \u8bbe\u8ba1(Design): - \u521b\u5efa\u65b0\u7684\u4ea4\u4e92\u6a21\u5f0f - \u52a0\u901f\u53cd\u9988\u5faa\u73af - \u5f00\u53d1 AI \u539f\u751f\u7cfb\u7edf\u67b6\u6784 B. \u6570\u636e(Data): - \u63d0\u9ad8\u7aef\u5230\u7aef\u6570\u636e\u7ba1\u7406\u7684\u4e25\u8c28\u6027 - \u5229\u7528\u6f5c\u5728\u6570\u636e\u4ef7\u503c - \u521b\u5efa\u65b0\u7684\u4e13\u6709\u6570\u636e\u96c6 C. \u9886\u57df\u4e13\u957f(Domain Expertise): - \u5c06\u7279\u5b9a\u9886\u57df\u6d3b\u52a8\u8f6c\u5316\u4e3a AI \u52a0\u901f\u7684\u5de5\u4f5c\u6d41 - \u5927\u89c4\u6a21\u5feb\u901f\u5408\u6210 - \u7ed3\u5408\u5168\u5c40\u548c\u672c\u5730\u77e5\u8bc6 D. \u52a8\u6001\u6027(Dynamism): - \u5b9e\u65f6\u4f18\u5316\u6027\u80fd\u4e0e\u6210\u672c\u6743\u8861 - \u521b\u5efa\u751f\u6210\u5f0f\u5ba2\u6237\u65c5\u7a0b - \u5b9e\u73b0\u591a\u5c42\u6b21\u8d85\u4e2a\u6027\u5316 E. \u5206\u53d1(Distribution): - \u589e\u52a0\u5b9a\u4ef7\u548c\u5305\u88c5\u7075\u6d3b\u6027 - \u5b9e\u73b0\u65b0\u5546\u4e1a\u6a21\u5f0f\u3002", "top": 0, "createdAt": 1752322135, "style": "", "script": "", "head": "", "ogImage": "https://github.githubassets.com/favicons/favicon.svg", "createdDate": "2025-07-12", "dateLabelColor": "#0969da"}, "P17": {"htmlDir": "docs/post/tao-zhe-xuan-\uff1aAI-yu-ren-lei-zai-xue-xi-he-wen-ti-jie-jue-shang-de-gen-ben-cha-yi-shi-\u2026\u2026.html", "labels": ["AI"], "postTitle": "\u9676\u54f2\u8f69\uff1aAI\u4e0e\u4eba\u7c7b\u5728\u5b66\u4e60\u548c\u95ee\u9898\u89e3\u51b3\u4e0a\u7684\u6839\u672c\u5dee\u5f02\u662f\u2026\u2026", "postUrl": "post/tao-zhe-xuan-%EF%BC%9AAI-yu-ren-lei-zai-xue-xi-he-wen-ti-jie-jue-shang-de-gen-ben-cha-yi-shi-%E2%80%A6%E2%80%A6.html", "postSourceUrl": "https://github.com/winnerineast/winnerineast.github.io/issues/17", "commentNum": 0, "wordCount": 1757, "description": "\nAI\uff0c\u5c3d\u7ba1\u5728\u5904\u7406\u590d\u6742\u4efb\u52a1\u65f6\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u601d\u8003\u8fc7\u7a0b\u66f4\u50cf\u662f\u4e00\u4e2a\u201c\u5e73\u5eb8\u7684\u7814\u7a76\u52a9\u7406\u201d\uff0c\u7f3a\u4e4f\u771f\u6b63\u7684\u7406\u89e3\u4e0e\u521b\u9020\u529b\u3002", "top": 0, "createdAt": 1752322157, "style": "", "script": "", "head": "", "ogImage": "https://github.githubassets.com/favicons/favicon.svg", "createdDate": "2025-07-12", "dateLabelColor": "#0969da"}, "P18": {"htmlDir": "docs/post/ding-ji- LLM -he- VLM -xue-xi-zi-yuan-qing-dan.html", "labels": ["AI"], "postTitle": "\u9876\u7ea7 LLM \u548c VLM \u5b66\u4e60\u8d44\u6e90\u6e05\u5355", "postUrl": "post/ding-ji-%20LLM%20-he-%20VLM%20-xue-xi-zi-yuan-qing-dan.html", "postSourceUrl": "https://github.com/winnerineast/winnerineast.github.io/issues/18", "commentNum": 0, "wordCount": 1606, "description": "\n1. makemore by [Andrej Karpathy](https://youtu.be/PaCmpygFfXo?si=dZCY8sictT658_Sk) \n2. minbpe by [karpathy](https://youtu.be/kCc8FmEb1nY?si=W61JnqMT4gjzF37q) \n3. attention? attention! [Lilian Weng](https://lilianweng.github.io/posts/2018-06-24-attention) ![[Attention_Attention.pdf]]\n4. gpt-2 again by [karpathy](https://youtu.be/kCc8FmEb1nY?si=454hxUDiqD-t4YU0) \n5. llama3 from scratch by [naklecha](https://github.com/naklecha/llama3-from-scratch) \n6. llm training in simple, raw by c/cuda [karpathy](https://github.com/karpathy/llm.c) \n7. decoding strategies in large language models [mlabonne](https://mlabonne.github.io/blog/posts/2023-06-07-Decoding_strategies.html) \n8. how to make llms go fast by [vgel](https://vgel.me/posts/faster-inference/#KV_caching) ![[How to make LLMs go fast.pdf]]\n9. a visual guide to quantization [maarten](https://www.maartengrootendorst.com/blog/quantization/)![[A visual guide to quantization.pdf]]\n10. extending the RoPE by [eleutherai](https://blog.eleuther.ai/yarn) ![[Extending the RoPE.pdf]]\n11. the novice's llm training guide by [alpin](https://rentry.org/llm-training) ![[The Novice LLM Training Guide.pdf]]\n12. a survey on evaluation of large language models [paper](https://arxiv.org/abs/2307.03109) ![[2307.03109v9.pdf]]\n13. mixture of experts explained [huggingface](https://huggingface.co/blog/moe) ![[Mixture of Experts Explained.pdf]]\n14. vision transformer by [aman-arora](https://amaarora.github.io/posts/2021-01-18-ViT.html) ![[Vision Transformer.pdf]]\n15. clip, siglip and paligemma by [umar-jamil](https://youtu.be/vAmKB7iPkWw?si=p4d2eoVNz0Nqwkxs)\u3002", "top": 0, "createdAt": 1752322190, "style": "", "script": "", "head": "", "ogImage": "https://github.githubassets.com/favicons/favicon.svg", "createdDate": "2025-07-12", "dateLabelColor": "#0969da"}, "P19": {"htmlDir": "docs/post/A statistical approach to model evaluations.html", "labels": ["paper", "AI"], "postTitle": "A statistical approach to model evaluations", "postUrl": "post/A%20statistical%20approach%20to%20model%20evaluations.html", "postSourceUrl": "https://github.com/winnerineast/winnerineast.github.io/issues/19", "commentNum": 0, "wordCount": 12585, "description": "\n\nNov 20, 2024\n\n[Read the paper](https://arxiv.org/abs/2411.00640)\n\nSuppose an AI model outperforms another model on a benchmark of interest\u2014testing its general knowledge, for example, or its ability to solve computer-coding questions. Is the difference in capabilities real, or could one model simply have gotten lucky in the choice of questions on the benchmark?\n\nWith the amount of public interest in AI model evaluations\u2014informally called \u201cevals\u201d\u2014this question remains surprisingly understudied among the AI research community. This month, we published a\u00a0[new research paper](https://arxiv.org/abs/2411.00640)\u00a0that attempts to answer the question rigorously. Drawing on statistical theory and the experiment design literature, the paper makes a number of recommendations to the AI research community for reporting eval results in a scientifically informative way. In this post, we briefly go over the reporting recommendations, and the logic behind them.\n\n### Recommendation #1: Use the Central Limit Theorem\n\nEvals often consist of hundreds or thousands of unrelated questions.\u00a0[MMLU](https://arxiv.org/abs/2009.03300v3), for instance, contains questions as diverse as:\n\n- Who discovered the first virus?\n- What is the inverse of \ud835\udc53(\ud835\udc65)=4\u22125\ud835\udc65?\n- Who said that \u201cJurisprudence is the eye of law\u201d?\n\nTo compute an overall eval score, each question is separately scored, and then the overall score is (usually) a simple average of these question scores. Typically, researchers focus their attention on this observed average. But in our paper, we argue that the real object of interest should not be the\u00a0_observed_\u00a0average, but rather the\u00a0_theoretical_\u00a0average across all possible questions. So if we imagine that eval questions were drawn from an unseen \u201cquestion universe,\u201d we can learn about the average score in that universe\u2014that is, we can measure the underlying\u00a0_skill_, independent of the \u201cluck of the draw\u201d\u2014using statistical theory.\n\n![](https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2Fb34871a36ad66fa0330e3ad6488ee87eb96bddda-2401x1260.png&w=3840&q=75)\n\nIf we imagine that eval questions were drawn from a \u201cquestion universe,\u201d then eval scores will tend to follow a normal distribution, centered around the average score of all possible questions.\n\nThis formulation buys us analytic robustness: if a new eval were to be created with questions having the same difficulty distribution as the original eval, we should generally expect our original conclusions to hold.\n\nIn technical terms: under the fairly mild conditions of the\u00a0[Central Limit Theorem](https://en.wikipedia.org/wiki/Central_limit_theorem), the mean values of several random samples taken from the same underlying distribution will tend to follow a\u00a0[normal distribution](https://en.wikipedia.org/wiki/Normal_distribution). The standard deviation (or width) of that normal distribution is commonly known as the\u00a0[standard error of the mean](https://en.wikipedia.org/wiki/Standard_error), or SEM. In our paper, we encourage researchers to report the SEM, derived from the Central Limit Theorem, alongside each calculated eval score\u2014and we show researchers how to use the SEM to quantify the difference in theoretical means between two models. A 95%\u00a0[confidence interval](https://en.wikipedia.org/wiki/Confidence_interval)\u00a0can be calculated from the SEM by adding and subtracting 1.96 \u00d7 SEM from the mean score.\n\n### Recommendation #2: Cluster standard errors\n\nMany evals violate the above assumption of independently selected questions, and instead consist of groups of closely related questions. For example, several questions in a reading-comprehension eval may ask about the same passage of text. Popular evals that follow this pattern include\u00a0[DROP](https://aclanthology.org/N19-1246/),\u00a0[QuAC](https://arxiv.org/abs/1808.07036),\u00a0[RACE](https://aclanthology.org/D17-1082/), and\u00a0[SQuAD](https://arxiv.org/abs/1806.03822).\n\nFor these evals, each question\u2019s selection from the \u201cquestion universe\u201d is no longer independent. Because including several questions about the same passage of text will yield less information than selecting the same number of questions about different passages of text, a naive application of the Central Limit Theorem to the case of non-independent questions will lead us to underestimate the standard error\u2014and potentially mislead analysts into drawing incorrect conclusions from the data.\n\nFortunately, the problem of\u00a0[clustered standard errors](https://en.wikipedia.org/wiki/Clustered_standard_errors)\u00a0has been extensively studied in the social sciences. When the inclusion of questions is non-independent, we recommend clustering standard errors on the unit of\u00a0[randomization](https://en.wikipedia.org/wiki/Randomization)\u00a0(for example, passage of text), and we provide applicable formulas in our paper.\n\n![](https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2Ff6f90f93dc66380904709a3ef4d63b92332871fd-2401x1260.png&w=3840&q=75)\n\nIf questions arrive in related clusters\u2014a common pattern in reading-comprehension evals\u2014eval scores will be more spread-out compared to the non-clustered case.\n\nIn practice, we have found that clustered standard errors on popular evals can be over three times as large as naive standard errors. Ignoring question clustering may lead researchers to inadvertently detect a difference in model capabilities when in fact none exists.\n\n### Recommendation #3: Reduce variance within questions\n\nVariance is a measurement of how spread-out a random variable is. The variance of an eval score is the square of the standard error of the mean, discussed above; this quantity depends on the amount of variance in the score on each individual eval question.\n\nA key insight of our paper is to decompose a model\u2019s score on a particular question into two terms that are added together:\n\n- The mean score (the average score that the model would achieve if asked the same question an infinite number of times\u2014even if the model might produce a different answer each time); and\n- A random component (the difference between a realized question score and the mean score for that question).\n\nThanks to the\u00a0[law of total variance](https://en.wikipedia.org/wiki/Law_of_total_variance), reducing the variance in the random component directly leads to a smaller standard error of the overall mean, and thus greater statistical precision. Our paper highlights two strategies for reducing variance in the random component depending on whether or not the model is asked to think step by step before answering (a prompting technique known as CoT, or chain-of-thought reasoning).\n\nIf an eval uses chain-of-thought reasoning, we recommend resampling answers from the same model several times, and using the question-level averages as the question scores fed into the Central Limit Theorem. We note that the\u00a0[Inspect framework](https://github.com/UKGovernmentBEIS/inspect_ai/)\u00a0correctly computes standard errors in this way via its\u00a0[_epochs_\u00a0parameter](https://inspect.ai-safety-institute.org.uk/scorers.html#sec-reducing-epochs).\n\n![](https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2Fefef59a06ddeb530fa15f31dc0937f28f70f655b-2401x1260.png&w=3840&q=75)\n\nIf a model produces answers non-deterministically, then generating (and grading) several answers per question will result in less spread-out eval scores.\n\nIf the eval does not use chain-of-thought reasoning (i.e., its answers are not \u201cpath dependent\u201d), we note that the random component in the score may often be eliminated altogether using next-token probabilities from the language model. For example, if the correct answer to a multiple-choice question is \u201cB\u201d, we would simply use the probability of the model producing the token \u201cB\u201d as the question score. We are not aware of an open-source evals framework which implements this technique.\n\n### Recommendation #4: Analyze paired differences\n\nEval scores don\u2019t have any meaning on their own; they only make sense in relation to one another (one model outperforms another model, or ties another model, or outperforms a person). But could a measured difference between two models be due to the specific choice of questions in the eval, and randomness in the models\u2019 answers? We can find out with a\u00a0[two-sample\u00a0_t_-test](https://en.wikipedia.org/wiki/Student%27s_t-test), using only the standard errors of the mean calculated from both eval scores.\n\nHowever, a two-sample test ignores the hidden structure inside eval data. Since the question list is shared across models, conducting a\u00a0[paired-differences test](https://en.wikipedia.org/wiki/Paired_difference_test)\u00a0lets us eliminate the variance in question difficulty and focus on the variance in responses. In our paper, we show how the result of a paired-differences test will be related to the\u00a0[Pearson correlation coefficient](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient)\u00a0between two models\u2019 question scores. When the correlation coefficient is higher, the standard error of the mean difference will be smaller.\n\nIn practice, we find the correlation of question scores on popular evals between frontier models to be substantial\u2014between 0.3 and 0.7 on a scale of \u22121 to +1. Put another way, frontier models have an overall tendency to get the same questions right and wrong. Paired-difference analysis thus represents a \u201cfree\u201d variance reduction technique that is very well suited for AI model evals. Therefore, in the interest of extracting the clearest signal from the data, our paper recommends reporting pairwise information\u2014mean differences, standard errors, confidence intervals, and correlations\u2014whenever two or more models are being compared.\n\n### Recommendation #5: Use power analysis\n\nThe flip side of the statistical significance coin is statistical power, which is the ability of a statistical test to detect a difference between two models, assuming such a difference exists. If an eval doesn\u2019t have very many questions, confidence intervals associated with any statistical tests will tend to be wide. This means that models will need to have a large underlying difference in capabilities in order to register a statistically significant result\u2014and that small differences will likely go undetected. Power analysis refers to the mathematical relationship between observation count,\u00a0[statistical power](https://en.wikipedia.org/wiki/Power_(statistics)), the\u00a0[false positive rate](https://en.wikipedia.org/wiki/False_positive_rate), and the\u00a0[effect size](https://en.wikipedia.org/wiki/Effect_size)\u00a0of interest.\n\nIn our paper, we show how to apply concepts from power analysis to evals. Specifically, we show researchers how to formulate a hypothesis (such as\u00a0_Model A outperforms Model B by 3 percentage points_) and calculate the number of questions that an eval should have in order to test this hypothesis against the null hypothesis (such as\u00a0_Model A and Model B are tied_).\n\nWe believe that power analysis will prove helpful to researchers in a number of situations. Our power formula will inform evaluators of models about the number of times to re-sample answers from questions (see Recommendation #3 above), as well as the number of questions that may be included in a random subsample while retaining the desired power properties. Researchers might use the power formula to conclude that an eval with a limited number of available questions is not worth running on a particular pair of models. Developers of new evals may wish to use the formula to help decide how many questions to include.\n\n### Conclusion\n\nStatistics is the science of measurement in the presence of noise. Evals present a number of practical\u00a0[challenges](https://www.anthropic.com/news/evaluating-ai-systems), and a true\u00a0[science of evals](https://www.apolloresearch.ai/blog/we-need-a-science-of-evals)\u00a0remains underdeveloped. Statistics can only form one aspect of a science of evals\u2014but a critical one, as an empirical science is only as good as its measuring tools. We hope that the recommendations in our paper\u00a0[**Adding Error Bars to Evals: A Statistical Approach to Language Model Evaluations**](https://arxiv.org/abs/2411.00640)\u00a0will help AI researchers calculate, interpret, and communicate eval numbers with greater precision and clarity than before\u2014and we encourage researchers in the AI community to explore other techniques from experiment design so that they\u00a0may understand more exactly all the things that they want to measure.\n\n  \n\u3002", "top": 0, "createdAt": 1752322219, "style": "", "script": "", "head": "", "ogImage": "https://github.githubassets.com/favicons/favicon.svg", "createdDate": "2025-07-12", "dateLabelColor": "#0969da"}, "P20": {"htmlDir": "docs/post/FLOAT.html", "labels": ["paper", "AI", "Source Code"], "postTitle": "FLOAT", "postUrl": "post/FLOAT.html", "postSourceUrl": "https://github.com/winnerineast/winnerineast.github.io/issues/20", "commentNum": 0, "wordCount": 6077, "description": "https://deepbrainai-research.github.io/float/\n### Generative Motion Latent Flow Matching for Audio-driven Talking Portrait\n\n[Taekyung Ki1](https://taekyungki.github.io/)\u00a0[Dongchan Min2](https://kevinmin95.github.io/)\u00a0[Gyeongsu Chae1](https://www.aistudios.com/ko/home)\n\n1DeepBrain AI Inc.\u00a02Graduate School of AI, KAIST\n\nArXiv 2024\n\n[Paper (Soon)](https://deepbrainai-research.github.io/float/)\u00a0[Video (Soon)](https://deepbrainai-research.github.io/float/)\u00a0[Code (Soon)](https://deepbrainai-research.github.io/float/)\n\n## Abstract\n\n![](https://deepbrainai-research.github.io/float/src/img/float-abstract.png)  \n\nWith the rapid advancement of diffusion-based generative models, portrait image animation has achieved remarkable results. However, it still faces challenges in temporally consistent video generation and fast sampling due to its iterative sampling nature. This paper presents FLOAT, an audio-driven talking portrait video generation method based on flow matching generative model. We shift the generative modeling from the pixel-based latent space to a learned motion latent space, enabling efficient design of temporally consistent motion. To achieve this, we introduce a transformer-based vector field predictor with a simple yet effective frame-wise conditioning mechanism. Additionally, our method supports speech-driven emotion enhancement, enabling a natural incorporation of expressive motions. Extensive experiments demonstrate that our method outperforms state-of-the-art audio-driven talking portrait methods in terms of visual quality, motion fidelity, and efficiency.\n\nTL;DR:\u00a0FLOAT is a flow matching based audio-driven talking portrait video generation method, which can enhance the speech-driven emotional motion.\n\n## Method Overview\n\n![Responsive image](https://deepbrainai-research.github.io/float/src/img/overview.png)\n\nAudio-driven talking portrait aims to synthesize talking portrait videos using a single source portrait image and driving audio. FLOAT is built upon a motion latent auto-encoder that encodes the given portrait image into an (identity-motion) latent representation. We generates audio-conditioned talking portrait motion latents through the flow matching (with optimal transport trajectories). To enhace the naturalness of generated talking motion, we incorporate the speech-driven emotion labels (\ud83d\ude00), providing a natural approach of emotion-aware talking portrait motion generation.\n\n## Results\n\n  \n\n#### Results with Out-of-distribution Datas\n\nFLOAT can generate realistic talking portrait videos using OOD portrait images and audio.\n\n  \n\n  \n\n  \n  \n\n#### Emotion Redirection\n\nSince FLOAT is trained with speech-driven emotion labels, it can re-direct the emotion of the talking portrait during the inference phase. Specifically, we can manipulate the predicted speech-driven emotion label with a simple one-hot emotion label, which can be further refined through classifier-free vector fields. This enables users to refine emotion even when the driving speech conveys ambiguious or mixed emotions.\n\n  \n\n  \n  \n\n#### Comparison with State-of-the-art Methods\n\nWe compare with state-of-the-art non-diffusion-based methods and diffusion-based methods. For non-diffusion-based methods, we choose SadTalker and EDTalk. For diffusion-based methods, we choose AniTalker, Hallo, and EchoMimic.\n\n[Previous](https://deepbrainai-research.github.io/float/#carouselExampleControls)[Next](https://deepbrainai-research.github.io/float/#carouselExampleControls)\n\n  \n  \n\n#### Ablation Studies on Frame-wise AdaLN and Flow Matching\n\nWe conduct ablation studies on frame-wise AdaLN (and gating) and flow matching. For frame-wise AdaLN (and gating), we compare it with cross-attention mechanism, which is widely used in conditional generation. For flow matching, we compare it with two types of diffusion models (\u03f5\u03f5-prediction and\u00a0x0x0-prediction). We observe that frame-wise AdaLN (and gating) can generate more diverse head motions than cross-attention. We also observe that flow mathcing can generate more temporally consistent videos with accurate lip-synchronization than diffusion models.\n\n[Previous](https://deepbrainai-research.github.io/float/#carouselExampleControls1)[Next](https://deepbrainai-research.github.io/float/#carouselExampleControls1)\n\n  \n  \n\n#### Different Number of Function Evaluations (NFEs)\n\nThe small number of function evaluations (NFEs) affects temporal consistency. This is because we generate the motion latents, not the content itself. FLOAT is capable of generating reasonable video results with approximately 10 NFEs.\n\n[Previous](https://deepbrainai-research.github.io/float/#carouselExampleControls2)[Next](https://deepbrainai-research.github.io/float/#carouselExampleControls2)\n\n  \n  \n\n#### Emotion Guidance Scales\n\nWe can control the intensity of the emotion by adjusting the emotion guidance scale. Note that the predicted speech-driven emotion label is\u00a0**Disgust**\u00a0with a 99.99% probability.\n\n  \n  \n\n#### Additional Driving Conditions\n\nWe also investigate another types of driving conditions, such as 3DMM head pose parameters, to improve the controllability and naturalness. Here, 3DPose, S2E, and I2E are 3DMM head pose parameters, Speech-to-emotion label, and Image-to-emotion label, resepctively.\n\n  \n\n  \n  \n\n#### Ablation Study on Facial Component Perceptual Loss in Phase 1\n\nThe proposed facial component perceptual loss for the motion latent auto-encoder significantly improves visual quality (e.g., teeth and eyes), as well as fine-grained motion (e.g., eyeball movement and lip motion).\n\n  \n\n## Citation\n\nIf you want to cite our work, please use:\n\n          TBA\n          \n      \n\n## Acknowledgement\n\nThe source images and audio are collected from the internet and other baselines, such as SadTalker, EMO, VASA-1, Hallo, LivePortrait, Loopy, and others. We appreciate their valuable contributions to this field. This project page is based on the project page of\u00a0[RegNeRF](https://m-niemeyer.github.io/regnerf). You can easily use it from\u00a0[the github repository](https://github.com/m-niemeyer/regnerf).\u3002", "top": 0, "createdAt": 1752322326, "style": "", "script": "", "head": "", "ogImage": "https://github.githubassets.com/favicons/favicon.svg", "createdDate": "2025-07-12", "dateLabelColor": "#0969da"}, "P21": {"htmlDir": "docs/post/Mitigating LLM Hallucinations - a multifaceted approach.html", "labels": ["paper", "AI", "Source Code"], "postTitle": "Mitigating LLM Hallucinations - a multifaceted approach", "postUrl": "post/Mitigating%20LLM%20Hallucinations%20-%20a%20multifaceted%20approach.html", "postSourceUrl": "https://github.com/winnerineast/winnerineast.github.io/issues/21", "commentNum": 1, "wordCount": 37935, "description": "\n(I recently turned this guide into a paper. You can find it\u00a0[here](https://amatria.in/blog/hallucination-paper))\n\n- [Introduction](https://amatria.in/blog/hallucinations#introduction)\n- [What we talk about when we talk about Hallucinations](https://amatria.in/blog/hallucinations#definition)\n- [How to Measure](https://amatria.in/blog/hallucinations#measurement)\n- [Mitigating Hallucinations: a multifacted approach](https://amatria.in/blog/hallucinations#mitigation)\n    - [Product design approaches](https://amatria.in/blog/hallucinations#product)\n    - [Prompt Engineering solutions](https://amatria.in/blog/hallucinations#promptengineering)\n    - [Grounding with RAG](https://amatria.in/blog/hallucinations#rag)\n    - [Advanced Prompt Engineering methods](https://amatria.in/blog/hallucinations#advancedprompting)\n    - [Model Choices](https://amatria.in/blog/hallucinations#modeling)\n    - [Reinforcement Learning from Human Feedback (RLHF)](https://amatria.in/blog/hallucinations#rlhf)\n    - [Domain adaptation through Fine-Tuning](https://amatria.in/blog/hallucinations#finetuning)\n- [Conclusion: Yann vs. Ilya](https://amatria.in/blog/hallucinations#conclusion)\n\n# Introduction[Permalink](https://amatria.in/blog/hallucinations#introduction 'Permalink')\n\nEver been curious about the complexities of integrating large language models (LLMs) into user-facing products? One challenge that has been gaining attention is the occurrence of \u2018hallucinations\u2019\u2014situations where these advanced AI systems produce misleading or incorrect information. This is a real-world issue that many of us in the tech industry are actively working to address as we develop new features and services.\n\nIn this blog post, you\u2019ll find a comprehensive guide to the most effective strategies for mitigating these hallucinations in user-facing products. The field is fast-evolving, so while I don\u2019t plan on continuously updating this post, I hope it serves as a valuable snapshot of current best practices. I\u2019m also open to your insights and ideas, so feel free to reach out with any suggestions or questions you might have.\n\n![](https://amatria.in/blog/images/106-0.png)\u00a0_A multifaceted approach to mitigating LLM hallucinations_\n\n# What we talk about when we talk about hallucinations[Permalink](https://amatria.in/blog/hallucinations#what-we-talk-about-when-we-talk-about-hallucinations 'Permalink')\n\nIn the context of Large Language Models (LLMs), the term \u201challucinations\u201d often surfaces. As defined by the \u201cSurvey of Hallucination in Natural Language Generation\u201d paper, a hallucination in an LLM refers to \u201cthe generation of content that is nonsensical or unfaithful to the provided source.\u201d\n\n## A Controversial Term: Unpacking the Use of \u201cHallucination\u201d in AI[Permalink](https://amatria.in/blog/hallucinations#a-controversial-term-unpacking-the-use-of-hallucination-in-ai 'Permalink')\n\nAccording to\u00a0[Wikipedia](https://en.wikipedia.org/wiki/Hallucination), a hallucination is defined as \u201ca perception in the absence of an external stimulus that has the qualities of a real perception.\u201d Such a description might evoke images of mysterious visions or imagined sounds. However, the term has taken on a different, though not uncontroversial, shade of meaning in the realm of artificial intelligence.\n\nThere are three main concerns I\u2019ve come across regarding the use of \u201challucination\u201d to describe phenomena in AI systems:\n\n- **Misattribution of Properties:**\u00a0The application of \u201challucination\u201d might inadvertently suggest that LLMs possess some form of consciousness or perception, which they certainly don\u2019t. LLMs generate text based on patterns in their training data, not because they \u201cperceive\u201d or \u201cimagine\u201d in the way living creatures do.\n- **Misunderstanding of Dynamics:**\u00a0Such terminology might cloud understanding about how LLMs function. They don\u2019t \u201csee\u201d or \u201cimagine.\u201d Instead, they churn out text based on statistical patterns from their training data.\n- **Ethical Implications:**\u00a0There\u2019s a fear that describing AI outputs as \u201challucinations\u201d trivializes the potential risks of LLMs providing incorrect or misleading information, especially if users over-rely on these models without proper fact-checking.\n\nHowever, the AI context for \u201challucination\u201d has even been acknowledged by dictionaries. For instance,\u00a0[Merriam-Webster](https://www.merriam-webster.com/dictionary/hallucination)\u00a0defines it in the context of AI as \u201ca plausible but false or misleading response generated by an artificial intelligence algorithm.\u201d\n\nInterestingly, this term isn\u2019t freshly minted. Andrej Karpathy\u00a0[suggested](https://twitter.com/karpathy/status/1702916988891193460)\u00a0that he might have popularized the term in his enlightening\u00a0[blog post](https://karpathy.github.io/2015/05/21/rnn-effectiveness/)\u00a0from 2015. But a little digging reveals earlier uses. Notably, an\u00a0[ACL conference paper](https://aclanthology.org/W14-1617.pdf)\u00a0from 2014 discussed \u201challucinating\u201d translations. Even further back, a 2009 paper titled\u00a0[\u201cReview Sentiment Scoring via a Parse-and-Paraphrase Paradigm\u201d](https://aclanthology.org/D09-1017.pdf)\u00a0used the term in the context of \u201challucinating\u201d topics. But perhaps the most ancient reference I found was in a 1996 paper,\u00a0[\u201cText Databases and Information Retrieval\u201d](https://dl.acm.org/doi/pdf/10.1145/234313.234371), which talked about systems that could \u201challucinate\u201d words not present in the original document.\n\nIn my view, it\u2019s clear that \u201challucination\u201d in AI has been in the lexicon for some time, carving out a niche meaning, distinct from its psychological roots.\n\nIt\u2019s also worth noting that AI is replete with terms borrowed from human analogies - take \u201cneural networks\u201d for instance. Despite initial reservations, these terms have become integral, largely uncontroversial components of AI discourse.\n\n## Types of Hallucinations[Permalink](https://amatria.in/blog/hallucinations#types-of-hallucinations 'Permalink')\n\nHallucinations can be categorized into two main types:\n\n- **Intrinsic Hallucinations:**\u00a0These directly contradict the source material, introducing factual inaccuracies or logical inconsistencies.\n- **Extrinsic Hallucinations:**\u00a0These do not contradict but also cannot be verified against the source, adding elements that could be considered speculative or unconfirmable.\n\n## The Nuanced Role of the \u2018Source\u2019[Permalink](https://amatria.in/blog/hallucinations#the-nuanced-role-of-the-source 'Permalink')\n\nThe concept of a \u2018source\u2019 varies depending on the specific task an LLM is performing. In dialogue-based tasks, the source can be considered as \u2018world knowledge.\u2019 However, when the task involves text summarization, the source is the input text itself. This is a critical nuance that significantly impacts both the evaluation and interpretation of hallucinations.\n\n## Contextual Importance of Hallucinations[Permalink](https://amatria.in/blog/hallucinations#contextual-importance-of-hallucinations 'Permalink')\n\nThe implications of hallucinations are highly context-dependent. For example, in creative applications such as poem-writing, the presence of hallucinations may not only be acceptable but could potentially enrich the output.\n\n## Why do LLMs hallucinate[Permalink](https://amatria.in/blog/hallucinations#why-do-llms-hallucinate 'Permalink')\n\nIt is important to first keep in mind that LLMs have been pre-trained to predict tokens. They do not have a notion of true/false or correct/incorrect, but rather base their text generation on probabilities. While that leads to some unexpected reasoning abilities (such as being able to pass the legal BAR exam or the medical USMLE), that is only a result of this probabilistic token by token reasoning. To be fair, the additional training steps of instruct tuning and RLHF that most modern LLMs have do introduce a bit more \u201cbias towards factuality\u201d, but they do not change the overal underlying mechanism and its pitfalls.\n\nLLMs have been trained on the whole internet, book collections, question/answers, and Wikipedia, among many others. They have good and not-so-good knowledge in their training set. Their responses are biased towards whatever they have seen the most. If you ask an LLM a medical question and you are not careful on how you prompt it, you might get an answer that is mostly aligned to the best medical literature or to random Reddit threads.\n\nIn a recent paper entitled\u00a0[\u201cSources of Hallucination by Large Language Models on Inference Tasks\u201d](https://arxiv.org/abs/2305.14552), the authors show how hallucinations are originated by two aspects of the LLM\u2019s training dataset: veracity prior and the relative frequency heuristic.\n\n# How to Measure Hallucinations in Large Language Models[Permalink](https://amatria.in/blog/hallucinations#how-to-measure-hallucinations-in-large-language-models 'Permalink')\n\nUnderstanding hallucinations is one thing, but quantifying them? That\u2019s where things get really interesting. Quantitative metrics are essential for assessing the effectiveness of mitigation strategies. In this section, I\u2019ll guide you through the recommended methodologies for measuring hallucinations.\n\n## A Five-Step Approach to Quantitative Measurement[Permalink](https://amatria.in/blog/hallucinations#a-five-step-approach-to-quantitative-measurement 'Permalink')\n\nBased on best practices in the field, here\u2019s a systematic five-step approach to accurately measure hallucinations:\n\n**1. Identify Grounding Data:**\u00a0Grounding data serves as the benchmark for what the LLM should produce. The choice of grounding data varies by use-case. For instance, actual resumes could serve as grounding data when generating resume-related information. On the other hand, search engine results could be used for web-based queries.\n\n**2. Create Measurement Test Sets:**\u00a0These sets usually consist of input/output pairs and may include human-LLM conversations, depending on the application. Ideally, you\u2019d have at least two types of test sets: * A generic or random test set * An adversarial test set, generated from red-teaming exercises to include challenging or high-risk edge cases.\n\n**3. Extract Claims:**\u00a0After preparing the test sets, the next step is to extract claims made by the LLM. This can be done manually, through rule-based methods, or even using machine learning models. Each technique has its pros and cons, which we will explore in detail.\n\n**4. Validate Against Grounding Data:**\u00a0Validation ensures that the LLM\u2019s generated content aligns with the grounding data. This step often mirrors the extraction methods used previously.\n\n**5. Report Metrics:**\u00a0The \u201cGrounding Defect Rate\u201d is a fundamental metric that quantifies the ratio of ungrounded responses to the total number of generated outputs. Additional metrics will be discussed later for a more nuanced evaluation.\n\n## Evaluating Hallucinations: Common Metrics and Methodologies[Permalink](https://amatria.in/blog/hallucinations#evaluating-hallucinations-common-metrics-and-methodologies 'Permalink')\n\nQuantifying hallucinations in Large Language Models isn\u2019t just about recognizing that they exist\u2014it\u2019s about measuring them rigorously. In this section, I\u2019ll delve into the different types of metrics commonly employed for this purpose.\n\n### Statistical Metrics[Permalink](https://amatria.in/blog/hallucinations#statistical-metrics 'Permalink')\n\nMetrics like ROUGE and BLEU are often the go-to choices for text similarity evaluations. They focus on the intrinsic type of hallucinations by comparing the generated output against a source. Advanced metrics such as PARENT, PARENT-T, and Knowledge F1 come into play when a structured knowledge source is available. However, these metrics have limitations: they primarily focus on intrinsic hallucinations and can falter when capturing syntactic and semantic nuances.\n\n### Model-Based Metrics[Permalink](https://amatria.in/blog/hallucinations#model-based-metrics 'Permalink')\n\nModel-based metrics leverage neural networks, making them more adaptable to syntactic and semantic complexities. They come in various flavors:\n\n**IE-based Metrics:**\u00a0These use Information Extraction (IE) models to distill the knowledge into a simpler relational tuple format\u2014think subject, relation, object. The model then validates these tuples against those extracted from the source or reference.\n\n- **QA-based Metrics:**\u00a0These implicitly measure the overlap or consistency between the generated content and the source. If the content is factually consistent with the source, similar answers will be generated to the same questions. (see e.g. \u201cEvaluating Factual Consistency in Knowledge-Grounded Dialogues via Question Generation and Question Answering\u201d)\n- **NLI-based Metrics:**\u00a0Utilizing Natural Language Inference (NLI) datasets, these metrics determine if a generated \u201chypothesis\u201d is true, false, or undetermined given a \u201cpremise\u201d.(see e.g. \u201cEvaluating Groundedness in Dialogue Systems: The BEGIN Benchmark\u201d).\n- **Faithfulness Classification Metrics:**\u00a0These improve upon NLI-based metrics by creating task-specific datasets, thereby providing a more nuanced evaluation. . (see e.g. \u201cRome was built in 1776: A Case Study on Factual Correctness in Knowledge-Grounded Response Generation\u201d).\n\n### The Role of Human Evaluation[Permalink](https://amatria.in/blog/hallucinations#the-role-of-human-evaluation 'Permalink')\n\nDespite the sophistication of automated metrics, human evaluation still holds significant value. Two primary approaches are commonly employed:\n\n1. **Scoring:**\u00a0Human annotators assign scores within a defined range to rate the level of hallucination.\n2. **Comparing:**\u00a0Here, human annotators evaluate the generated content against baselines or ground-truth references, providing an additional layer of validation.\n\n### The example of FActScore[Permalink](https://amatria.in/blog/hallucinations#the-example-of-factscore 'Permalink')\n\n[FActScore](https://arxiv.org/abs/2305.14251)\u00a0is a recent example of a metric that can be used both for human and model-based evaluation. The metric breaks an LLM generation into \u201catomic facts\u201d. The final score is computed as the sum of the accuracy of each atomic fact, giving each of them equal weight. Accuracy is a binary number that simply states whether the atomic fact is supported by the source. The authors implement different automation strategies that use LLMs to estimate this metric.\n\n![](https://amatria.in/blog/images/106-9.png)\n\n# The Art of Red Teaming: Best Practices for Stress-Testing LLMs[Permalink](https://amatria.in/blog/hallucinations#the-art-of-red-teaming-best-practices-for-stress-testing-llms 'Permalink')\n\nWhile statistical and model-based metrics are indispensable for measuring hallucinations in LLMs, it\u2019s equally important to put these models through the rigor of human evaluation. Red teaming provides an essential layer of scrutiny that complements systematic measurement. Here are some best practices to follow:\n\n**Keep Red Teaming Complementary:**\u00a0Although red teaming and stress-testing are invaluable tools, they should not replace systematic measurement. They are meant to augment, not substitute.\n\n**Test in Real-world Conditions:**\u00a0Whenever possible, conduct your testing on the production endpoint. This allows for a more realistic assessment of how the model behaves under actual conditions.\n\n**Define Harms and Guidelines:**\u00a0Clearly outline the potential harms and provide specific guidelines to the testers. This ensures that everyone is aligned on what to look for during testing.\n\n**Prioritize Your Focus Areas:**\u00a0Identify the key features, harms, and scenarios that should be prioritized in the red teaming exercise. This focused approach yields more actionable insights.\n\n**Diverse and Skilled Testers:**\u00a0A diverse set of testers with different areas of expertise can provide a multi-faceted evaluation. Diversity here can mean different domains of knowledge, different cultural backgrounds, or even different biases.\n\n**Documentation is Key:**\u00a0Decide in advance what kinds of data or findings you\u2019d like your testers to document. Clear documentation aids in a more structured evaluation process.\n\n**Manage Tester Time and Well-being:**\u00a0Determine how much time each tester should ideally dedicate to the task. Moreover, be cognizant of potential burnout or a decline in creativity over time, and plan accordingly.\n\nNew approaches to red teaming include using an LLM to read team another LLM. See e.g. Deepmind\u2019s\u00a0[\u201cRed Teaming Language Models with Language Models\u201d](https://arxiv.org/abs/2202.03286)\n\n# Mitigating Hallucinations in Large Language Models: A Multifaceted Approach[Permalink](https://amatria.in/blog/hallucinations#mitigating-hallucinations-in-large-language-models-a-multifaceted-approach 'Permalink')\n\nThe road to minimizing hallucinations is paved with both challenges and opportunities. In this section, we\u2019ll explore various mitigation strategies that can be customized to fit the unique demands of different applications of large language models.\n\n## Leverage Product Design to Minimize Impact[Permalink](https://amatria.in/blog/hallucinations#leverage-product-design-to-minimize-impact 'Permalink')\n\nThe first piece of advice is straightforward: if possible, design your use case in such a way that hallucinations become a non-issue. For instance, in applications that generate written content, focusing on opinion pieces rather than factual articles may naturally lower the risk of problematic hallucinations.\n\n### Product-Level Recommendations[Permalink](https://amatria.in/blog/hallucinations#product-level-recommendations 'Permalink')\n\n- **User Editability:**\u00a0Allow users to edit AI-generated outputs. This not only adds an extra layer of scrutiny but also improves the overall reliability of the content.\n- **User Responsibility:**\u00a0Make it clear that users are ultimately responsible for the content that is generated and published.\n- **Citations and References:**\u00a0Enabling a feature that incorporates citations can serve as a safety net, helping users verify the information before disseminating it.\n- **User Optionality:**\u00a0Offer various operational modes, such as a \u201cprecision\u201d mode that uses a more accurate (but computationally expensive) model.\n- **User Feedback:**\u00a0Implement a feedback mechanism where users can flag generated content as inaccurate, harmful, or incomplete. This data can be invaluable for refining the model in future iterations.\n- **Limit Output and Turns:**\u00a0Be mindful of the length and complexity of generated responses, as longer and more complex outputs have a higher chance of producing hallucinations.\n- **Structured Input/Output:**\u00a0Consider using structured fields instead of free-form text to lower the risk of hallucinations. For example, if the application involves resume generation, predefined fields for educational background, work experience, and skills could be beneficial.\n\n### Data Practices for Continuous Improvement[Permalink](https://amatria.in/blog/hallucinations#data-practices-for-continuous-improvement 'Permalink')\n\n- **Maintain a Tracking Set:**\u00a0A dynamic database should be maintained to log different types of hallucinations along with the necessary information to reproduce them. This can serve as a powerful tool for regression testing.\n- **Privacy and Trust:**\u00a0Given that the tracking set may contain sensitive data, adhere to best practices for data privacy and security.\n\n## Prompt Engineering: Mastering the Art of Metaprompt Design[Permalink](https://amatria.in/blog/hallucinations#prompt-engineering-mastering-the-art-of-metaprompt-design 'Permalink')\n\nAlthough large language models (LLMs) have come a long way, they are not yet perfect\u2014especially when it comes to grounding their responses. That\u2019s why understanding and effectively utilizing metaprompts can make a world of difference. A study revealed that simply instructing the LLM on what not to do could lower hallucination rates dramatically. Even better, guiding the model towards alternative actions slashed these rates further.\n\n## General Guidelines to Curb Hallucinations[Permalink](https://amatria.in/blog/hallucinations#general-guidelines-to-curb-hallucinations 'Permalink')\n\n- **Simplify Complex Tasks:**\u00a0Break down intricate actions into simpler steps.\n- **Harness Affordances:**\u00a0Utilize built-in functions within your metaprompt.\n- **Use Few-Shot Learning:**\u00a0Include examples when you can.\n- **Iterative Refinement:**\u00a0Don\u2019t hesitate to tweak the model\u2019s output.\n\nOne important thing to note is that while these techniques improve grounding, they also come at a computational cost. Anyone leveraging LLMs in product design will need to balance this trade-off carefully.\n\n## Fine-Tuning Your Metaprompts[Permalink](https://amatria.in/blog/hallucinations#fine-tuning-your-metaprompts 'Permalink')\n\n- **Assertive Tone:**\u00a0Using ALL CAPS and highlighting certain directives can improve model compliance.\n- **Context is King:**\u00a0Providing more background information can better ground the model.\n- **Refinement Steps:**\u00a0Reevaluate the initial output and make necessary adjustments.\n- **Inline Citations:**\u00a0Ask the model to substantiate its claims.\n- **Framing:**\u00a0Approaching tasks as summarization often yields more grounded results compared to question-answering.\n- **Selective Grounding:**\u00a0Ascertain scenarios where grounding is a must versus where it may be optional.\n- **Reiterate Key Points:**\u00a0Repeating essential instructions at the end of the prompt can underline their importance.\n- **Echoing Input:**\u00a0Request the model to recap vital input details, ensuring alignment with the source data.\n- **Algorithmic Filtering:**\u00a0Utilize algorithms to sift through and prioritize the most relevant information.\n\nIn upcoming sections, we\u2019ll dissect advanced metaprompting techniques, such as the \u201cchain of thought\u201d approach, and delve into how Retrieval-Augmented Generation (RAG) can be leveraged for better grounding.\n\n## Chain of Thought[Permalink](https://amatria.in/blog/hallucinations#chain-of-thought 'Permalink')\n\nChain of thought was initially described in the\u00a0[\u201cChain-of-Thought Prompting Elicits Reasoning in Large Language Models\u201d](https://arxiv.org/abs/2201.11903)\u00a0paper by Google researchers. The simple idea here is that given that LLMs have been trained to predict tokens and not explicitly reason, you can get them closer to reasoning if you specify those required reasoning steps. Here is a simple example from the original paper:\n\n![](https://amatria.in/blog/images/106-1.png)\n\nNote that in this case the \u201crequired reasoning steps\u201d are given in the example in blue. This is the so-called \u201cManual CoT\u201d. There are in fact two ways of doing basic chain of thought prompting (see below). In the basic one, called zero-shot CoT, you simply ask the LLM to \u201cthink step by step\u201d. In the more complex version, called \u201cmanual CoT\u201d you have to give the LLM examples of thinking step by step to illustrate how to reason. Manual prompting is more effective, but harder to scale and maintain.\n\n![](https://amatria.in/blog/images/106-2.png)\n\nCoT is just a more structured approach to the \u201csimplify complex tasks\u201d generic recommendation above and is known to mitigate hallucinations in many situations.\n\n## Grounding with RAG[Permalink](https://amatria.in/blog/hallucinations#grounding-with-rag 'Permalink')\n\nRetrieval-Augmented Generation, commonly known as RAG, is a technique aimed at augmenting the capabilities of Large Language Models (LLMs). Initially\u00a0[presented](https://ai.facebook.com/blog/retrieval-augmented-generation-streamlining-the-creation-of-intelligent-natural-language-processing-models/)\u00a0by Facebook in 2020 in the context of their BART model, RAG has since been incorporated as a feature in the\u00a0[Hugging Face library](https://huggingface.co/docs/transformers/model_doc/rag).\n\n### The Core Concept[Permalink](https://amatria.in/blog/hallucinations#the-core-concept 'Permalink')\n\nThe fundamental idea behind RAG is straightforward: it merges a retrieval component with a generative component, allowing the two to complement each other. This process is visually explained in the diagram below, extracted from the original research paper.\n\n![](https://amatria.in/blog/images/106-3.png)\n\nBy combining these two elements, RAG enables the LLM to access and incorporate external information, thereby grounding the generated content more effectively. The retrieval component fetches relevant data, while the generative aspect of the model synthesizes this data into coherent and contextually appropriate responses.\n\nRAG has evolved to become an indispensable part of the prompt engineer\u2019s toolkit. Over time, it has expanded into more complex applications, effectively serving as a concrete example within the broader framework of Toolkits, where the \u201ctool\u201d is typically a straightforward retriever or query engine.\n\nBecause RAG\u00a0**grounds**\u00a0the response to the LLM to external data, it is known to be a very effective technique to mitigate hallucinations. However, there are some caveats.\n\n### RAG known caveats and guardrails[Permalink](https://amatria.in/blog/hallucinations#rag-known-caveats-and-guardrails 'Permalink')\n\n**The Pitfall of Over-Reliance**. One significant drawback of using RAG is a pronounced over-reliance on the retrieval results, which can, in certain cases, lead to hallucinations. It\u2019s crucial to understand that retrieval might produce results that are either empty, incorrect, or require further disambiguation. Below are strategies to handle each of these scenarios.\n\n**Empty Results: Be Prepared for Voids**. When the retrieval engine returns empty results, it could either be due to a lack of relevant data in the document source or an incorrect query formulation. Meta-prompts should be designed to anticipate and guard against this scenario. If the retrieval engine returns no results, the system should opt for caution and decline to answer, stating something along the lines of, \u201cSorry, we don\u2019t have enough information on this topic. Could you please rephrase your question?\u201d More advanced strategies might involve internally reformulating the query to handle issues like user misspellings, which can lead to void results.\n\n**Ambiguous Results: Seek Clarification**. For ambiguous queries such as \u201cWhat is a good restaurant in Portland?\u201d, where Portland could refer to multiple locations, it\u2019s advisable to seek further clarification from the user. For example, \u201cDid you mean Portland, OR, or Portland, ME?\u201d\n\n**Wrong Results: Navigate Carefully**. Incorrect retrieval results are particularly challenging to address because they are difficult to identify without an external ground truth. While improving the accuracy of retrieval engines is a complex problem that\u2019s beyond the scope of this document, we recommend analyzing the performance of your retrieval solution within your application\u2019s specific use cases. Design your prompts to be extra cautious in areas where the retrieval engine has been identified to be less accurate.\n\n## Advanced Prompt Engineering methods[Permalink](https://amatria.in/blog/hallucinations#advanced-prompt-engineering-methods 'Permalink')\n\nOver the past few months, significant efforts have been directed towards mitigating the issues of hallucinations and grounding in Large Language Models (LLMs). These endeavors have led to a variety of innovative approaches that tackle the problem from a prompt engineering perspective. It\u2019s important to note that these advanced methods are distinctly different from the more straightforward \u201cdesign tricks\u201d discussed earlier. I will give a few examples of advanced prompt engineering methods that are relevant in the context of preventing hallucination. If you are interested in a more comprehensive catalog, check my previous post\u00a0[\u201cPrompt Engineering 201: Advanced methods and toolkits\u201d](https://amatriain.net/blog/prompt201)\n\n**Complexity, Latency, and Cost**: Advanced prompt engineering techniques often introduce additional complexity, latency, and cost, primarily because they frequently involve making multiple calls to the LLM. However, it\u2019s crucial to grasp their functionality and to have these advanced methods in your prompt engineering toolbox.\n\n**Trade-offs and Opportunities**: In some cases, the incremental costs and latency might be justifiable, given the improvement in grounding and reduction in hallucinations. Additionally, you may find opportunities to implement some of these advanced methods using smaller, more cost-effective models. This could offer a valuable compromise between performance and expense.\n\nBy understanding these advanced prompt engineering methods, you can make more informed decisions about when and how to apply them, and whether their benefits outweigh their costs for your specific application.\n\n### Self-consistency[Permalink](https://amatria.in/blog/hallucinations#self-consistency 'Permalink')\n\nSelf consistency, introduced in the paper\u00a0[\u201cSelfCheckGPT: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models\u201d](https://arxiv.org/abs/2303.08896), is a method to use an LLM to fact-check itself. The idea is a simple ensemble-based approach where the LLM is asked to generate several responses to the same prompt. The consistency between those responses indicates how accurate the response may be.\n\n![](https://amatria.in/blog/images/106-4.png)\n\nThe diagram above illustrates the approach in a QA scenario. In this case, the \u201cconsistency\u201d is measured by the number of answers to passages that agree with the overall answer. However, the authors introduce two other measures of consistency (BERT-scores, and n-gram), and a fourth one that combines the three.\n\n### Reason and act (React)[Permalink](https://amatria.in/blog/hallucinations#reason-and-act-react 'Permalink')\n\nReact is a specific approach to designing agents introduced by Google in\u00a0[\u201cReAct: Synergizing Reasoning and Acting in Language Models\u201d](https://www.promptingguide.ai/techniques/react). This method prompts the LLM to generate both verbal reasoning traces and actions in an interleaved manner, which allows the model to perform dynamic reasoning. Importantly, the authors find that the React approach reduces hallucination from CoT. However, this increase in groundedness and trustworthiness, also comes at the cost of slightly reduced flexibility in reasoning steps (see the paper for more details).\n\n![](https://amatria.in/blog/images/106-5.png)\n\n### Reflection[Permalink](https://amatria.in/blog/hallucinations#reflection 'Permalink')\n\nIn the Self-consistency approach we saw how LLMs can be used to infer the confidence in a response. In that approach, confidence is measured as a by-product of how similar several responses to the same question are. Reflection goes a step further and tries to answer the question of whether (or how) we can ask an LLM directly about the confidence in its response. As\u00a0[Eric Jang puts it](https://evjang.com/2023/03/26/self-reflection.html), there is \u201csome preliminary evidence that GPT-4 possesses some ability to edit own prior generations based on reasoning whether their output makes sense\u201d.\n\nThe Reflexion\u00a0[paper](https://arxiv.org/abs/2303.11366)\u00a0proposes an approach defined as \u201creinforcement via verbal reflection\u201d with different components. The actor, an LLM itself, produces a trajectory (hypothesis). The evaluator produces a score on how good that hypothesis is. The self reflection component produces a summary that is stored in memory. The process is repeated iteratively until the Evaluator decides it has a \u201cgood enough\u201d answer. The authors show through experiments how reflection greatly improves the ability of detecting hallucinations even when compared to a ReAct agent.\n\n![](https://amatria.in/blog/images/106-6.png)\n\n### Dialog-Enabled Resolving Agents (DERA)[Permalink](https://amatria.in/blog/hallucinations#dialog-enabled-resolving-agents-dera 'Permalink')\n\n[DERA](https://arxiv.org/abs/2303.17071), developed by my former team at Curai Health for their specific healthcare approach, defines different agents that, in the context of a dialog, take different roles. In the case of high stakes situations like a medical conversation, it pays off to define a set of \u201cResearchers\u201d and a \u201cDecider\u201d. The main difference here is that the Researchers operate in parallel vs. the Reflexion Actors that operate sequentially only if the Evaluator decides.\n\n![](https://amatria.in/blog/images/106-7.png)\n\n### Chain-of-Verification (COVE)[Permalink](https://amatria.in/blog/hallucinations#chain-of-verification-cove 'Permalink')\n\n[COVE](https://arxiv.org/abs/2309.11495), recently presented by Meta, presents yet another variation on using different instances of the LLM to produce several responses and self-validate. In their approach, illustrated in the figure below, the model first (i) drafts an initial response; then (ii) plans verification questions to fact-check its draft; (iii) answers those questions independently so the answers are not biased by other responses; and (iv) generates its final verified response.\n\n![](https://amatria.in/blog/images/106-8.png)\n\n### Rails[Permalink](https://amatria.in/blog/hallucinations#rails 'Permalink')\n\nA\u00a0[rail](https://github.com/NVIDIA/NeMo-Guardrails/blob/main/docs/README.md)\u00a0is simply a programmable way to control the output of an LLM. Rails are specified using Colang, a simple modeling language, and Canonical Forms, templates to standardize natural language sentences (see\u00a0[here](https://github.com/NVIDIA/NeMo-Guardrails/blob/main/docs/getting_started/hello-world.md))\n\nUsing rails, one can implement ways to have the LLM behave in a particular way. Of particular interest to our discussion, there is a rail to minimize hallucination (Fact checking rail).\n\n### Guidance (Constrained Prompting)[Permalink](https://amatria.in/blog/hallucinations#guidance-constrained-prompting 'Permalink')\n\n\u201cConstrained Prompting\u201d is a term recently\u00a0[introduced](https://youtu.be/bZQun8Y4L2A?t=2093)\u00a0by Andrej Karpathy to describe approaches and languages that allow us to interleave generation, prompting, and logical control in an LLM flow.\n\n[Guidance](https://github.com/microsoft/guidance)\u00a0is the only example of such an approach that I know although one could argue that React is also a constrained prompting approach. The tool is not so much a prompting approach but rather a \u201cprompting language\u201d. Using guidance templates, you can pretty much implement most if not all the approaches in this post. Guidance uses a syntax based on\u00a0[Handlebars](https://handlebarsjs.com/)\u00a0that allows to interleave prompting and generation, as well as manage logical control flow and variables. Because Guidance programs are declared in the exact linear order that they will be executed, the LLM can, at any point, be used to generate text or make logical decisions.\n\n## Model Choices for Mitigating Hallucinations[Permalink](https://amatria.in/blog/hallucinations#model-choices-for-mitigating-hallucinations 'Permalink')\n\n### Size and Model Complexity as a General Heuristic[Permalink](https://amatria.in/blog/hallucinations#size-and-model-complexity-as-a-general-heuristic 'Permalink')\n\nA well-accepted guideline within the field suggests that larger, more complex models typically offer superior grounding capabilities. For example, empirical evaluations have shown that GPT-4 substantially outperforms its predecessor, GPT-3.5, in reducing the occurrence of hallucinations.\n\n### The Significance of Model Temperature[Permalink](https://amatria.in/blog/hallucinations#the-significance-of-model-temperature 'Permalink')\n\nModel temperature serves as a critical hyperparameter that influences the stochastic behavior of the model\u2019s output. In a nutshell, it determines the level of randomness when predicting subsequent tokens. Higher temperatures increase the selection probabilities for tokens that are less likely, making the model\u2019s output more diverse but potentially less grounded. Conversely, a lower temperature, approaching zero, results in the model sticking more closely to high-probability tokens, generally yielding more reliable and grounded outputs.\n\n### Leveraging Reinforcement Learning from Human Feedback (RLHF)[Permalink](https://amatria.in/blog/hallucinations#leveraging-reinforcement-learning-from-human-feedback-rlhf 'Permalink')\n\nRLHF methods can be applied during the later stages of training to optimize for more accurate and grounded outputs. These methods have shown marked improvements in hallucination mitigation, especially for models that have undergone domain-specific fine-tuning.\n\n### Domain adaptation through Fine-Tuning[Permalink](https://amatria.in/blog/hallucinations#domain-adaptation-through-fine-tuning 'Permalink')\n\nLastly, if you\u2019re developing for a specific application, you might want to consider fine-tuning your internal models. Fine-tuning to your own data and examples can make a world of difference in grounding your outputs and minimizing those pesky hallucinations, particularly if you want to use a smaller and more efficient LLM. As of this writing, OpenAI offers fine-tuning for GPT-3.5 Turbo and acknowledges that in some applications this can yield better results than using the much larger and expensive GPT-4.\n\n# Conclusion[Permalink](https://amatria.in/blog/hallucinations#conclusion 'Permalink')\n\nAs we have seen in this discussion of hallucinations, the problem is not an easy one to solve. In fact, Yann Lecun argues that\u00a0[it cannot be solved](https://spectrum.ieee.org/ai-hallucination)\u00a0without a complete redesign of the underlying models (although Ilya Sutskever disagrees). I stand somewhere in between: with the current underlying technology, hallucinations are just an expected side-effect and are hard to completely rule out. However, a combination of techniques can mitigate them and make them completely acceptable for most if not all use cases. After all, as I explained in\u00a0[a previous blog post](https://amatriain.net/blog/llmsdoctors), even medical doctors hallucinate!\u3002", "top": 0, "createdAt": 1752322392, "style": "", "script": "", "head": "", "ogImage": "https://github.githubassets.com/favicons/favicon.svg", "createdDate": "2025-07-12", "dateLabelColor": "#0969da"}, "P22": {"htmlDir": "docs/post/tong-zhi-ruan-jian-kai-fa-de-zhu-ming-ding-lv.html", "labels": ["software"], "postTitle": "\u7edf\u6cbb\u8f6f\u4ef6\u5f00\u53d1\u7684\u8457\u540d\u5b9a\u5f8b", "postUrl": "post/tong-zhi-ruan-jian-kai-fa-de-zhu-ming-ding-lv.html", "postSourceUrl": "https://github.com/winnerineast/winnerineast.github.io/issues/22", "commentNum": 0, "wordCount": 3149, "description": "\n\u539f\u6587\uff1ahttps://www.timsommer.be/famous-laws-of-software-development/\n\n\u7ffb\u8bd1| \u7801\u519c\u7ffb\u8eab\n\n\u548c\u5176\u4ed6\u9886\u57df\u4e00\u6837\uff0c\u5728\u8f6f\u4ef6\u5f00\u53d1\u7684\u4e16\u754c\u4e2d\u4e5f\u6709\u4e00\u4e9b\u6709\u8da3\u800c\u8457\u540d\u7684\u5b9a\u5f8b\uff0c\u5f00\u53d1\u4eba\u5458\u3001\u7ba1\u7406\u4eba\u5458\u8fd8\u662f\u67b6\u6784\u5e08\uff0c\u90fd\u7ecf\u5e38\u5728\u4f1a\u8bae\u6216\u95f2\u8c08\u4e2d\u63d0\u5230\u4ed6\u4eec\uff0c\u5f88\u591a\u65f6\u5019\u6211\u4eec\u90fd\u53ea\u662f\u70b9\u5934\u9644\u548c\uff0c\u514d\u5f97\u8ba9\u4eba\u77e5\u9053\u81ea\u5df1\u5176\u5b9e\u6839\u672c\u6ca1\u542c\u8bf4\u8fc7\u5e03\u9c81\u514b\u65af(Brooks)\u3001\u6469\u5c14(Moore)\u6216\u5eb7\u5a01(Conway)\u8fd9\u4e9b\u5927\u4f6c\u3002", "top": 0, "createdAt": 1752322425, "style": "", "script": "", "head": "", "ogImage": "https://github.githubassets.com/favicons/favicon.svg", "createdDate": "2025-07-12", "dateLabelColor": "#0969da"}, "P23": {"htmlDir": "docs/post/2024-nian- AI Agents -ji-shu-zhan-quan-mian-jie-xi.html", "labels": ["AI"], "postTitle": "2024\u5e74 AI Agents \u6280\u672f\u6808\u5168\u9762\u89e3\u6790", "postUrl": "post/2024-nian-%20AI%20Agents%20-ji-shu-zhan-quan-mian-jie-xi.html", "postSourceUrl": "https://github.com/winnerineast/winnerineast.github.io/issues/23", "commentNum": 0, "wordCount": 8102, "description": "Heisenberg \u4eba\u5de5\u4e16\u754cArtificial World\n\n![profile_qrcode](https://mp.weixin.qq.com/mp/qrcode?scene=10000005&size=102&__biz=MzkxMjY3MDMxMQ==&mid=2247484600&idx=1&sn=d24556d20e9dd8878b0239ce98741f0f&send_time=)\n\n\u4eba\u5de5\u4e16\u754cArtificial World\n\n\u63ed\u793a\u9690\u85cf\u53d8\u91cf\uff0c\u5904\u4e8eAGI\u7684\u8fb9\u7f18\u3002", "top": 0, "createdAt": 1752322451, "style": "", "script": "", "head": "", "ogImage": "https://github.githubassets.com/favicons/favicon.svg", "createdDate": "2025-07-12", "dateLabelColor": "#0969da"}, "P24": {"htmlDir": "docs/post/DeepSearcher-kai-yuan-\uff1a-gao-bie-chuan-tong-RAG\uff0c-si-you-shu-ju-+Deepseek\uff0c-da-zao-ben-di-ban-Deep Research.html", "labels": ["AI"], "postTitle": "DeepSearcher\u5f00\u6e90\uff1a\u544a\u522b\u4f20\u7edfRAG\uff0c\u79c1\u6709\u6570\u636e+Deepseek\uff0c\u6253\u9020\u672c\u5730\u7248Deep Research", "postUrl": "post/DeepSearcher-kai-yuan-%EF%BC%9A-gao-bie-chuan-tong-RAG%EF%BC%8C-si-you-shu-ju-%2BDeepseek%EF%BC%8C-da-zao-ben-di-ban-Deep%20Research.html", "postSourceUrl": "https://github.com/winnerineast/winnerineast.github.io/issues/24", "commentNum": 0, "wordCount": 27013, "description": "\n\u539f\u521b\u00a0\u548c\u4f60\u4e00\u8d77\u8fdb\u6b65\u7684\u00a0Zilliz\n\n\u00a0_2025\u5e7402\u670813\u65e5 19:06_\n\n![\u56fe\u7247](https://mmbiz.qpic.cn/mmbiz_png/MqgA8Ylgeh69MGal1BGtDcKwvTTfp5d0DHtYLLGMqYNKG5qwPU0VWeyZibj6YxULOqb3M2heiaUflhftAyLmniciag/640?wx_fmt=other&wxfrom=5&wx_lazy=1&wx_co=1&tp=wxpic)\n\n![\u56fe\u7247](https://mmbiz.qpic.cn/mmbiz_jpg/MqgA8Ylgeh5IGBMTYqBrAcxy5ZYQ8PyPustqwVcWc7JZ44y2ibSqYia0apqCylI22ia3goM9kmP8CHvwef4CgOLicw/640?wx_fmt=jpeg&tp=wxpic&wxfrom=5&wx_lazy=1&wx_co=1)\n\n# **\u524d\u8a00**\n\n\u8fd1\u65e5\uff0cOpen AI\u7684Deep Research\uff08\u6df1\u5ea6\u7814\u7a76\uff09\u529f\u80fd\u4e00\u7ecf\u63a8\u51fa\uff0c\u8fc5\u901f\u53d7\u5230\u8bf8\u591a\u5173\u6ce8\uff0c\u901a\u8fc7\u5c06\u5927\u6a21\u578b+\u8d85\u7ea7\u641c\u7d22+\u7814\u7a76\u52a9\u7406\u7684\u4e09\u5408\u4e00\uff0c\u91d1\u878d\u673a\u6784\u4e00\u952e\u751f\u6210\u62a5\u544a\u3001\u79d1\u7814\u515a\u4e00\u952e\u751f\u6210\u7efc\u8ff0\u6210\u4e3a\u53ef\u80fd\u3002", "top": 0, "createdAt": 1752322488, "style": "", "script": "", "head": "", "ogImage": "https://github.githubassets.com/favicons/favicon.svg", "createdDate": "2025-07-12", "dateLabelColor": "#0969da"}, "P25": {"htmlDir": "docs/post/kotaemon-he-xin-GraphRAG\u3001Agent\u3001-duo-mo-tai-dai-ma-jie-du-\uff01.html", "labels": ["AI", "Source Code"], "postTitle": "kotaemon\u6838\u5fc3GraphRAG\u3001Agent\u3001\u591a\u6a21\u6001\u4ee3\u7801\u89e3\u8bfb\uff01", "postUrl": "post/kotaemon-he-xin-GraphRAG%E3%80%81Agent%E3%80%81-duo-mo-tai-dai-ma-jie-du-%EF%BC%81.html", "postSourceUrl": "https://github.com/winnerineast/winnerineast.github.io/issues/25", "commentNum": 0, "wordCount": 2609, "description": "\u8981\u8bf4\u6700\u8fd1RAG\u65b9\u9762\u706b\u70ed\u7684\u9879\u76ee\u5f53\u5c5e**kotaemon**\uff0c\u77ed\u65f6\u95f4\u66b4\u6da88k star\n\n[\u4e00\u4e2a\u5f00\u6e90\u3001\u6e05\u6670\u3001\u5f3a\u5927\u4e14\u53ef\u5b9a\u5236\u7684RAG UI](http://mp.weixin.qq.com/s?__biz=Mzk0MTYzMzMxMA==&mid=2247489169&idx=1&sn=c3defd706d2b427bf74d0f8b6af2d048&chksm=c2ce2ce0f5b9a5f6116e8e7d5b04b4cbb4b2a996e1e31adbd655511cedc892b21ae26a0d1727&scene=21#wechat_redirect)  \n\n![\u56fe\u7247](https://mmbiz.qpic.cn/sz_mmbiz_png/AE74ia62XricGhdXs2gM0uttZzFDZFWhsp650npa8BbyQ3fHQmUaIBLZ3RuXrxpicL2ic9elt2OQPwbqxPMovQjtnQ/640?wx_fmt=png&from=appmsg&wxfrom=13)\n\n**kotaemon**\u7684\u4eae\u70b9\u662f\u53ef\u5b9a\u5236\u5316**RAG UI**\uff0c\u6838\u5fc3\u6280\u672f\u70b9\u662f\u6df7\u5408\u7d22\u5f15\uff08Vector\u3001Keyword\u3001**GraphRAG**\uff09\u3001\u590d\u6742\u63a8\u7406**Agent**\uff08ReAct\u3001ReWOO\u3001MemoryGIST \u548c GraphReader\uff09\u3001**\u591a\u6a21\u6001**\u3002", "top": 0, "createdAt": 1752322512, "style": "", "script": "", "head": "", "ogImage": "https://github.githubassets.com/favicons/favicon.svg", "createdDate": "2025-07-12", "dateLabelColor": "#0969da"}, "P26": {"htmlDir": "docs/post/MiniMind\uff1a-yi-kai-yuan-\uff01-zhi-yao-3-xiao-shi-jiu-neng-xun-lian-yi-ge-26M-de-da-mo-xing-\uff0c-jiao-cheng-fei-chang-qing-xi-wo-gan-jue-wo-you-xing-le-\uff01.html", "labels": ["AI", "Source Code"], "postTitle": "MiniMind\uff1a\u5df2\u5f00\u6e90\uff01\u53ea\u89813\u5c0f\u65f6\u5c31\u80fd\u8bad\u7ec3\u4e00\u4e2a26M\u7684\u5927\u6a21\u578b\uff0c\u6559\u7a0b\u975e\u5e38\u6e05\u6670\u6211\u611f\u89c9\u6211\u53c8\u884c\u4e86\uff01", "postUrl": "post/MiniMind%EF%BC%9A-yi-kai-yuan-%EF%BC%81-zhi-yao-3-xiao-shi-jiu-neng-xun-lian-yi-ge-26M-de-da-mo-xing-%EF%BC%8C-jiao-cheng-fei-chang-qing-xi-wo-gan-jue-wo-you-xing-le-%EF%BC%81.html", "postSourceUrl": "https://github.com/winnerineast/winnerineast.github.io/issues/26", "commentNum": 0, "wordCount": 1804, "description": "\u5f88\u591a\u4eba\u90fd\u89c9\u5f97\u8bad\u7ec3\u5927\u6a21\u578b\u662f\u4e2a\u5f88\u96be\u7684\u4e8b\u60c5\uff0c\u5305\u62ec\u5927\u90e8\u5206\u7684\u7a0b\u5e8f\u5458\u81ea\u5df1\u4e5f\u641e\u4e0d\u51fa\u6765\u3002", "top": 0, "createdAt": 1752322548, "style": "", "script": "", "head": "", "ogImage": "https://github.githubassets.com/favicons/favicon.svg", "createdDate": "2025-07-12", "dateLabelColor": "#0969da"}, "P27": {"htmlDir": "docs/post/OpenCoder\uff1a-shou-ge-wan-quan-kai-yuan-de-ding-ji-dai-ma-da-mo-xing-\uff0c-xun-lian-mi-ji-quan-gong-kai-\uff0cLLM\u00d7MapReduce\uff0c-wu-xu-xun-lian-jiu-chao-yue-GPT-4.html", "labels": ["paper", "AI", "Source Code"], "postTitle": "OpenCoder\uff1a\u9996\u4e2a\u5b8c\u5168\u5f00\u6e90\u7684\u9876\u7ea7\u4ee3\u7801\u5927\u6a21\u578b\uff0c\u8bad\u7ec3\u79d8\u7c4d\u5168\u516c\u5f00\uff0cLLM\u00d7MapReduce\uff0c\u65e0\u9700\u8bad\u7ec3\u5c31\u8d85\u8d8aGPT-4", "postUrl": "post/OpenCoder%EF%BC%9A-shou-ge-wan-quan-kai-yuan-de-ding-ji-dai-ma-da-mo-xing-%EF%BC%8C-xun-lian-mi-ji-quan-gong-kai-%EF%BC%8CLLM%C3%97MapReduce%EF%BC%8C-wu-xu-xun-lian-jiu-chao-yue-GPT-4.html", "postSourceUrl": "https://github.com/winnerineast/winnerineast.github.io/issues/27", "commentNum": 0, "wordCount": 3201, "description": "\u5927\u6a21\u578b\u9886\u57df\u7684\u53d1\u5c55\u65e5\u65b0\u6708\u5f02\uff0c\u6bcf\u5929\u90fd\u6709\u8bb8\u591a\u6709\u8da3\u7684\u8bba\u6587\u503c\u5f97\u6df1\u5165\u54c1\u8bfb\u3002", "top": 0, "createdAt": 1752322581, "style": "", "script": "", "head": "", "ogImage": "https://github.githubassets.com/favicons/favicon.svg", "createdDate": "2025-07-12", "dateLabelColor": "#0969da"}, "P28": {"htmlDir": "docs/post/yi-ge-quan-mian-\u3001-xian-jin-\u3001-gong-ping-qie-mo-kuai-hua-de-kai-yuan-RAG-kuang-jia.html", "labels": ["paper", "AI", "Source Code"], "postTitle": "\u4e00\u4e2a\u5168\u9762\u3001\u5148\u8fdb\u3001\u516c\u5e73\u4e14\u6a21\u5757\u5316\u7684\u5f00\u6e90RAG\u6846\u67b6", "postUrl": "post/yi-ge-quan-mian-%E3%80%81-xian-jin-%E3%80%81-gong-ping-qie-mo-kuai-hua-de-kai-yuan-RAG-kuang-jia.html", "postSourceUrl": "https://github.com/winnerineast/winnerineast.github.io/issues/28", "commentNum": 0, "wordCount": 2786, "description": "\n\u4e24\u4e2a\u5173\u952e\u95ee\u9898\u9650\u5236\u4e86 RAG \u7684\u53d1\u5c55\uff1a\n\n- \u65b0\u578b RAG \u7b97\u6cd5\u4e4b\u95f4\u7f3a\u4e4f**\u5168\u9762\u548c\u516c\u5e73**\u7684\u6bd4\u8f83\u3002", "top": 0, "createdAt": 1752322598, "style": "", "script": "", "head": "", "ogImage": "https://github.githubassets.com/favicons/favicon.svg", "createdDate": "2025-07-12", "dateLabelColor": "#0969da"}, "P29": {"htmlDir": "docs/post/da-yi-tong-2.0\uff01CNN, RNN, GNN-he-Transformer-mo-xing-de-tong-yi-biao-shi-he-fan-hua-wu-cha-li-lun-fen-xi.html", "labels": ["paper", "AI", "Source Code"], "postTitle": "\u5927\u4e00\u7edf2.0\uff01CNN, RNN, GNN\u548cTransformer\u6a21\u578b\u7684\u7edf\u4e00\u8868\u793a\u548c\u6cdb\u5316\u8bef\u5dee\u7406\u8bba\u5206\u6790", "postUrl": "post/da-yi-tong-2.0%EF%BC%81CNN%2C%20RNN%2C%20GNN-he-Transformer-mo-xing-de-tong-yi-biao-shi-he-fan-hua-wu-cha-li-lun-fen-xi.html", "postSourceUrl": "https://github.com/winnerineast/winnerineast.github.io/issues/29", "commentNum": 0, "wordCount": 24857, "description": "\nOriginal\u00a0\u8ba9\u4f60\u66f4\u61c2AI\u7684\u00a0PaperWeekly\n\n\u00a0_2024\u5e7411\u670829\u65e5 22:37_\n\n![Image](https://mmbiz.qpic.cn/mmbiz_gif/Psho9dm7oDHKVtfYDubjKdZRUjAfBQQicXjoZWJ3qnK42ooD4eeJUfJBM4SSZVa2RE5lO0j6rWwzliby0j9u4bDg/640?wx_fmt=gif&tp=wxpic&wxfrom=5&wx_lazy=1)\n\n![Image](https://mmbiz.qpic.cn/mmbiz_png/Psho9dm7oDEpFIiarozsxcDrKF6ib5G4gvAXKjq7NY1gVRh2S7gokhal1S23ibxkic9LYeQLwQ6AjZOx1jUKNPaFUw/640?wx_fmt=png&from=appmsg&tp=wxpic&wxfrom=5&wx_lazy=1&wx_co=1)\n\n**\u8bba\u6587\u6807\u9898\uff1a**\n\nRPN 2: On Interdependence Function Learning Towards Unifying and Advancing CNN, RNN, GNN, and Transformer\n\n**\u8bba\u6587\u94fe\u63a5\uff1a**\n\nhttps://arxiv.org/abs/2411.11162\n\n**\u5b98\u65b9\u7f51\u7ad9\uff1a**\n\nhttps://www.tinybig.org/\n\n**\u4ee3\u7801\u94fe\u63a5\uff1a**\n\nhttps://github.com/jwzhanggy/tinyBIG\n\n**PyPI Package:**\n\nhttps://pypi.org/project/tinybig/\n\n  \n\n  \n\n![Image](data:image/svg+xml,%3C%3Fxml version='1.0' encoding='UTF-8'%3F%3E%3Csvg width='1px' height='1px' viewBox='0 0 1 1' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg stroke='none' stroke-width='1' fill='none' fill-rule='evenodd' fill-opacity='0'%3E%3Cg transform='translate(-249.000000, -126.000000)' fill='%23FFFFFF'%3E%3Crect x='249' y='126' width='1' height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)\n\n**\u80cc\u666f\u4ecb\u7ecd**\n\n![Image](data:image/svg+xml,%3C%3Fxml version='1.0' encoding='UTF-8'%3F%3E%3Csvg width='1px' height='1px' viewBox='0 0 1 1' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg stroke='none' stroke-width='1' fill='none' fill-rule='evenodd' fill-opacity='0'%3E%3Cg transform='translate(-249.000000, -126.000000)' fill='%23FFFFFF'%3E%3Crect x='249' y='126' width='1' height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)\n\n\u672c\u6587\u662f\u57fa\u4e8e\u6211\u4eec\u4e4b\u524d\u7684\u00a0[RPN\uff08Reconciled Polynomial Network\uff09](https://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&mid=2247674123&idx=1&sn=1e17a56a803e6a7be051ebd04c5bf526&token=1293808530&lang=zh_CN&scene=21#wechat_redirect)\u7814\u7a76\u7684\u540e\u7eed\u5de5\u4f5c\u3002", "top": 0, "createdAt": 1752322615, "style": "", "script": "", "head": "", "ogImage": "https://github.githubassets.com/favicons/favicon.svg", "createdDate": "2025-07-12", "dateLabelColor": "#0969da"}, "P30": {"htmlDir": "docs/post/da-mo-xing-RAG-de-xia-yi-xing-tai-\uff0cMAG \uff08Pending on more progress from GitHub - noted on 7Sep24).html", "labels": ["paper", "AI", "Source Code"], "postTitle": "\u5927\u6a21\u578bRAG\u7684\u4e0b\u4e00\u5f62\u6001\uff0cMAG \uff08Pending on more progress from GitHub - noted on 7Sep24)", "postUrl": "post/da-mo-xing-RAG-de-xia-yi-xing-tai-%EF%BC%8CMAG%20%EF%BC%88Pending%20on%20more%20progress%20from%20GitHub%20-%20noted%20on%207Sep24%29.html", "postSourceUrl": "https://github.com/winnerineast/winnerineast.github.io/issues/30", "commentNum": 0, "wordCount": 1589, "description": "\u8bba\u6587\u7b14\u8bb0\u5206\u4eab\uff0c\n\u6807\u9898MemLong: Memory-Augmented Retrieval for Long Text Modeling ![[2408.16967v1.pdf]]\n\u4ee3\u7801\u5f00\u6e90\uff1ahttps://github.com/Bui1dMySea/MemLong\n\nLLMs\u5728\u5904\u7406\u957f\u6587\u672c\u65f6\uff0c\u56e0\u4e3a\u6ce8\u610f\u529b\u673a\u5236\u4e8c\u6b21\u65f6\u95f4\u548c\u7a7a\u95f4\u590d\u6742\u5ea6\u7684\u95ee\u9898\uff0c\u6240\u4ee5\u5904\u7406\u957f\u6587\u672c\u65f6\u7684\u5185\u5b58\u6d88\u8017\u548c\u8ba1\u7b97\u6210\u672c\u6709\u70b9\u6050\u6016\u3002", "top": 0, "createdAt": 1752322633, "style": "", "script": "", "head": "", "ogImage": "https://github.githubassets.com/favicons/favicon.svg", "createdDate": "2025-07-12", "dateLabelColor": "#0969da"}, "P31": {"htmlDir": "docs/post/shen-du-jie-mi-xian-dai- AI Agent\uff1a-liu-da-he-xin-zu-jian-de-gou-jian-yu-shi-jian-zhi-nan.html", "labels": ["paper", "AI", "Source Code"], "postTitle": "\u6df1\u5ea6\u89e3\u5bc6\u73b0\u4ee3 AI Agent\uff1a\u516d\u5927\u6838\u5fc3\u7ec4\u4ef6\u7684\u6784\u5efa\u4e0e\u5b9e\u8df5\u6307\u5357", "postUrl": "post/shen-du-jie-mi-xian-dai-%20AI%20Agent%EF%BC%9A-liu-da-he-xin-zu-jian-de-gou-jian-yu-shi-jian-zhi-nan.html", "postSourceUrl": "https://github.com/winnerineast/winnerineast.github.io/issues/31", "commentNum": 0, "wordCount": 5347, "description": "\u5f15\u8a00\uff1aAI Agent \u7684\u6f14\u8fdb\n\nAI Agent \u7cfb\u7edf\u7684\u53d1\u5c55\u5386\u7a0b\uff0c\u5c55\u73b0\u4e86 AI \u9886\u57df\u4ece\u7b80\u5355\u5230\u590d\u6742\u3001\u4ece\u7279\u5b9a\u5230\u901a\u7528\u7684\u6280\u672f\u6f14\u8fdb\u8fc7\u7a0b\u3002", "top": 0, "createdAt": 1752322649, "style": "", "script": "", "head": "", "ogImage": "https://github.githubassets.com/favicons/favicon.svg", "createdDate": "2025-07-12", "dateLabelColor": "#0969da"}, "P32": {"htmlDir": "docs/post/Absolute Zero Reinforced Self-play Reasoning with Zero Data.html", "labels": ["paper", "AI", "Source Code"], "postTitle": "Absolute Zero Reinforced Self-play Reasoning with Zero Data", "postUrl": "post/Absolute%20Zero%20Reinforced%20Self-play%20Reasoning%20with%20Zero%20Data.html", "postSourceUrl": "https://github.com/winnerineast/winnerineast.github.io/issues/32", "commentNum": 0, "wordCount": 1582, "description": "\u300cReasoning, RLVR\u300d\u8bba\u6587\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nPaper of the week!\n\n\u4eae\u70b9\uff1aAZR \u5b8c\u5168\u4e0d\u4f9d\u8d56\u4eba\u7c7b\u6807\u6ce8\u6570\u636e\uff0c\u901a\u8fc7\u81ea\u535a\u5f08\u5b9e\u73b0\u63a8\u7406\u80fd\u529b\u8fdb\u5316\u3002", "top": 0, "createdAt": 1752322685, "style": "", "script": "", "head": "", "ogImage": "https://github.githubassets.com/favicons/favicon.svg", "createdDate": "2025-07-12", "dateLabelColor": "#0969da"}, "P33": {"htmlDir": "docs/post/AI -shi-dai-xia-de-gong-cheng-ling-dao-li-\uff1a-ru-he-da-zao-gao-xiao-tuan-dui- - -lai-zi-gu-ge-gong-cheng-fu-ze-ren-\u3001Chrome -kai-fa-zhe-de-bao-gui-jing-yan-fen-xiang.html", "labels": ["AI"], "postTitle": "AI \u65f6\u4ee3\u4e0b\u7684\u5de5\u7a0b\u9886\u5bfc\u529b\uff1a\u5982\u4f55\u6253\u9020\u9ad8\u6548\u56e2\u961f - \u6765\u81ea\u8c37\u6b4c\u5de5\u7a0b\u8d1f\u8d23\u4eba\u3001Chrome \u5f00\u53d1\u8005\u7684\u5b9d\u8d35\u7ecf\u9a8c\u5206\u4eab", "postUrl": "post/AI%20-shi-dai-xia-de-gong-cheng-ling-dao-li-%EF%BC%9A-ru-he-da-zao-gao-xiao-tuan-dui-%20-%20-lai-zi-gu-ge-gong-cheng-fu-ze-ren-%E3%80%81Chrome%20-kai-fa-zhe-de-bao-gui-jing-yan-fen-xiang.html", "postSourceUrl": "https://github.com/winnerineast/winnerineast.github.io/issues/33", "commentNum": 0, "wordCount": 3379, "description": "Original \u90b5\u731b *2025\u5e7403\u670825\u65e5 19:32*\n\n\u4eca\u5929\u5076\u7136\u8bfb\u5230 Chrome \u5f00\u53d1\u8005\u3001Google \u5de5\u7a0b\u8d1f\u8d23\u4eba\u3001\u8457\u540d\u6280\u672f\u4e66\u7c4d\u4f5c\u8005 - Addy Osmani \u7684\u4e00\u7bc7\u6587\u7ae0 \u300c Leading Effective Engineering Teams in the Age of GenAI\u300d \uff0c\u8bb2\u7684\u7279\u522b\u597d\uff0c\u5bf9\u4e8e\u4ea7\u54c1\u548c\u7814\u53d1\u65b9\u5411\u5982\u4f55\u53d8\u5f97\u9ad8\u6548\uff0c\u4e0d\u7ba1\u4f60\u662f\u56e2\u961f\u9886\u5bfc\u8005\u3001\u8fd8\u662f\u56e2\u961f\u6210\u5458\uff0c\u90fd\u5f88\u6709\u4ef7\u503c\uff0c\u5206\u4eab\u7ed9\u670b\u53cb\u4eec\uff0c\u53ef\u4ee5\u5148\u770b\u6211\u7684\u9605\u8bfb\u7b14\u8bb0\uff0c\u9488\u5bf9\u81ea\u5df1\u611f\u5174\u8da3\u7684\u90e8\u95e8\u518d\u9605\u8bfb\u539f\u6587\uff08\u63a8\u8350\u9605\u8bfb\uff0c\u4f5c\u8005\u4fe1\u606f\u548c\u6587\u7ae0\u94fe\u63a5\u653e\u5728\u6587\u672b\uff09\n\n![Image](https://mmbiz.qpic.cn/mmbiz_png/5CmZha6ohm38sDnEfwzjLtq29sCwghGUjmFhN5danpeEvVtb5WDldZDnQ2ycAe4FIymxLYyBbQyQfk54eflBDA/640?wx_fmt=png&from=appmsg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)\n\n> \u6587\u7ae0\u7684\u6838\u5fc3\u89c2\u70b9\u662f\uff1a\u5728\u751f\u6210\u5f0f AI \u65f6\u4ee3\uff0c\u9886\u5bfc\u4e00\u4e2a\u5de5\u7a0b\u56e2\u961f\u4e0d\u662f\u4e3a\u4e86\u8ba9\u4ee3\u7801\u5199\u5f97\u66f4\u5feb\uff0c\u800c\u662f\u8981\u8ba9\u8f6f\u4ef6\u505a\u5f97\u66f4\u597d\u3002", "top": 0, "createdAt": 1752322706, "style": "", "script": "", "head": "", "ogImage": "https://github.githubassets.com/favicons/favicon.svg", "createdDate": "2025-07-12", "dateLabelColor": "#0969da"}, "P34": {"htmlDir": "docs/post/ai-toolkit Various AI scripts. Mostly Stable Diffusion stuff.html", "labels": ["AI", "Source Code"], "postTitle": "ai-toolkit Various AI scripts. Mostly Stable Diffusion stuff", "postUrl": "post/ai-toolkit%20Various%20AI%20scripts.%20Mostly%20Stable%20Diffusion%20stuff.html", "postSourceUrl": "https://github.com/winnerineast/winnerineast.github.io/issues/34", "commentNum": 0, "wordCount": 38571, "description": "## Support My Work\n\nIf you enjoy my work, or use it for commercial purposes, please consider sponsoring me so I can continue to maintain it. Every bit helps!\n\n[Become a sponsor on GitHub](https://github.com/orgs/ostris) or [support me on Patreon](https://www.patreon.com/ostris).\n\nThank you to all my current supporters!\n\n*Last updated: 2025-03-08*\n\n### GitHub Sponsors\n\n[![Replicate](https://avatars.githubusercontent.com/u/60410876?v=4)](https://github.com/replicate 'Replicate')\n\n### Patreon Supporters\n\n[![Abraham Irawan](https://camo.githubusercontent.com/01304a218435d2dc7d9d79e0598a28d2d41161a97ada67ca986d10b8fe8069d5/68747470733a2f2f6331302e70617472656f6e75736572636f6e74656e742e636f6d2f342f70617472656f6e2d6d656469612f702f757365722f36303939353639342f39326530653866333336656234613562623864393962393430323437643164312f65794a33496a6f794d4442392f312e706e673f746f6b656e2d74696d653d3231343539313638303026746f6b656e2d686173683d706a36546d3858526470474a634145646e43616b7159534e69536a6f41596a765a65736358376430696330253344)](https://github.com/ostris/ai-toolkit/blob/main/None 'Abraham Irawan') [![Al H](https://camo.githubusercontent.com/2940ae9ac9e51292f80903abe5c2fa1859635c0cf8db65220b9cc2552971909e/68747470733a2f2f6331302e70617472656f6e75736572636f6e74656e742e636f6d2f342f70617472656f6e2d6d656469612f702f757365722f3537303734322f34636562333334353361356134373435623433306132313661626139323830662f65794a33496a6f794d4442392f312e6a70673f746f6b656e2d74696d653d3231343539313638303026746f6b656e2d686173683d77557a734935634f3545767032756b49476453674262764b65596776354c534f514d613642723333527273253344)](https://github.com/ostris/ai-toolkit/blob/main/None 'Al H') [![Armin Behjati](https://camo.githubusercontent.com/9dd1426d004086651156a61b86724bb0db062685a051ccb3a2ce421ab3ceddb5/68747470733a2f2f6331302e70617472656f6e75736572636f6e74656e742e636f6d2f342f70617472656f6e2d6d656469612f702f757365722f39333334383231302f35633635306633326130626334383164383039303064323637343532383737372f65794a33496a6f794d4442392f312e6a7065673f746f6b656e2d74696d653d3231343539313638303026746f6b656e2d686173683d5070584b39425f6979323838616e6e6c4e644c4f657868695148625466745045446543682d735451324b41253344)](https://github.com/ostris/ai-toolkit/blob/main/None 'Armin Behjati') [![Bharat Prabhakar](https://camo.githubusercontent.com/ca278ecbb51e76fa443277f55e7902868885e082f11ce4ca9043ad74018b0667/68747470733a2f2f6331302e70617472656f6e75736572636f6e74656e742e636f6d2f342f70617472656f6e2d6d656469612f702f757365722f3133343132393838302f36383063376531346364316134643161396661636539323166623031306638382f65794a33496a6f794d4442392f312e706e673f746f6b656e2d74696d653d3231343539313638303026746f6b656e2d686173683d764e4b6f6a7636376b724e7178376764704b425831525f73745832546b4d525976526330785a7262593673253344)](https://github.com/ostris/ai-toolkit/blob/main/None 'Bharat Prabhakar') [![clement Delangue](https://camo.githubusercontent.com/4bd350ab3856de5fd210ebef6753064966bd2462279c3894ce2988d3202e6632/68747470733a2f2f63382e70617472656f6e2e636f6d2f332f3230302f3333313538353433)](https://github.com/ostris/ai-toolkit/blob/main/None 'clement Delangue') [![Cosmosis](https://camo.githubusercontent.com/70a035ed3f572e383521f4cdb265649894272f95501732e80207389b1ce9ef9c/68747470733a2f2f63382e70617472656f6e2e636f6d2f332f3230302f3730323138383436)](https://github.com/ostris/ai-toolkit/blob/main/None 'Cosmosis') [![David Garrido](https://camo.githubusercontent.com/c8ff2c47678a2ec2608b46cc796e1158f7cc2636574bf9fd19c3d688536c57e9/68747470733a2f2f6331302e70617472656f6e75736572636f6e74656e742e636f6d2f342f70617472656f6e2d6d656469612f702f757365722f32373238383933322f36633335643264393631656534653134613761333638633939303739313331352f65794a33496a6f794d4442392f312e6a7065673f746f6b656e2d74696d653d3231343539313638303026746f6b656e2d686173683d6470464673735a585a4d5f4b5a4d4b51686c3375447777757364467731635f7639785f43684a55375f7a63253344)](https://github.com/ostris/ai-toolkit/blob/main/None 'David Garrido') [![Doron Adler](https://camo.githubusercontent.com/d103374f89aa9110163aa208d9a99c145ef0c6d3be3a0f51e7bf73ca807b6868/68747470733a2f2f6331302e70617472656f6e75736572636f6e74656e742e636f6d2f342f70617472656f6e2d6d656469612f702f757365722f38323736332f66393963633438343336316434623964393466653466303831346164613330332f65794a33496a6f794d4442392f312e6a7065673f746f6b656e2d74696d653d3231343539313638303026746f6b656e2d686173683d4270774330323070523354525a3472305253436953494f682d6a6d61746b72707931683258553473476134253344)](https://github.com/ostris/ai-toolkit/blob/main/None 'Doron Adler') [![Eli Slugworth](https://camo.githubusercontent.com/3e87c3d4e5e54442dada635551e1ad78ce638f87faad3be80e07e9a9339e7473/68747470733a2f2f6331302e70617472656f6e75736572636f6e74656e742e636f6d2f342f70617472656f6e2d6d656469612f702f757365722f35343839303336392f34356365613231643832393734633738626634333935366465376662306531322f65794a33496a6f794d4442392f322e6a7065673f746f6b656e2d74696d653d3231343539313638303026746f6b656e2d686173683d494b364f543655707573486764614334793849684b3558785869503554754c7933766a76674c373746686f253344)](https://github.com/ostris/ai-toolkit/blob/main/None 'Eli Slugworth') [![EmmanuelMr18](https://camo.githubusercontent.com/ba49dd498546303a8dbc66de9b969bca00561411bcd5f904af1926e6eae21d3c/68747470733a2f2f6331302e70617472656f6e75736572636f6e74656e742e636f6d2f342f70617472656f6e2d6d656469612f702f757365722f39383831313433352f33613336333264313739356234633262396638663032373066326636613635302f65794a33496a6f794d4442392f312e6a7065673f746f6b656e2d74696d653d3231343539313638303026746f6b656e2d686173683d39337738524d7877586c634d34583734743033753650355f53724b766c6d3149706a6e4432537a56704a6b253344)](https://github.com/ostris/ai-toolkit/blob/main/None 'EmmanuelMr18') [![Gili Ben Shahar](https://camo.githubusercontent.com/c99f0b1aebc9a0772ef5aa7d904385f1285d2b0eb790750875e5f9e2bbe9224a/68747470733a2f2f6331302e70617472656f6e75736572636f6e74656e742e636f6d2f342f70617472656f6e2d6d656469612f702f757365722f38333035343937302f31336465366362313033616434316135383431656466353439653636636435312f65794a33496a6f794d4442392f312e6a7065673f746f6b656e2d74696d653d3231343539313638303026746f6b656e2d686173683d77555f456b653956596366493430464151766445563834587370716c6f3556536961664c7168675f464f45253344)](https://github.com/ostris/ai-toolkit/blob/main/None 'Gili Ben Shahar') [![HestoySeghuro .](https://camo.githubusercontent.com/637fc5b80c91d46d45788827d5e5ad25320752b801a45ff3c8f59bd1e141c27a/68747470733a2f2f6331302e70617472656f6e75736572636f6e74656e742e636f6d2f342f70617472656f6e2d6d656469612f702f757365722f33303933313938332f35346162346534636561623934366537396136343138643230356639656435312f65794a33496a6f794d4442392f312e706e673f746f6b656e2d74696d653d3231343539313638303026746f6b656e2d686173683d4c426d7353734d515a684f3679525a5f5979527754674536613742565772474e73415676654c5848585230253344)](https://github.com/ostris/ai-toolkit/blob/main/None 'HestoySeghuro .') [![Jack Blakely](https://camo.githubusercontent.com/e84a64fa5ac0184cca8fa76c77da559e5b7a568fa43cbe56eb5cd31e564e12ae/68747470733a2f2f63382e70617472656f6e2e636f6d2f332f3230302f34313035333834)](https://github.com/ostris/ai-toolkit/blob/main/None 'Jack Blakely') [![Jack English](https://camo.githubusercontent.com/18a220995b159e647074f76d14de2ec82a212a921aada77bacdd29f1490524a7/68747470733a2f2f6331302e70617472656f6e75736572636f6e74656e742e636f6d2f342f70617472656f6e2d6d656469612f702f757365722f34353536323937382f30646533336366353265633634326165386132663631326364646563346361362f65794a33496a6f794d4442392f312e6a7065673f746f6b656e2d74696d653d3231343539313638303026746f6b656e2d686173683d685341766144347068694c634630707658374650306a7549354e5157436f6e2d5f545a534e704a7a514a67253344)](https://github.com/ostris/ai-toolkit/blob/main/None 'Jack English') [![Jason](https://camo.githubusercontent.com/8870f5b4308041cd3d420c5f02bf0af8bfb645aa972d21c1617186be6c54adc7/68747470733a2f2f6331302e70617472656f6e75736572636f6e74656e742e636f6d2f342f70617472656f6e2d6d656469612f702f757365722f3135303235373031332f30653965333333643330323934656566396634643638323131363639363664382f65794a33496a6f794d4442392f322e706e673f746f6b656e2d74696d653d3231343539313638303026746f6b656e2d686173683d6850485f7270354c354f4a395a4d5331775a667056584442346c527632474870563672384a6d626d717777253344)](https://github.com/ostris/ai-toolkit/blob/main/None 'Jason') [![Jean-Tristan Marin](https://camo.githubusercontent.com/20f7d3b28193b7bd07044d6cc38fe2eb2591f89f1fcc17457363f7854378a9b4/68747470733a2f2f63382e70617472656f6e2e636f6d2f332f3230302f3237373931363830)](https://github.com/ostris/ai-toolkit/blob/main/None 'Jean-Tristan Marin') [![Jodh Singh](https://camo.githubusercontent.com/76ba06b964d36da7d76c98630be77cff50d05a8d266856cf5d5c666bd97891eb/68747470733a2f2f6331302e70617472656f6e75736572636f6e74656e742e636f6d2f342f70617472656f6e2d6d656469612f702f757365722f3133313737333934372f65646133343035616135383234333764623435383266636539303863383733392f65794a33496a6f794d4442392f312e706e673f746f6b656e2d74696d653d3231343539313638303026746f6b656e2d686173683d5334426830734d71544e6d4a6c6f3375527237636f35645f6b7876426a4954656d445466695f314b724341253344)](https://github.com/ostris/ai-toolkit/blob/main/None 'Jodh Singh') [![John Dopamine](https://camo.githubusercontent.com/d5e0cdf8032d530916c2e3098dc3d693929b2cae560d08bda196aa278282cee9/68747470733a2f2f6331302e70617472656f6e75736572636f6e74656e742e636f6d2f342f70617472656f6e2d6d656469612f702f757365722f3130333037373731312f62623231353736316363303034653830626439636563376434626364363336642f65794a33496a6f794d4442392f322e6a7065673f746f6b656e2d74696d653d3231343539313638303026746f6b656e2d686173683d7a767442696532397252544b5458764141324b684f492d6c336d534d6b397878722d6d675f436b734c7463253344)](https://github.com/ostris/ai-toolkit/blob/main/None 'John Dopamine') [![Joseph Rocca](https://camo.githubusercontent.com/ae4ab43a64fb9c6ca3c61b96cf8cebec40719f34766575630b6fae0d91f2146b/68747470733a2f2f63382e70617472656f6e2e636f6d2f332f3230302f3933333034)](https://github.com/ostris/ai-toolkit/blob/main/None 'Joseph Rocca') [![Kas\u0131m A\u00e7\u0131kba\u015f](https://camo.githubusercontent.com/390f729a6f4e424de66ecfbd67c1f8c0843eaf73d11ebc68ace581dcff0d309e/68747470733a2f2f6331302e70617472656f6e75736572636f6e74656e742e636f6d2f342f70617472656f6e2d6d656469612f702f757365722f33313437313337392f30613838373531336565333134613163383664306236663837393265393739352f65794a33496a6f794d4442392f312e6a7065673f746f6b656e2d74696d653d3231343539313638303026746f6b656e2d686173683d444a4d5a733372446c533066434d5f61686d39354641626a6c654d5f4c3067734f397141507a7164306e41253344)](https://github.com/ostris/ai-toolkit/blob/main/None 'Kas\u0131m A\u00e7\u0131kba\u015f') [![Kristjan Retter](https://camo.githubusercontent.com/de91761acfb490905a4ba2773e94f73d4b63f7bf31c2e53ffb35ffea1157f4fe/68747470733a2f2f6331302e70617472656f6e75736572636f6e74656e742e636f6d2f342f70617472656f6e2d6d656469612f702f757365722f3135323131383834382f33623135613433643731373134353532623565643163396638346536366164662f65794a33496a6f794d4442392f312e706e673f746f6b656e2d74696d653d3231343539313638303026746f6b656e2d686173683d49454b453138434248565a336b2d3038554437446b623748626946486238345730464154644c4d49304467253344)](https://github.com/ostris/ai-toolkit/blob/main/None 'Kristjan Retter') [![Maciej Pop\u0142awski](https://camo.githubusercontent.com/b51bfe7c69442757412ddfd91d0a3718c3b73bb0b27dc33a3e4ab957adaebcbd/68747470733a2f2f6331302e70617472656f6e75736572636f6e74656e742e636f6d2f342f70617472656f6e2d6d656469612f702f757365722f32343435383439352f37643739343132653362393934616535616662336362373830346265383565652f65794a33496a6f794d4442392f312e6a7065673f746f6b656e2d74696d653d3231343539313638303026746f6b656e2d686173683d574a4667436b7770335f764d64446543784b396b36435373426e454c703755636d45474459674f56303777253344)](https://github.com/ostris/ai-toolkit/blob/main/None 'Maciej Pop\u0142awski') [![Michael Levine](https://camo.githubusercontent.com/810c9c43581ee78d17a5da0352da3b6ce1fc253bf2560069d40d6d8257fb0ba8/68747470733a2f2f63382e70617472656f6e2e636f6d2f332f3230302f3232383039363930)](https://github.com/ostris/ai-toolkit/blob/main/None 'Michael Levine') [![Miguel Lara](https://camo.githubusercontent.com/40d0fc2826eeff00a955ccf1cf081471ee8c06143e863717cadf67b6bfd1753c/68747470733a2f2f63382e70617472656f6e2e636f6d2f332f3230302f3833333139323330)](https://github.com/ostris/ai-toolkit/blob/main/None 'Miguel Lara') [![Misch Strotz](https://camo.githubusercontent.com/f0bb7d62b7faaa8d0805a728c7c61c00b887790a97b913abff6cbaa830baf493/68747470733a2f2f6331302e70617472656f6e75736572636f6e74656e742e636f6d2f342f70617472656f6e2d6d656469612f702f757365722f383635343330322f62306635656265646336326134376334623536323232363933653132353465392f65794a33496a6f794d4442392f312e6a7065673f746f6b656e2d74696d653d3231343539313638303026746f6b656e2d686173683d6c706569634968315f532d334a693357323767796952423769587572703842783848417a444866744f756f253344)](https://github.com/ostris/ai-toolkit/blob/main/None 'Misch Strotz') [![Mohamed Oumoumad](https://camo.githubusercontent.com/ccf7cfa9a2a29859553810a0eb1a7dade836329e0508bc80587211822f3b9048/68747470733a2f2f6331302e70617472656f6e75736572636f6e74656e742e636f6d2f342f70617472656f6e2d6d656469612f702f757365722f323239383139322f31323238623639626437643734383162616633313033333135313833323530642f65794a33496a6f794d4442392f312e6a70673f746f6b656e2d74696d653d3231343539313638303026746f6b656e2d686173683d314237646258795f67416350543957584265734c6873377a5f394150697a326b31577834566d6c5f2d3851253344)](https://github.com/ostris/ai-toolkit/blob/main/None 'Mohamed Oumoumad') [![Noctre](https://camo.githubusercontent.com/d1097267ceb4d89fc5858b4adc7e46f68e324dc98b96d1c2bcbbc94aa0d39549/68747470733a2f2f6331302e70617472656f6e75736572636f6e74656e742e636f6d2f342f70617472656f6e2d6d656469612f702f757365722f39393033363335362f37616539633464383065363034653733396236386363613132656532656430312f65794a33496a6f794d4442392f332e706e673f746f6b656e2d74696d653d3231343539313638303026746f6b656e2d686173683d7a4b30644865364139333757744e6c72476465666f58465450507a485543666e5f5f32334850382d556930253344)](https://github.com/ostris/ai-toolkit/blob/main/None 'Noctre') [![Patron](https://camo.githubusercontent.com/4560fc70cc6e94d5efb15b820e6c8b9061ad0e4de6f4466dfb5278261ad4d7e0/68747470733a2f2f63382e70617472656f6e2e636f6d2f332f3230302f38343439353630)](https://github.com/ostris/ai-toolkit/blob/main/None 'Patron') [![Paul Fidika](https://camo.githubusercontent.com/70d43882056ed1267bc02c7f4e6204443affe9524376ccb3561709b0d09246f8/68747470733a2f2f63382e70617472656f6e2e636f6d2f332f3230302f3234343430373839)](https://github.com/ostris/ai-toolkit/blob/main/None 'Paul Fidika') [![Plaidam](https://camo.githubusercontent.com/b10b684569cdcace5e46c720d58f5537f3f3bf318b01b2672ba851820a939978/68747470733a2f2f6331302e70617472656f6e75736572636f6e74656e742e636f6d2f342f70617472656f6e2d6d656469612f702f757365722f3333383535312f65386632353764386433646434366333383237326233393161353738353934382f65794a33496a6f794d4442392f312e6a70673f746f6b656e2d74696d653d3231343539313638303026746f6b656e2d686173683d474c6f6d317247674f5a6a42654f3749314f6e6a694967576d6a6c36504f395a6a4242385954766337414d253344)](https://github.com/ostris/ai-toolkit/blob/main/None 'Plaidam') [![Prasanth Veerina](https://camo.githubusercontent.com/5f9cd9c6f3360d4646a8847ea58f1949ec62c3b81ce7cd90364d80bfb5adbe24/68747470733a2f2f6331302e70617472656f6e75736572636f6e74656e742e636f6d2f342f70617472656f6e2d6d656469612f702f757365722f3136323532343130312f38316137323638396333373534616335623965333836313263653563653931342f65794a33496a6f794d4442392f312e706e673f746f6b656e2d74696d653d3231343539313638303026746f6b656e2d686173683d33584c536c4c46435741512d307764325f765a4d696b796f7464514e537a4b4f6a6f79656f4a695a457730253344)](https://github.com/ostris/ai-toolkit/blob/main/None 'Prasanth Veerina') [![Razvan Grigore](https://camo.githubusercontent.com/b784270c227676eab339d50334ef2d9d2cb883e1d708d3a9156ea770bce38872/68747470733a2f2f6331302e70617472656f6e75736572636f6e74656e742e636f6d2f342f70617472656f6e2d6d656469612f702f757365722f3135363338323830322f61353437653161383935323134346261393466333031623130656133376637612f65794a33496a6f794d4442392f312e6a7065673f746f6b656e2d74696d653d3231343539313638303026746f6b656e2d686173683d34446f656353597751477166364c67595543484b6345694b67327033526a5f33426d656e506b4833454b38253344)](https://github.com/ostris/ai-toolkit/blob/main/None 'Razvan Grigore') [![Steve Hanff](https://camo.githubusercontent.com/23e4e6d3fe1e8c7c49ca91a485b763f20db1386ca71ec43d99fd02cda66082f7/68747470733a2f2f63382e70617472656f6e2e636f6d2f332f3230302f353438353234)](https://github.com/ostris/ai-toolkit/blob/main/None 'Steve Hanff') [![Steve Informal](https://camo.githubusercontent.com/1468174a3e66df5ca204070ca2809d710b4c85ef5393a0eb4bec7b83ec932e0f/68747470733a2f2f6331302e70617472656f6e75736572636f6e74656e742e636f6d2f342f70617472656f6e2d6d656469612f702f757365722f36323738333231322f63623731663933386263363534373438623763636538356137303863653061622f65794a33496a6f794d4442392f312e6a7065673f746f6b656e2d74696d653d3231343539313638303026746f6b656e2d686173683d4d4b43655869524869536244394978397464764d6e37386b52505152545634514a6c374f304f4c4e303945253344)](https://github.com/ostris/ai-toolkit/blob/main/None 'Steve Informal') [![Steven Simmons](https://camo.githubusercontent.com/9bc6a4d290112971d58821b976f2f0c060452b96c65828a53d19202a93fb1437/68747470733a2f2f6331302e70617472656f6e75736572636f6e74656e742e636f6d2f342f70617472656f6e2d6d656469612f702f757365722f3136333432363937372f66633339343163373965383934666566393835643966353434303235353331332f65794a33496a6f794d4442392f312e706e673f746f6b656e2d74696d653d3231343539313638303026746f6b656e2d686173683d546a776c6c664b43645f46747431433277465964634f644a5a7879755061527045624b6a72667a6b305a77253344)](https://github.com/ostris/ai-toolkit/blob/main/None 'Steven Simmons') [![S\u00f6ren ](https://camo.githubusercontent.com/e8508e533245b9cd7749a7ac432c39470facfd6770486413b18cff24c75d0557/68747470733a2f2f63382e70617472656f6e2e636f6d2f332f3230302f34353431343233)](https://github.com/ostris/ai-toolkit/blob/main/None 'S\u00f6ren ') [![The Local Lab](https://camo.githubusercontent.com/53414c85b15e60b77dc3ff3fac3a4490decef0b2616735638a9db5c6b6195f3f/68747470733a2f2f6331302e70617472656f6e75736572636f6e74656e742e636f6d2f342f70617472656f6e2d6d656469612f702f757365722f3134313039383537392f31613966306131323439643434376137613064663731386135373334333931322f65794a33496a6f794d4442392f322e706e673f746f6b656e2d74696d653d3231343539313638303026746f6b656e2d686173683d52645f416a5a47684d4154566b5a446638453935494c63306e3933677676465765314967305f6478776634253344)](https://github.com/ostris/ai-toolkit/blob/main/None 'The Local Lab') [![Trent Hunter](https://camo.githubusercontent.com/b1af5edbb6d608f68694959df13dd3a2228861e05cfa3f989d1ca0903c87b33d/68747470733a2f2f6331302e70617472656f6e75736572636f6e74656e742e636f6d2f342f70617472656f6e2d6d656469612f702f757365722f33313935303835372f63353637646336343866363134346265396636323334393436646630356461322f65794a33496a6f794d4442392f312e6a7065673f746f6b656e2d74696d653d3231343539313638303026746f6b656e2d686173683d335678345231654f664434585f5a50506434304d735a2d336c796b6e4c4d3335586d614852454c6e576a4d253344)](https://github.com/ostris/ai-toolkit/blob/main/None 'Trent Hunter') [![Un Defined](https://camo.githubusercontent.com/ca61328f3d24d4ecce60e285f918a2d6daf0a90222634eb1d3051fbb64de833a/68747470733a2f2f6331302e70617472656f6e75736572636f6e74656e742e636f6d2f342f70617472656f6e2d6d656469612f702f757365722f3135353936333235302f36663866643730373563336234323437626665623035346261343931373264362f65794a33496a6f794d4442392f312e706e673f746f6b656e2d74696d653d3231343539313638303026746f6b656e2d686173683d74776d4b73346d4144465f6837624b68356a42756967595653634d656165487632704550696e394b304467253344)](https://github.com/ostris/ai-toolkit/blob/main/None 'Un Defined') [![Vladimir Sotnikov](https://camo.githubusercontent.com/efa70da1ea18d064349b0370434ee60afb6061bed20242e8c4e0bf52790ba87f/68747470733a2f2f6331302e70617472656f6e75736572636f6e74656e742e636f6d2f342f70617472656f6e2d6d656469612f702f757365722f3136313437313732302f64643333306234303336643434613539383565643539383563313261356465662f65794a33496a6f794d4442392f312e6a7065673f746f6b656e2d74696d653d3231343539313638303026746f6b656e2d686173683d716b527672456335674c50786158784c7663766259763157316c636d4f6f5477686a344139437135427851253344)](https://github.com/ostris/ai-toolkit/blob/main/None 'Vladimir Sotnikov') [![Wesley Reitzfeld](https://camo.githubusercontent.com/d51d60e5aa457f21dc46c7514bba3175de77739c638f7ca3fdea6de2275e586b/68747470733a2f2f6331302e70617472656f6e75736572636f6e74656e742e636f6d2f342f70617472656f6e2d6d656469612f702f757365722f3131303430373431342f33306639653964383865663934356464623066343766643233613863626163322f65794a33496a6f794d4442392f312e6a7065673f746f6b656e2d74696d653d3231343539313638303026746f6b656e2d686173683d515152574f6b4d794f6644424552486e344f384e32774d4233327a65694945737964565462534e55772d49253344)](https://github.com/ostris/ai-toolkit/blob/main/None 'Wesley Reitzfeld') [![Zolt\u00e1n-Csaba Nyir\u00f3](https://camo.githubusercontent.com/d99860bbd5affc6271592f5343d347d406f9fdcaf54663b11e534988b51f0f84/68747470733a2f2f6331302e70617472656f6e75736572636f6e74656e742e636f6d2f342f70617472656f6e2d6d656469612f702f757365722f3136323339383639312f38396437386438396565636234643662393831636538633363366133643462382f65794a33496a6f794d4442392f312e706e673f746f6b656e2d74696d653d3231343539313638303026746f6b656e2d686173683d535768492d306a477059364e635f62555165587a34706139445255526939566e6e6e4a334d786a6731706f253344)](https://github.com/ostris/ai-toolkit/blob/main/None 'Zolt\u00e1n-Csaba Nyir\u00f3') [![\u0410\u043b\u0435\u043a\u0441\u0435\u0439 \u041d\u0430\u0443\u043c\u043e\u0432](https://camo.githubusercontent.com/d60eef8aadd641bc8c3c49f96cd274e4e27bf214a28f74ffbe8eff3887995492/68747470733a2f2f6331302e70617472656f6e75736572636f6e74656e742e636f6d2f342f70617472656f6e2d6d656469612f702f757365722f32363031393038322f36626139363831323965323834633836393036396232363163383735616530322f65794a33496a6f794d4442392f312e706e673f746f6b656e2d74696d653d3231343539313638303026746f6b656e2d686173683d4a7a2d4b6b396c38524976474d4e63614758754e385f76615933473433356c466d744a74465a41334f4373253344)](https://github.com/ostris/ai-toolkit/blob/main/None '\u0410\u043b\u0435\u043a\u0441\u0435\u0439 \u041d\u0430\u0443\u043c\u043e\u0432') [![\u05e2\u05d5\u05de\u05e8 \u05de\u05db\u05dc\u05d5\u05e3](https://camo.githubusercontent.com/220e8b7c298c0c461ba0a47f6e4d2c6a3d6face34deb837fe7cb6035f97cbe26/68747470733a2f2f6331302e70617472656f6e75736572636f6e74656e742e636f6d2f342f70617472656f6e2d6d656469612f702f757365722f39373938353234302f33643164306536393035643034356162613731336538313332636162346133302f65794a33496a6f794d4442392f312e706e673f746f6b656e2d74696d653d3231343539313638303026746f6b656e2d686173683d70473358326d2d7079326c52594932616f4a69584934375f3441724437385a4864536d366a434148415f77253344)](https://github.com/ostris/ai-toolkit/blob/main/None '\u05e2\u05d5\u05de\u05e8 \u05de\u05db\u05dc\u05d5\u05e3')\n\n---\n\n## Installation\n\nRequirements:\n\n- python >3.10\n- Nvidia GPU with enough ram to do what you need\n- python venv\n- git\n\nLinux:\n\n```\ngit clone https://github.com/ostris/ai-toolkit.git\ncd ai-toolkit\ngit submodule update --init --recursive\npython3 -m venv venv\nsource venv/bin/activate\n# .\\venv\\Scripts\\activate on windows\n# install torch first\npip3 install torch\npip3 install -r requirements.txt\n```\n\nWindows:\n\n```\ngit clone https://github.com/ostris/ai-toolkit.git\ncd ai-toolkit\ngit submodule update --init --recursive\npython -m venv venv\n.\\venv\\Scripts\\activate\npip install torch==2.5.1 torchvision==0.20.1 --index-url https://download.pytorch.org/whl/cu124\npip install -r requirements.txt\n```\n\n## AI Toolkit UI\n\n[![AI Toolkit UI](https://camo.githubusercontent.com/ea35b399e0d659f9f2ee09cbedb58e1a3ec7a0eab763e8ae8d11d076aad5be40/68747470733a2f2f6f73747269732e636f6d2f77702d636f6e74656e742f75706c6f6164732f323032352f30322f746f6f6c6b69742d75692e6a7067)](https://camo.githubusercontent.com/ea35b399e0d659f9f2ee09cbedb58e1a3ec7a0eab763e8ae8d11d076aad5be40/68747470733a2f2f6f73747269732e636f6d2f77702d636f6e74656e742f75706c6f6164732f323032352f30322f746f6f6c6b69742d75692e6a7067)\n\nThe AI Toolkit UI is a web interface for the AI Toolkit. It allows you to easily start, stop, and monitor jobs. It also allows you to easily train models with a few clicks. It is still in early beta and will likely have bugs and frequent breaking changes. It is currently only tested on linux for now.\n\nWARNING: The UI is not secure and should not be exposed to the internet. It is only meant to be run locally or on a server that does not have ports exposed. Adding additional security is on the roadmap.\n\n## Installing the UI\n\nRequirements:\n\n- Node.js > 18\n\nYou will need to do this with every update as well.\n\n```\ncd ui\nnpm install\nnpm run build\nnpm run update_db\n```\n\n## Running the UI\n\nMake sure you built it as shown above. The UI does not need to be kept running for the jobs to run. It is only needed to start/stop/monitor jobs.\n\n```\ncd ui\nnpm run start\n```\n\nYou can now access the UI at `http://localhost:8675` or `http://<your-ip>:8675` if you are running it on a server.\n\n## FLUX.1 Training\n\n### Tutorial\n\nTo get started quickly, check out [@araminta\\_k](https://x.com/araminta_k) tutorial on [Finetuning Flux Dev on a 3090](https://www.youtube.com/watch?v=HzGW_Kyermg) with 24GB VRAM.\n\n### Requirements\n\nYou currently need a GPU with **at least 24GB of VRAM** to train FLUX.1. If you are using it as your GPU to control your monitors, you probably need to set the flag `low_vram: true` in the config file under `model:`. This will quantize the model on CPU and should allow it to train with monitors attached. Users have gotten it to work on Windows with WSL, but there are some reports of a bug when running on windows natively. I have only tested on linux for now. This is still extremely experimental and a lot of quantizing and tricks had to happen to get it to fit on 24GB at all.\n\n### FLUX.1-dev\n\nFLUX.1-dev has a non-commercial license. Which means anything you train will inherit the non-commercial license. It is also a gated model, so you need to accept the license on HF before using it. Otherwise, this will fail. Here are the required steps to setup a license.\n\n1. Sign into HF and accept the model access here [black-forest-labs/FLUX.1-dev](https://huggingface.co/black-forest-labs/FLUX.1-dev)\n2. Make a file named `.env` in the root on this folder\n3. [Get a READ key from huggingface](https://huggingface.co/settings/tokens/new?) and add it to the `.env` file like so `HF_TOKEN=your_key_here`\n\n### FLUX.1-schnell\n\nFLUX.1-schnell is Apache 2.0. Anything trained on it can be licensed however you want and it does not require a HF\\_TOKEN to train. However, it does require a special adapter to train with it, [ostris/FLUX.1-schnell-training-adapter](https://huggingface.co/ostris/FLUX.1-schnell-training-adapter). It is also highly experimental. For best overall quality, training on FLUX.1-dev is recommended.\n\nTo use it, You just need to add the assistant to the `model` section of your config file like so:\n\n```\n      model:\n        name_or_path: 'black-forest-labs/FLUX.1-schnell'\n        assistant_lora_path: 'ostris/FLUX.1-schnell-training-adapter'\n        is_flux: true\n        quantize: true\n```\n\nYou also need to adjust your sample steps since schnell does not require as many\n\n```\n      sample:\n        guidance_scale: 1  # schnell does not do guidance\n        sample_steps: 4  # 1 - 4 works well\n```\n\n### Training\n\n1. Copy the example config file located at `config/examples/train_lora_flux_24gb.yaml` (`config/examples/train_lora_flux_schnell_24gb.yaml` for schnell) to the `config` folder and rename it to `whatever_you_want.yml`\n2. Edit the file following the comments in the file\n3. Run the file like so `python run.py config/whatever_you_want.yml`\n\nA folder with the name and the training folder from the config file will be created when you start. It will have all checkpoints and images in it. You can stop the training at any time using ctrl+c and when you resume, it will pick back up from the last checkpoint.\n\nIMPORTANT. If you press crtl+c while it is saving, it will likely corrupt that checkpoint. So wait until it is done saving\n\n### Need help?\n\nPlease do not open a bug report unless it is a bug in the code. You are welcome to [Join my Discord](https://discord.gg/VXmU2f5WEU) and ask for help there. However, please refrain from PMing me directly with general question or support. Ask in the discord and I will answer when I can.\n\n## Gradio UI\n\nTo get started training locally with a with a custom UI, once you followed the steps above and `ai-toolkit` is installed:\n\n```\ncd ai-toolkit #in case you are not yet in the ai-toolkit folder\nhuggingface-cli login #provide a \\`write\\` token to publish your LoRA at the end\npython flux_train_ui.py\n```\n\nYou will instantiate a UI that will let you upload your images, caption them, train and publish your LoRA [![image](https://github.com/ostris/ai-toolkit/raw/main/assets/lora_ease_ui.png)](https://github.com/ostris/ai-toolkit/blob/main/assets/lora_ease_ui.png)\n\n## Training in RunPod\n\nExample RunPod template: **runpod/pytorch:2.2.0-py3.10-cuda12.1.1-devel-ubuntu22.04**\n\n> You need a minimum of 24GB VRAM, pick a GPU by your preference.\n\n#### Example config ($0.5/hr):\n\n- 1x A40 (48 GB VRAM)\n- 19 vCPU 100 GB RAM\n\n#### Custom overrides (you need some storage to clone FLUX.1, store datasets, store trained models and samples):\n\n- ~120 GB Disk\n- ~120 GB Pod Volume\n- Start Jupyter Notebook\n\n### 1\\. Setup\n\n```\ngit clone https://github.com/ostris/ai-toolkit.git\ncd ai-toolkit\ngit submodule update --init --recursive\npython -m venv venv\nsource venv/bin/activate\npip install torch\npip install -r requirements.txt\npip install --upgrade accelerate transformers diffusers huggingface_hub #Optional, run it if you run into issues\n```\n\n### 2\\. Upload your dataset\n\n- Create a new folder in the root, name it `dataset` or whatever you like.\n- Drag and drop your .jpg, .jpeg, or .png images and .txt files inside the newly created dataset folder.\n\n### 3\\. Login into Hugging Face with an Access Token\n\n- Get a READ token from [here](https://huggingface.co/settings/tokens) and request access to Flux.1-dev model from [here](https://huggingface.co/black-forest-labs/FLUX.1-dev).\n- Run `huggingface-cli login` and paste your token.\n\n### 4\\. Training\n\n- Copy an example config file located at `config/examples` to the config folder and rename it to `whatever_you_want.yml`.\n- Edit the config following the comments in the file.\n- Change `folder_path: '/path/to/images/folder'` to your dataset path like `folder_path: '/workspace/ai-toolkit/your-dataset'`.\n- Run the file: `python run.py config/whatever_you_want.yml`.\n\n### Screenshot from RunPod\n\n[![RunPod Training Screenshot](https://private-user-images.githubusercontent.com/101264514/358849579-53a1b8ef-92fa-4481-81a7-bde45a14a7b5.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NDE2MDU3NzUsIm5iZiI6MTc0MTYwNTQ3NSwicGF0aCI6Ii8xMDEyNjQ1MTQvMzU4ODQ5NTc5LTUzYTFiOGVmLTkyZmEtNDQ4MS04MWE3LWJkZTQ1YTE0YTdiNS5wbmc_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjUwMzEwJTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI1MDMxMFQxMTE3NTVaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT04ZWQzNGJlNTY5YTVmZDg0OGZiYmUwYzg2NjVhZDViM2RmNGU2NGUzMjY0ZDA3MmM2ZmZiZDE0MDEzYTE0YmRmJlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCJ9.D-qN_uU5citkDFrTyazA2sifVlurocNZbzI-7YTCaks)](https://private-user-images.githubusercontent.com/101264514/358849579-53a1b8ef-92fa-4481-81a7-bde45a14a7b5.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NDE2MDU3NzUsIm5iZiI6MTc0MTYwNTQ3NSwicGF0aCI6Ii8xMDEyNjQ1MTQvMzU4ODQ5NTc5LTUzYTFiOGVmLTkyZmEtNDQ4MS04MWE3LWJkZTQ1YTE0YTdiNS5wbmc_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjUwMzEwJTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI1MDMxMFQxMTE3NTVaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT04ZWQzNGJlNTY5YTVmZDg0OGZiYmUwYzg2NjVhZDViM2RmNGU2NGUzMjY0ZDA3MmM2ZmZiZDE0MDEzYTE0YmRmJlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCJ9.D-qN_uU5citkDFrTyazA2sifVlurocNZbzI-7YTCaks)\n\n## Training in Modal\n\n### 1\\. Setup\n\n#### ai-toolkit:\n\n```\ngit clone https://github.com/ostris/ai-toolkit.git\ncd ai-toolkit\ngit submodule update --init --recursive\npython -m venv venv\nsource venv/bin/activate\npip install torch\npip install -r requirements.txt\npip install --upgrade accelerate transformers diffusers huggingface_hub #Optional, run it if you run into issues\n```\n\n#### Modal:\n\n- Run `pip install modal` to install the modal Python package.\n- Run `modal setup` to authenticate (if this doesn\u2019t work, try `python -m modal setup`).\n\n#### Hugging Face:\n\n- Get a READ token from [here](https://huggingface.co/settings/tokens) and request access to Flux.1-dev model from [here](https://huggingface.co/black-forest-labs/FLUX.1-dev).\n- Run `huggingface-cli login` and paste your token.\n\n### 2\\. Upload your dataset\n\n- Drag and drop your dataset folder containing the .jpg, .jpeg, or .png images and .txt files in `ai-toolkit`.\n\n### 3\\. Configs\n\n- Copy an example config file located at `config/examples/modal` to the `config` folder and rename it to `whatever_you_want.yml`.\n- Edit the config following the comments in the file, **be careful and follow the example `/root/ai-toolkit` paths**.\n\n### 4\\. Edit run\\_modal.py\n\n- Set your entire local `ai-toolkit` path at `code_mount = modal.Mount.from_local_dir` like:\n\n```\ncode_mount = modal.Mount.from_local_dir('/Users/username/ai-toolkit', remote_path='/root/ai-toolkit')\n```\n- Choose a `GPU` and `Timeout` in `@app.function` *(default is A100 40GB and 2 hour timeout)*.\n\n### 5\\. Training\n\n- Run the config file in your terminal: `modal run run_modal.py --config-file-list-str=/root/ai-toolkit/config/whatever_you_want.yml`.\n- You can monitor your training in your local terminal, or on [modal.com](https://modal.com/).\n- Models, samples and optimizer will be stored in `Storage > flux-lora-models`.\n\n### 6\\. Saving the model\n\n- Check contents of the volume by running `modal volume ls flux-lora-models`.\n- Download the content by running `modal volume get flux-lora-models your-model-name`.\n- Example: `modal volume get flux-lora-models my_first_flux_lora_v1`.\n\n### Screenshot from Modal\n\n[![Modal Traning Screenshot](https://private-user-images.githubusercontent.com/101264514/360713889-7497eb38-0090-49d6-8ad9-9c8ea7b5388b.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NDE2MDU3NzUsIm5iZiI6MTc0MTYwNTQ3NSwicGF0aCI6Ii8xMDEyNjQ1MTQvMzYwNzEzODg5LTc0OTdlYjM4LTAwOTAtNDlkNi04YWQ5LTljOGVhN2I1Mzg4Yi5wbmc_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjUwMzEwJTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI1MDMxMFQxMTE3NTVaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT03YjNiYTZkNzQ2OGY5MDk3NGE3NGUyMmYzZGZlNTM2MmM5YzMyM2YzMWQ5MjA3ZTUxZmFlMWUyZTZkZmJlNTdlJlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCJ9.QDJuhwX_i6btxzYE14HQKw1BqMe4FpqkbT_ppH_C1io)](https://private-user-images.githubusercontent.com/101264514/360713889-7497eb38-0090-49d6-8ad9-9c8ea7b5388b.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NDE2MDU3NzUsIm5iZiI6MTc0MTYwNTQ3NSwicGF0aCI6Ii8xMDEyNjQ1MTQvMzYwNzEzODg5LTc0OTdlYjM4LTAwOTAtNDlkNi04YWQ5LTljOGVhN2I1Mzg4Yi5wbmc_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjUwMzEwJTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI1MDMxMFQxMTE3NTVaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT03YjNiYTZkNzQ2OGY5MDk3NGE3NGUyMmYzZGZlNTM2MmM5YzMyM2YzMWQ5MjA3ZTUxZmFlMWUyZTZkZmJlNTdlJlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCJ9.QDJuhwX_i6btxzYE14HQKw1BqMe4FpqkbT_ppH_C1io)\n\n---\n\n## Dataset Preparation\n\nDatasets generally need to be a folder containing images and associated text files. Currently, the only supported formats are jpg, jpeg, and png. Webp currently has issues. The text files should be named the same as the images but with a `.txt` extension. For example `image2.jpg` and `image2.txt`. The text file should contain only the caption. You can add the word `[trigger]` in the caption file and if you have `trigger_word` in your config, it will be automatically replaced.\n\nImages are never upscaled but they are downscaled and placed in buckets for batching. **You do not need to crop/resize your images**. The loader will automatically resize them and can handle varying aspect ratios.\n\n## Training Specific Layers\n\nTo train specific layers with LoRA, you can use the `only_if_contains` network kwargs. For instance, if you want to train only the 2 layers used by The Last Ben, [mentioned in this post](https://x.com/__TheBen/status/1829554120270987740), you can adjust your network kwargs like so:\n\n```\n      network:\n        type: 'lora'\n        linear: 128\n        linear_alpha: 128\n        network_kwargs:\n          only_if_contains:\n            - 'transformer.single_transformer_blocks.7.proj_out'\n            - 'transformer.single_transformer_blocks.20.proj_out'\n```\n\nThe naming conventions of the layers are in diffusers format, so checking the state dict of a model will reveal the suffix of the name of the layers you want to train. You can also use this method to only train specific groups of weights. For instance to only train the `single_transformer` for FLUX.1, you can use the following:\n\n```\n      network:\n        type: 'lora'\n        linear: 128\n        linear_alpha: 128\n        network_kwargs:\n          only_if_contains:\n            - 'transformer.single_transformer_blocks.'\n```\n\nYou can also exclude layers by their names by using `ignore_if_contains` network kwarg. So to exclude all the single transformer blocks,\n\n```\n      network:\n        type: 'lora'\n        linear: 128\n        linear_alpha: 128\n        network_kwargs:\n          ignore_if_contains:\n            - 'transformer.single_transformer_blocks.'\n```\n\n`ignore_if_contains` takes priority over `only_if_contains`. So if a weight is covered by both, if will be ignored.\n\n## LoKr Training\n\nTo learn more about LoKr, read more about it at [KohakuBlueleaf/LyCORIS](https://github.com/KohakuBlueleaf/LyCORIS/blob/main/docs/Guidelines.md). To train a LoKr model, you can adjust the network type in the config file like so:\n\n```\n      network:\n        type: 'lokr'\n        lokr_full_rank: true\n        lokr_factor: 8\n```\n\nEverything else should work the same including layer targeting.\u3002", "top": 0, "createdAt": 1752322731, "style": "", "script": "", "head": "", "ogImage": "https://github.githubassets.com/favicons/favicon.svg", "createdDate": "2025-07-12", "dateLabelColor": "#0969da"}, "P35": {"htmlDir": "docs/post/AI-zhong-yao-fa-zhan-qu-shi-\uff1aMCP -ji-shu-ke-pu.html", "labels": ["AI"], "postTitle": "AI\u91cd\u8981\u53d1\u5c55\u8d8b\u52bf\uff1aMCP \u6280\u672f\u79d1\u666e", "postUrl": "post/AI-zhong-yao-fa-zhan-qu-shi-%EF%BC%9AMCP%20-ji-shu-ke-pu.html", "postSourceUrl": "https://github.com/winnerineast/winnerineast.github.io/issues/35", "commentNum": 0, "wordCount": 1362, "description": "This article popularizes the MCP technology, an important trend in AI development, including its definition, core components, related websites, clients, practical operations, and other resources. Key points include: \u200b\n\n1.\n\nDefinition of MCP : MCP, short for Model Context Protocol, is an open protocol that standardizes how applications provide context to LLMs, similar to the USB-C interface for AI applications. \u200b\n\n2.\n\nCore components : MCP server exposes specific functions or data sources for AI models, while MCP client connects to these servers on behalf of the model, following principles like server connection and tool use. \u200b\n\n3.\n\nMCP service websites : Websites such as Smithery - Model Context Protocol Registry, PulseMCP, etc., provide MCP services. \u200b\n\n4.\n\nSupported clients : There are clients that support MCP, though specific ones aren't detailed here. \u200b\n\n5.\n\nPractical operations : Ten recommended MCP Servers for functions like search enhancement and database access are provided, along with installation methods using VS code with different tools. \u200b\n\n6.\n\nUse Case : An example of making the API of Conch AI into MCP to generate audio in one sentence is given. \u200b\n\n7.\n\nOther resources : Resources like Awesome MCP, official Anthropic MCP documents, sharing of MCP server configurations, learning videos, and product introductions are available. \u200b\u3002", "top": 0, "createdAt": 1752322748, "style": "", "script": "", "head": "", "ogImage": "https://github.githubassets.com/favicons/favicon.svg", "createdDate": "2025-07-12", "dateLabelColor": "#0969da"}, "P36": {"htmlDir": "docs/post/Circuit Tracing Revealing Computational Graphs in Language Models.html", "labels": ["paper", "AI"], "postTitle": "Circuit Tracing Revealing Computational Graphs in Language Models", "postUrl": "post/Circuit%20Tracing%20Revealing%20Computational%20Graphs%20in%20Language%20Models.html", "postSourceUrl": "https://github.com/winnerineast/winnerineast.github.io/issues/36", "commentNum": 0, "wordCount": 69, "description": "https://transformer-circuits.pub/2025/attribution-graphs/methods.html\u3002", "top": 0, "createdAt": 1752322836, "style": "", "script": "", "head": "", "ogImage": "https://github.githubassets.com/favicons/favicon.svg", "createdDate": "2025-07-12", "dateLabelColor": "#0969da"}, "P37": {"htmlDir": "docs/post/Claude-han-AI-liang-chao-Cursor-yi-bei-\uff01-zi-shen-gong-cheng-zhu-guan-jie-mi-AI-bian-ma-zhen-xiang-\uff01-gu-ge-jin-shen-quan-gao-zi-yan-\uff1b-ruan-jian-jia-gou-jiao-fu-\uff1a-xiang-cong-hui-bian-shi-dai-yue-qian-dao-gao-ji-yu-yan.html", "labels": ["AI"], "postTitle": "Claude\u542bAI\u91cf\u8d85Cursor\u4e00\u500d\uff01\u8d44\u6df1\u5de5\u7a0b\u4e3b\u7ba1\u63ed\u79d8AI\u7f16\u7801\u771f\u76f8\uff01\u8c37\u6b4c\u8c28\u614e\u5168\u641e\u81ea\u7814\uff1b\u8f6f\u4ef6\u67b6\u6784\u6559\u7236\uff1a\u50cf\u4ece\u6c47\u7f16\u65f6\u4ee3\u8dc3\u8fc1\u5230\u9ad8\u7ea7\u8bed\u8a00", "postUrl": "post/Claude-han-AI-liang-chao-Cursor-yi-bei-%EF%BC%81-zi-shen-gong-cheng-zhu-guan-jie-mi-AI-bian-ma-zhen-xiang-%EF%BC%81-gu-ge-jin-shen-quan-gao-zi-yan-%EF%BC%9B-ruan-jian-jia-gou-jiao-fu-%EF%BC%9A-xiang-cong-hui-bian-shi-dai-yue-qian-dao-gao-ji-yu-yan.html", "postSourceUrl": "https://github.com/winnerineast/winnerineast.github.io/issues/37", "commentNum": 0, "wordCount": 8942, "description": "Claude\u542bAI\u91cf\u8d85Cursor\u4e00\u500d\uff01\u8d44\u6df1\u5de5\u7a0b\u4e3b\u7ba1\u63ed\u79d8AI\u7f16\u7801\u771f\u76f8\uff01\u8c37\u6b4c\u8c28\u614e\u5168\u641e\u81ea\u7814\uff1b\u8f6f\u4ef6\u67b6\u6784\u6559\u7236\uff1a\u50cf\u4ece\u6c47\u7f16\u65f6\u4ee3\u8dc3\u8fc1\u5230\u9ad8\u7ea7\u8bed\u8a00\nImage\nImage\n\u7f16\u8f91 | \u4f0a\u98ce\n\n\n\n\u8fd9\u5e94\u8be5\u662f\u6211\u542c\u8fc7\u6700\u624e\u5b9e\u3001\u6700\u5ba2\u89c2\u7684\u4e00\u573a AI \u7f16\u7a0b\u6f14\u8bb2\u3002", "top": 0, "createdAt": 1752322943, "style": "", "script": "", "head": "", "ogImage": "https://github.githubassets.com/favicons/favicon.svg", "createdDate": "2025-07-12", "dateLabelColor": "#0969da"}, "P38": {"htmlDir": "docs/post/IBM - GneissWeb\uff1a-da-zao-10-wan-yi-ji-gao-zhi-liang-xun-lian-shu-ju-\uff01.html", "labels": ["paper", "AI"], "postTitle": "IBM - GneissWeb\uff1a\u6253\u902010\u4e07\u4ebf\u7ea7\u9ad8\u8d28\u91cf\u8bad\u7ec3\u6570\u636e\uff01", "postUrl": "post/IBM%20-%20GneissWeb%EF%BC%9A-da-zao-10-wan-yi-ji-gao-zhi-liang-xun-lian-shu-ju-%EF%BC%81.html", "postSourceUrl": "https://github.com/winnerineast/winnerineast.github.io/issues/38", "commentNum": 0, "wordCount": 2704, "description": "Original *2025\u5e7403\u670807\u65e5 08:23*\n\n![Image](https://mmbiz.qpic.cn/mmbiz_png/KQq0TwTibbBCkxtlNyIIqPxczibeCnD19H1xH4Ivok6zfqGxqkUI5UmgPjFPxhbzzzwIS4TGjUH11wtCJ60sJYwA/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)\n\n\u5f53\u4e0b\u7684AI\u70ed\u6f6e\u4e2d\uff0c\u5927\u5bb6\u90fd\u5728\u8c08\u8bbaChatGPT\u3001Claude\u8fd9\u6837\u7684\u5927\u8bed\u8a00\u6a21\u578b\u6709\u591a\u4e48\u5f3a\u5927\uff0c\u4f46\u5f88\u5c11\u6709\u4eba\u5173\u6ce8\u5b83\u4eec\u80cc\u540e\u7684'\u7cae\u98df'\u2014\u2014\u8bad\u7ec3\u6570\u636e\u3002", "top": 0, "createdAt": 1752322990, "style": "", "script": "", "head": "", "ogImage": "https://github.githubassets.com/favicons/favicon.svg", "createdDate": "2025-07-12", "dateLabelColor": "#0969da"}, "P39": {"htmlDir": "docs/post/LLM-yi-neng-zi-wo-geng-xin-quan-zhong-\uff0c-zi-shi-ying-\u3001-zhi-shi-zheng-he-neng-li-da-fu-ti-sheng-\uff0cAI-xing-le.html", "labels": ["paper", "AI"], "postTitle": "LLM\u5df2\u80fd\u81ea\u6211\u66f4\u65b0\u6743\u91cd\uff0c\u81ea\u9002\u5e94\u3001\u77e5\u8bc6\u6574\u5408\u80fd\u529b\u5927\u5e45\u63d0\u5347\uff0cAI\u9192\u4e86", "postUrl": "post/LLM-yi-neng-zi-wo-geng-xin-quan-zhong-%EF%BC%8C-zi-shi-ying-%E3%80%81-zhi-shi-zheng-he-neng-li-da-fu-ti-sheng-%EF%BC%8CAI-xing-le.html", "postSourceUrl": "https://github.com/winnerineast/winnerineast.github.io/issues/39", "commentNum": 0, "wordCount": 10971, "description": "*2025\u5e7406\u670814\u65e5 12:12*\n\n\u673a\u5668\u4e4b\u5fc3\u62a5\u9053\n\n**\u7f16\u8f91\uff1aPanda**\n\n  \n\n\u8fd1\u6bb5\u65f6\u95f4\uff0c\u5173\u4e8e AI \u81ea\u6211\u6f14\u8fdb/\u8fdb\u5316\u8fd9\u4e00\u8bdd\u9898\u7684\u7814\u7a76\u548c\u8ba8\u8bba\u5f00\u59cb\u53d8\u5f97\u6108\u6e10\u5bc6\u96c6\u3002", "top": 0, "createdAt": 1752323014, "style": "", "script": "", "head": "", "ogImage": "https://github.githubassets.com/favicons/favicon.svg", "createdDate": "2025-07-12", "dateLabelColor": "#0969da"}, "P40": {"htmlDir": "docs/post/LSTM-zhi-fu-22-nian-qian-gou-xiang-jiang-cheng-zhen-\uff1f-yi-zhou-nei-AI\u300c-zi-wo-jin-hua-\u300d-lun-wen-ji-zhong-fa-bu-\uff0c-xin-qu-shi-yong-xian-\uff1f.html", "labels": ["paper", "AI"], "postTitle": "LSTM\u4e4b\u723622\u5e74\u524d\u6784\u60f3\u5c06\u6210\u771f\uff1f\u4e00\u5468\u5185AI\u300c\u81ea\u6211\u8fdb\u5316\u300d\u8bba\u6587\u96c6\u4e2d\u53d1\u5e03\uff0c\u65b0\u8d8b\u52bf\u6d8c\u73b0\uff1f", "postUrl": "post/LSTM-zhi-fu-22-nian-qian-gou-xiang-jiang-cheng-zhen-%EF%BC%9F-yi-zhou-nei-AI%E3%80%8C-zi-wo-jin-hua-%E3%80%8D-lun-wen-ji-zhong-fa-bu-%EF%BC%8C-xin-qu-shi-yong-xian-%EF%BC%9F.html", "postSourceUrl": "https://github.com/winnerineast/winnerineast.github.io/issues/40", "commentNum": 0, "wordCount": 11145, "description": "*2025\u5e7406\u670802\u65e5 13:23*\n\n\u673a\u5668\u4e4b\u5fc3\u62a5\u9053\n\n**\u7f16\u8f91\uff1a\u5f20\u5029\u3001+0**\n\n  \n\n\u8ba9 AI \u5b9e\u73b0\u81ea\u6211\u8fdb\u5316\u662f\u4eba\u7c7b\u4e00\u76f4\u4ee5\u6765\u7684\u68a6\u60f3\u3002", "top": 0, "createdAt": 1752323030, "style": "", "script": "", "head": "", "ogImage": "https://github.githubassets.com/favicons/favicon.svg", "createdDate": "2025-07-12", "dateLabelColor": "#0969da"}, "P41": {"htmlDir": "docs/post/Omnitool\uff1a-yong-yi-ge-kai-yuan-mian-fei-de-gong-ju-\uff0c-wan-zhuan-suo-you-AI-mo-xing-\uff01.html", "labels": ["AI", "Source Code"], "postTitle": "Omnitool\uff1a\u7528\u4e00\u4e2a\u5f00\u6e90\u514d\u8d39\u7684\u5de5\u5177\uff0c\u73a9\u8f6c\u6240\u6709AI\u6a21\u578b\uff01", "postUrl": "post/Omnitool%EF%BC%9A-yong-yi-ge-kai-yuan-mian-fei-de-gong-ju-%EF%BC%8C-wan-zhuan-suo-you-AI-mo-xing-%EF%BC%81.html", "postSourceUrl": "https://github.com/winnerineast/winnerineast.github.io/issues/41", "commentNum": 0, "wordCount": 3135, "description": "Original *2025\u5e7402\u670818\u65e5 06:05*\n\n\u5e2e\u5fd9\u70b9\u51fb**\u84dd\u8272\u5b57**\uff0c\u7ed9\u4e2a\u5173\u6ce8\u5457!\n\n![Image](https://mmbiz.qpic.cn/sz_mmbiz_gif/Km2ahm6mpT3HBcWbIUIRp50ZULLYJWR29LRFAorV3TYpkbIOVz33kk9TF9icuQdTuXd65licNMmuTuoice1icdpqzA/640?wx_fmt=gif&from=appmsg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)\n\n  \n\n\u8bf7\u5728\u5fae\u4fe1\u5ba2\u6237\u7aef\u6253\u5f00\n\n\u5728\u5f53\u4ecaAI\u98de\u901f\u53d1\u5c55\u7684\u65f6\u4ee3\uff0c\u65e0\u8bba\u662f\u6280\u672f\u7231\u597d\u8005\u3001\u5f00\u53d1\u8005\uff0c\u8fd8\u662f\u666e\u901a\u7528\u6237\uff0c\u90fd\u70ed\u5207\u6e34\u671b\u8f7b\u677e\u63a5\u89e6\u548c\u8fd0\u7528\u6700\u65b0\u7684AI\u6280\u672f\u3002", "top": 0, "createdAt": 1752323053, "style": "", "script": "", "head": "", "ogImage": "https://github.githubassets.com/favicons/favicon.svg", "createdDate": "2025-07-12", "dateLabelColor": "#0969da"}, "P42": {"htmlDir": "docs/post/Meta Superintelligence \u2013 Leadership Compute, Talent, and Data.html", "labels": ["AI"], "postTitle": "Meta Superintelligence \u2013 Leadership Compute, Talent, and Data", "postUrl": "post/Meta%20Superintelligence%20%E2%80%93%20Leadership%20Compute%2C%20Talent%2C%20and%20Data.html", "postSourceUrl": "https://github.com/winnerineast/winnerineast.github.io/issues/42", "commentNum": 0, "wordCount": 198, "description": "Here's an interesting article I found:\n[Meta Superintelligence: Leadership, Compute, Talent, and Data](https://semianalysis.com/2025/07/11/meta-superintelligence-leadership-compute-talent-and-data/)\u3002", "top": 0, "createdAt": 1752378256, "style": "", "script": "", "head": "", "ogImage": "https://github.githubassets.com/favicons/favicon.svg", "createdDate": "2025-07-13", "dateLabelColor": "#0969da"}, "P43": {"htmlDir": "docs/post/qian-nao-xi-tong-\uff0c-fang-sheng-kong-jian-zhi-neng-xue-xi.html", "labels": ["paper", "AI"], "postTitle": "\u5343\u8111\u7cfb\u7edf\uff0c\u4eff\u751f\u7a7a\u95f4\u667a\u80fd\u5b66\u4e60", "postUrl": "post/qian-nao-xi-tong-%EF%BC%8C-fang-sheng-kong-jian-zhi-neng-xue-xi.html", "postSourceUrl": "https://github.com/winnerineast/winnerineast.github.io/issues/43", "commentNum": 0, "wordCount": 45163, "description": "A good paper: https://arxiv.org/abs/2507.04494\n\n<img width='1700' height='1098' alt='Image' src='https://github.com/user-attachments/assets/e6a69ae4-ac1f-4b62-83ae-dd6155ee1eee' />\n\n<img width='1302' height='1652' alt='Image' src='https://github.com/user-attachments/assets/7a21e093-2de2-4a12-ad0a-061832a33a4d' />\n\n<img width='1918' height='1268' alt='Image' src='https://github.com/user-attachments/assets/3f746ded-ae5f-42bb-b7ee-29da4b5c8ec4' />\n\n<html>\n<body>\n<!--StartFragment--><b style='font-weight:normal;' id='docs-internal-guid-8669252e-7fff-0ca9-302e-1f22518783c1'><p dir='ltr' style='line-height:1.3799999713897704;margin-top:0pt;margin-bottom:12pt;'><span style='font-size:11pt;font-family:'Google Sans Text',sans-serif;color:#1b1c1d;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;'>\u597d\u7684\uff0c\u8fd9\u7bc7\u8bba\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u540d\u4e3a&ldquo;\u5343\u8111\u7cfb\u7edf&rdquo;\uff08Thousand-Brains Systems\uff09\u7684\u65b0\u578b\u4eba\u5de5\u667a\u80fd\u67b6\u6784\uff0c\u5e76\u8be6\u7ec6\u9610\u8ff0\u4e86\u5176\u9996\u4e2a\u5b9e\u73b0\u2014\u2014\u4e00\u4e2a\u53eb\u505a </span><span style='font-size:11pt;font-family:'Google Sans Text',sans-serif;color:#1b1c1d;background-color:transparent;font-weight:700;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;'>Monty</span><span style='font-size:11pt;font-family:'Google Sans Text',sans-serif;color:#1b1c1d;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;'> \u7684\u7cfb\u7edf\u3002", "top": 0, "createdAt": 1752378413, "style": "", "script": "", "head": "", "ogImage": "https://github.githubassets.com/favicons/favicon.svg", "createdDate": "2025-07-13", "dateLabelColor": "#0969da"}, "P44": {"htmlDir": "docs/post/DeepSeek V3-R1 vs. Kimi K2.html", "labels": ["AI"], "postTitle": "DeepSeek V3/R1 vs. Kimi K2", "postUrl": "post/DeepSeek%20V3-R1%20vs.%20Kimi%20K2.html", "postSourceUrl": "https://github.com/winnerineast/winnerineast.github.io/issues/44", "commentNum": 0, "wordCount": 336, "description": "<img width='4096' height='2142' alt='Image' src='https://github.com/user-attachments/assets/edfb843f-0e70-4d71-bb88-ec963ea0a412' />\n\nKimi-K2 \u4e0e DeepSeek-R1 \u67b6\u6784\u5bf9\u6bd4\uff0c\u76f8\u6bd4\u8f83\u4e0b Kimi-k2 \u589e\u52a0\u4e86\u4e13\u5bb6\u6570\u91cf\uff0c\u51cf\u5c11\u4e86\u6ce8\u610f\u529b\u5934\u7684\u6570\u91cf\u3002", "top": 0, "createdAt": 1752379865, "style": "", "script": "", "head": "", "ogImage": "https://github.githubassets.com/favicons/favicon.svg", "createdDate": "2025-07-13", "dateLabelColor": "#0969da"}, "P45": {"htmlDir": "docs/post/yi-ge-ke-yi-fen-xi-qiu-sai-de-kuang-jia-\u2014\u2014supervision.html", "labels": ["AI", "Source Code"], "postTitle": "\u4e00\u4e2a\u53ef\u4ee5\u5206\u6790\u7403\u8d5b\u7684\u6846\u67b6\u2014\u2014supervision", "postUrl": "post/yi-ge-ke-yi-fen-xi-qiu-sai-de-kuang-jia-%E2%80%94%E2%80%94supervision.html", "postSourceUrl": "https://github.com/winnerineast/winnerineast.github.io/issues/45", "commentNum": 0, "wordCount": 229, "description": "\u8fd9\u4e2a\u6846\u67b6\u662f\u4e00\u4e2a\u4e00\u4f53\u5316\u89e3\u51b3\u65b9\u6848\uff0c\u4ece\u52a0\u8f7d\u6570\u636e\u5230\u7ed8\u5236\u68c0\u6d4b\u5eb7\u518d\u5230\u8ba1\u7b97\u68c0\u6d4b\u6846\u90fd\u53ef\u4ee5\u5b8c\u6210\u3002", "top": 0, "createdAt": 1752725786, "style": "", "script": "", "head": "", "ogImage": "https://github.githubassets.com/favicons/favicon.svg", "createdDate": "2025-07-17", "dateLabelColor": "#0969da"}, "P46": {"htmlDir": "docs/post/yong-zi-ji-dian-nao-de-CPU-wei-diao-da-mo-xing.html", "labels": ["AI", "Source Code"], "postTitle": "\u7528\u81ea\u5df1\u7535\u8111\u7684CPU\u5fae\u8c03\u5927\u6a21\u578b", "postUrl": "post/yong-zi-ji-dian-nao-de-CPU-wei-diao-da-mo-xing.html", "postSourceUrl": "https://github.com/winnerineast/winnerineast.github.io/issues/46", "commentNum": 0, "wordCount": 367, "description": "\u6709\u6ca1\u6709\u60f3\u8fc7\u7528\u81ea\u5df1\u7535\u8111\u7684CPU\u5fae\u8c03\u5927\u6a21\u578b\uff1f\u6765\u770b\u2014\u2014LoFT\n\n\u8fd9\u4e2a\u9879\u76ee\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u3001CPU \u53cb\u597d\u7684\u5de5\u5177\u5305\uff0c\u53ef\u4ee5\u4f7f\u7528 QLoRA \u5fae\u8c03 1\u20133B \u7684 \u5927\u6a21\u578b\u3002", "top": 0, "createdAt": 1752725844, "style": "", "script": "", "head": "", "ogImage": "https://github.githubassets.com/favicons/favicon.svg", "createdDate": "2025-07-17", "dateLabelColor": "#0969da"}, "P47": {"htmlDir": "docs/post/WebMCP = 50-xing-dai-ma-ji-ke-rang-wang-zhan-zhi-chi-MCP\uff0c-rang-AI-zhu-shou-yu-Web-ying-yong-jin-xing-jiao-hu.html", "labels": ["AI", "Source Code"], "postTitle": "WebMCP = 50\u884c\u4ee3\u7801\u5373\u53ef\u8ba9\u7f51\u7ad9\u652f\u6301MCP\uff0c\u8ba9AI\u52a9\u624b\u4e0eWeb\u5e94\u7528\u8fdb\u884c\u4ea4\u4e92", "postUrl": "post/WebMCP%20%3D%2050-xing-dai-ma-ji-ke-rang-wang-zhan-zhi-chi-MCP%EF%BC%8C-rang-AI-zhu-shou-yu-Web-ying-yong-jin-xing-jiao-hu.html", "postSourceUrl": "https://github.com/winnerineast/winnerineast.github.io/issues/47", "commentNum": 0, "wordCount": 336, "description": "50\u884c\u4ee3\u7801\u5373\u53ef\u8ba9\u7f51\u7ad9\u652f\u6301MCP\uff0c\u8ba9AI\u52a9\u624b\u4e0eWeb\u5e94\u7528\u8fdb\u884c\u4ea4\u4e92\uff1aWebMCP\n\n\u5176\u4e0eWeb\u5e94\u7528\u76f4\u63a5\u96c6\u6210\uff0c\u65e0\u9700API\u5bc6\u94a5\u6216OAuth\u914d\u7f6e\uff0c\u76f4\u63a5\u6d4f\u89c8\u5668\u8bbf\u95ee\n\n\u5b9e\u65f6\u540c\u6b65\uff0c\u4e0eWeb\u5e94\u7528\u72b6\u6001\u76f4\u63a5\u96c6\u6210\n\n\u652f\u6301\u8de8\u5e94\u7528\u5de5\u4f5c\u6d41\uff0c\u8ba9AI\u52a9\u624b\u5728\u591a\u4e2a\u4e0d\u540c\u7f51\u9875\u5e94\u7528\u95f4\u81ea\u52a8\u5b8c\u6210\u4efb\u52a1\n\n\u5176\u4f18\u70b9\u662f\u65e0\u9700\u89c6\u89c9\u89e3\u6790\uff0c\u76f4\u63a5\u64cd\u4f5c\u7f51\u9875\u5143\u7d20\uff0c\u5927\u5927\u63d0\u5347\u4e86AI\u52a9\u624b\u4e0e\u7f51\u9875\u4ea4\u4e92\u901f\u5ea6\n\nhttps://github.com/MiguelsPizza/WebMCP\n\n<img width='680' height='489' alt='Image' src='https://github.com/user-attachments/assets/bfeb30f0-f224-46d5-b996-093be33de3ac' />\u3002", "top": 0, "createdAt": 1752725924, "style": "", "script": "", "head": "", "ogImage": "https://github.githubassets.com/favicons/favicon.svg", "createdDate": "2025-07-17", "dateLabelColor": "#0969da"}, "P48": {"htmlDir": "docs/post/yi-kuan-AI-zi-xun-xin-xi-guan-li-gong-ju-\uff1aRevornix\uff0c-ta-ke-yi-zheng-he-suo-you-ke-jian-xin-xi-\uff0c-an-she-ding-shi-jian-shu-chu-zong-he-bao-gao.html", "labels": ["AI", "Source Code"], "postTitle": "\u4e00\u6b3eAI\u8d44\u8baf\u4fe1\u606f\u7ba1\u7406\u5de5\u5177\uff1aRevornix\uff0c\u5b83\u53ef\u4ee5\u6574\u5408\u6240\u6709\u53ef\u89c1\u4fe1\u606f\uff0c\u6309\u8bbe\u5b9a\u65f6\u95f4\u8f93\u51fa\u7efc\u5408\u62a5\u544a", "postUrl": "post/yi-kuan-AI-zi-xun-xin-xi-guan-li-gong-ju-%EF%BC%9ARevornix%EF%BC%8C-ta-ke-yi-zheng-he-suo-you-ke-jian-xin-xi-%EF%BC%8C-an-she-ding-shi-jian-shu-chu-zong-he-bao-gao.html", "postSourceUrl": "https://github.com/winnerineast/winnerineast.github.io/issues/48", "commentNum": 0, "wordCount": 240, "description": "\u57fa\u4e8e\u591a\u6a21\u6001\u6a21\u578b\u6587\u6863\u8f6c\u5316\uff0c\u5e76\u5411\u91cf\u5316\u5b58\u50a8\n\n\u539f\u751f\u591a\u79df\u6237\uff0c\u652f\u6301\u591a\u7528\u6237\u5e76\u53d1\u4f7f\u7528\uff0c\u6bcf\u4e2a\u7528\u6237\u8bbe\u6709\u72ec\u7acb\u7684\u6587\u6863\u5e93\n\n\u5185\u7f6eMCP\u9a71\u52a8\u7684AI\u52a9\u624b\uff1b\u652f\u6301\u591a\u8bed\u8a00\n\nhttps://github.com/Qingyon-AI/Revornix\n\n<img width='680' height='423' alt='Image' src='https://github.com/user-attachments/assets/d0632569-d3cf-4386-8aa9-2a7bcb514599' />\u3002", "top": 0, "createdAt": 1752726079, "style": "", "script": "", "head": "", "ogImage": "https://github.githubassets.com/favicons/favicon.svg", "createdDate": "2025-07-17", "dateLabelColor": "#0969da"}, "P49": {"htmlDir": "docs/post/FireGEO-  -kai-yuan-de- SaaS -qi-dong-mo-ban-\uff0c-zhuan-wei-xi-wang-kuai-su-gou-jian-xian-dai-hua- Web -ying-yong-de-kai-fa-zhe-she-ji-\uff0c-ji-cheng-le-pin-pai-jian-kong-\u3001-yong-hu-ren-zheng-\u3001-ji-fei-xi-tong-he- AI -liao-tian-gong-neng-\u3002.html", "labels": ["Source Code"], "postTitle": "FireGEO:  \u5f00\u6e90\u7684 SaaS \u542f\u52a8\u6a21\u677f\uff0c\u4e13\u4e3a\u5e0c\u671b\u5feb\u901f\u6784\u5efa\u73b0\u4ee3\u5316 Web \u5e94\u7528\u7684\u5f00\u53d1\u8005\u8bbe\u8ba1\uff0c\u96c6\u6210\u4e86\u54c1\u724c\u76d1\u63a7\u3001\u7528\u6237\u8ba4\u8bc1\u3001\u8ba1\u8d39\u7cfb\u7edf\u548c AI \u804a\u5929\u529f\u80fd\u3002", "postUrl": "post/FireGEO-%20%20-kai-yuan-de-%20SaaS%20-qi-dong-mo-ban-%EF%BC%8C-zhuan-wei-xi-wang-kuai-su-gou-jian-xian-dai-hua-%20Web%20-ying-yong-de-kai-fa-zhe-she-ji-%EF%BC%8C-ji-cheng-le-pin-pai-jian-kong-%E3%80%81-yong-hu-ren-zheng-%E3%80%81-ji-fei-xi-tong-he-%20AI%20-liao-tian-gong-neng-%E3%80%82.html", "postSourceUrl": "https://github.com/winnerineast/winnerineast.github.io/issues/49", "commentNum": 1, "wordCount": 1245, "description": "\u9879\u76ee\u6838\u5fc3\uff1a\u5feb\u901f\u542f\u52a8\u4f60\u7684 SaaS\nFireGEO \u7684\u76ee\u6807\u662f\u5e2e\u5f00\u53d1\u8005\u7701\u53bb\u7e41\u7410\u7684\u914d\u7f6e\u548c\u642d\u5efa\u5de5\u4f5c\uff0c\u8ba9\u4f60\u4e13\u6ce8\u4e8e\u4e1a\u52a1\u903b\u8f91\u3002", "top": 0, "createdAt": 1752726150, "style": "", "script": "", "head": "", "ogImage": "https://github.githubassets.com/favicons/favicon.svg", "createdDate": "2025-07-17", "dateLabelColor": "#0969da"}, "P50": {"htmlDir": "docs/post/Mistral-gang-gang-kai-yuan-le-yi-kuan-yu-yin-li-jie-mo-xing-\uff1aVoxtral\uff0c-yu-yin-zhuan-lu-chao-guo-Whisper large-v3.html", "labels": ["AI", "Source Code"], "postTitle": "Mistral\u521a\u521a\u5f00\u6e90\u4e86\u4e00\u6b3e\u8bed\u97f3\u7406\u89e3\u6a21\u578b\uff1aVoxtral\uff0c\u8bed\u97f3\u8f6c\u5f55\u8d85\u8fc7Whisper large-v3", "postUrl": "post/Mistral-gang-gang-kai-yuan-le-yi-kuan-yu-yin-li-jie-mo-xing-%EF%BC%9AVoxtral%EF%BC%8C-yu-yin-zhuan-lu-chao-guo-Whisper%20large-v3.html", "postSourceUrl": "https://github.com/winnerineast/winnerineast.github.io/issues/50", "commentNum": 0, "wordCount": 227, "description": "\u6027\u80fd\u8d85GPT-4o mini Transcribe\u3001Gemini 2.5 Flash\uff0c\u5728\u82f1\u8bed\u77ed\u7bc7\u548cMozilla Common Voice\u4e0a\u8d85\u8fc7\u4e86ElevenLabs Scribe\n\n\u4e0a\u4e0b\u6587\u957f\u5ea632k token\n\u53ef\u5904\u7406\u957f30\u5206\u949f\u97f3\u9891\u8f6c\u5f55\uff0c\u621640\u5206\u949f\u7684\u8bed\u4e49\u7406\u89e3\n\n\u5185\u7f6e\u95ee\u7b54\u3001\u6458\u8981\u3001\u591a\u8bed\u8a00\u652f\u6301\u548c\u8bed\u97f3\u51fd\u6570\u8c03\u7528\u529f\u80fd\n\n\u6709\u4e24\u4e2a\u578b\u53f7\uff0cVoxtral (24B)\u3001Voxtral Mini (3B)\n\nhttps://huggingface.co/mistralai\u3002", "top": 0, "createdAt": 1752726248, "style": "", "script": "", "head": "", "ogImage": "https://github.githubassets.com/favicons/favicon.svg", "createdDate": "2025-07-17", "dateLabelColor": "#0969da"}, "P51": {"htmlDir": "docs/post/xun-lian- Agent -neng-li-de-zhuan-yong-kuang-jia- - ART.html", "labels": ["AI", "Source Code"], "postTitle": "\u8bad\u7ec3 Agent \u80fd\u529b\u7684\u4e13\u7528\u6846\u67b6 - ART", "postUrl": "post/xun-lian-%20Agent%20-neng-li-de-zhuan-yong-kuang-jia-%20-%20ART.html", "postSourceUrl": "https://github.com/winnerineast/winnerineast.github.io/issues/51", "commentNum": 0, "wordCount": 320, "description": "\u7b80\u5355\u6765\u8bb2\uff0c\u8fd9\u4e2a\u6846\u67b6\u53ef\u4ee5\u5c06 GRPO \u96c6\u6210\u5230\u4f60\u7684 python \u5e94\u7528\u4e2d\uff0c\u6bd4\u5982\u4f7f\u7528\u8fd9\u4e2a\u8bad\u7ec3 Qwen2.5-7B \u641c\u7d22\u90ae\u4ef6\uff0c\u6216\u8005\u73a9\u5404\u79cd\u6e38\u620f\u3002", "top": 0, "createdAt": 1752807326, "style": "", "script": "", "head": "", "ogImage": "https://github.githubassets.com/favicons/favicon.svg", "createdDate": "2025-07-18", "dateLabelColor": "#0969da"}, "P52": {"htmlDir": "docs/post/A Survey of Context Engineering.html", "labels": ["paper", "AI"], "postTitle": "A Survey of Context Engineering", "postUrl": "post/A%20Survey%20of%20Context%20Engineering.html", "postSourceUrl": "https://github.com/winnerineast/winnerineast.github.io/issues/52", "commentNum": 0, "wordCount": 3235, "description": "<img width='604' height='680' alt='Image' src='https://github.com/user-attachments/assets/52e55e02-c9fb-4dda-a584-69e7bad821c9' />\n\nThe paper provides a taxonomy of context engineering in LLMs categorized into foundational components, system implementations, evaluation methodologies, and future directions.\n\n<img width='680' height='634' alt='Image' src='https://github.com/user-attachments/assets/63546a00-ecb1-4c4f-93e7-9f436b1302bb' />\n\nThe context engineering evolution timeline from 2020 to 2025 involves foundational RAG systems to complex multi-agent architectures.\n\n<img width='680' height='373' alt='Image' src='https://github.com/user-attachments/assets/16d3a7b2-02a1-409e-b0fb-4bac0c04fe59' />\n\nThe work distinguishes prompt engineering from context engineering on dimensions like state, scalability, error analysis, complexity, etc.\n\n<img width='680' height='208' alt='Image' src='https://github.com/user-attachments/assets/f6fd6566-1e4a-4a97-80d0-7dc5ec102dc2' />\n\nContext engineering components include context retrieval and generation, context processing, context management, and how they are all integrated into systems implementation, such as RAG, memory architectures, tool-integrated reasoning, and multi-agent coordination mechanisms.\n\n<img width='679' height='245' alt='Image' src='https://github.com/user-attachments/assets/dce2fca1-293b-4ac0-a7ab-2466b14c4de8' />\n\nOne important aspect of context processing is contextual self-refinement, which aims to improve outputs through cyclical feedback mechanisms.\n\n<img width='680' height='453' alt='Image' src='https://github.com/user-attachments/assets/1b940c4e-b413-49e6-8226-ee2016f8f262' />\n\nAn important aspect of context management is how to deal efficiently with long context and reasoning chains. The paper provides an overview of and characteristics of key methods for long-chain reasoning.\n\n<img width='680' height='301' alt='Image' src='https://github.com/user-attachments/assets/a421300c-aac0-4342-94a8-06f258076879' />\n\nMemory is key to building complex agentic systems that can adapt, learn, and perform coherent long-term tasks.\n\n<img width='680' height='367' alt='Image' src='https://github.com/user-attachments/assets/586808eb-ebdd-4d6d-a54a-6624b06ca857' />\n\nThere is also a nice overview of different memory implementation patterns.\n\n<img width='680' height='624' alt='Image' src='https://github.com/user-attachments/assets/97001f83-a770-4bdc-a9bc-75b6b20da762' />\n\nTool-calling capabilities in an area of continuous development in the space. The paper provides an overview of tool-augmented language model architectures and how they compare across tool categories.\n\n<img width='680' height='409' alt='Image' src='https://github.com/user-attachments/assets/30414437-683b-489a-9972-47d09cae9003' />\n\nContext engineering is going to evolve rapidly.\n\nBut this is a great overview to better map and keep track of this rapidly evolving landscape.\n\nThere is a lot more in the paper. Over 1000+ references included.\n\nThis survey tries to capture the most common methods and biggest trends, but there is more on the horizon as models continue to improve in capability and new agent architectures emerge.\n\n[arxiv.org/abs/2507.13334](https://t.co/fj8CBVd46H)\u3002", "top": 0, "createdAt": 1752882939, "style": "", "script": "", "head": "", "ogImage": "https://github.githubassets.com/favicons/favicon.svg", "createdDate": "2025-07-19", "dateLabelColor": "#0969da"}, "P53": {"htmlDir": "docs/post/The Big LLM Architecture Comparison From DeepSeek-V3 to Kimi K2- A Look At Modern LLM Architecture Design.html", "labels": ["AI"], "postTitle": "The Big LLM Architecture Comparison From DeepSeek-V3 to Kimi K2: A Look At Modern LLM Architecture Design", "postUrl": "post/The%20Big%20LLM%20Architecture%20Comparison%20From%20DeepSeek-V3%20to%20Kimi%20K2-%20A%20Look%20At%20Modern%20LLM%20Architecture%20Design.html", "postSourceUrl": "https://github.com/winnerineast/winnerineast.github.io/issues/53", "commentNum": 0, "wordCount": 188, "description": "[The Big LLM Architecture Comparison\nFrom DeepSeek-V3 to Kimi K2: A Look At Modern LLM Architecture Design](https://sebastianraschka.com/blog/2025/the-big-llm-architecture-comparison.html)\u3002", "top": 0, "createdAt": 1753069638, "style": "", "script": "", "head": "", "ogImage": "https://github.githubassets.com/favicons/favicon.svg", "createdDate": "2025-07-21", "dateLabelColor": "#0969da"}, "P54": {"htmlDir": "docs/post/Rethinking industrial artificial intelligence- A unified foundation framework.html", "labels": ["paper", "AI"], "postTitle": "Rethinking industrial artificial intelligence: A unified foundation framework", "postUrl": "post/Rethinking%20industrial%20artificial%20intelligence-%20A%20unified%20foundation%20framework.html", "postSourceUrl": "https://github.com/winnerineast/winnerineast.github.io/issues/54", "commentNum": 0, "wordCount": 8735, "description": "\u7f8e\u56fd\u9a6c\u91cc\u5170\u5927\u5b66Jay Lee\u6559\u6388\u548cHanqi Su\u535a\u58eb\u6700\u65b0\u524d\u77bb\u6027\u6587\u7ae0\uff1a\u5de5\u4e1a\u4eba\u5de5\u667a\u80fd\u7684\u9769\u65b0\uff1a\u7edf\u4e00\u57fa\u7840\u6846\u67b6 | IJAMD\u5c01\u9762\u6587\u7ae0\n\n\u82f1\u6587\u539f\u9898\uff1aRethinking industrial artificial intelligence: A unified foundation framework\n\n\u901a\u8baf\u4f5c\u8005\uff1aHanqi Su, University of Maryland\n\n\u4f5c\u8005\uff1aJay Lee, Hanqi Su*\n\n\u5f15\u7528\u4fe1\u606f: \n\nLee J, Su H. Rethinking industrial artificial intelligence: A unified foundation framework. Int J AI Mater Design. 2025;2(2):56-68. doi: 10.36922/IJAMD025080006\n\n\n\n\u6587\u7ae0\u80cc\u666f\n\n\n\n\u5de5\u4e1a\u4eba\u5de5\u667a\u80fd\uff08AI\uff09\u7684\u5feb\u901f\u53d1\u5c55\u6b63\u5728\u91cd\u5851\u5168\u7403\u5404\u884c\u5404\u4e1a\u3002", "top": 0, "createdAt": 1754193682, "style": "", "script": "", "head": "", "ogImage": "https://github.githubassets.com/favicons/favicon.svg", "createdDate": "2025-08-03", "dateLabelColor": "#0969da"}, "P55": {"htmlDir": "docs/post/Efficient Agents- Building Effective Agents While Reducing Cost.html", "labels": ["paper", "AI"], "postTitle": "Efficient Agents: Building Effective Agents While Reducing Cost", "postUrl": "post/Efficient%20Agents-%20Building%20Effective%20Agents%20While%20Reducing%20Cost.html", "postSourceUrl": "https://github.com/winnerineast/winnerineast.github.io/issues/55", "commentNum": 0, "wordCount": 1594, "description": "The remarkable capabilities of Large Language Model (LLM)-driven agents have enabled sophisticated systems to tackle complex, multi-step tasks, but their escalating costs threaten scalability and accessibility. This work presents the first systematic study of the efficiency-effectiveness trade-off in modern agent systems, addressing the critical need for cost-effective designs without sacrificing performance. We investigate three key questions: (1) How much complexity do agentic tasks inherently require? (2) When do additional modules yield diminishing returns? (3) How much efficiency can be gained through the design of efficient agent frameworks? Through an empirical analysis on the GAIA benchmark, we evaluate the impact of LLM backbone selection, agent framework designs, and test-time scaling strategies. Using the cost-of-pass metric, we quantify the efficiency-performance trade-off across these dimensions. Our findings inform the development of Efficient Agents , a novel agent framework that has an optimal complexity to task requirements. Efficient Agents retains 96.7% of the performance of OWL, one leading open-source agent framework, while reducing operational costs from 0.398 to 0.228, resulting in a 28.4% improvement in cost-of-pass. Our work provides actionable insights for designing efficient, high-performing agent systems, advancing the accessibility and sustainability of AI-driven solutions.\n\nhttps://arxiv.org/abs/2508.02694\n\n\n<img width='1624' height='1868' alt='Image' src='https://github.com/user-attachments/assets/e9430bbd-31d2-4068-bd54-ab43fa3bcf3e' />\u3002", "top": 0, "createdAt": 1754537670, "style": "", "script": "", "head": "", "ogImage": "https://github.githubassets.com/favicons/favicon.svg", "createdDate": "2025-08-07", "dateLabelColor": "#0969da"}, "P56": {"htmlDir": "docs/post/Fine-tuning with gpt-oss and Hugging Face Transformers.html", "labels": ["AI", "Source Code"], "postTitle": "Fine-tuning with gpt-oss and Hugging Face Transformers", "postUrl": "post/Fine-tuning%20with%20gpt-oss%20and%20Hugging%20Face%20Transformers.html", "postSourceUrl": "https://github.com/winnerineast/winnerineast.github.io/issues/56", "commentNum": 1, "wordCount": 423, "description": "https://cookbook.openai.com/articles/gpt-oss/fine-tune-transfomers\n\nOpenAI \u521a\u653e\u51fa\u4e86\u6628\u5929\u53d1\u5e03\u7684 OSS \u7cfb\u5217\u5f00\u653e\u6743\u91cd\u6a21\u578b\u7684\u5fae\u8c03\u6559\u7a0b\u3002", "top": 0, "createdAt": 1754537955, "style": "", "script": "", "head": "", "ogImage": "https://github.githubassets.com/favicons/favicon.svg", "createdDate": "2025-08-07", "dateLabelColor": "#0969da"}, "P57": {"htmlDir": "docs/post/AI Real Estate Agent Team.html", "labels": ["AI", "Source Code"], "postTitle": "AI Real Estate Agent Team", "postUrl": "post/AI%20Real%20Estate%20Agent%20Team.html", "postSourceUrl": "https://github.com/winnerineast/winnerineast.github.io/issues/57", "commentNum": 0, "wordCount": 1068, "description": "https://cookbook.openai.com/articles/gpt-oss/fine-tune-transfomers\n\nOpenAI gpt-oss (\u5f00\u6e90 LLM) + \n[@firecrawl_dev](https://x.com/firecrawl_dev)\n (\u641c\u7d22) + \n[@AgnoAgi](https://x.com/AgnoAgi)\n (Agent \u6846\u67b6) \u6253\u9020\u4e00\u4e2a Multi-Agents AI \u623f\u5730\u4ea7\u4e2d\u4ecb\u56e2\u961f\uff0c\u5e2e\u4f60\u5feb\u901f\u627e\u5230\u5fc3\u4eea\u7684\u623f\u5b50 \ud83c\udfe1 \u2014\u2014 \u6765\u81ea \n[@Saboo_Shubham_](https://x.com/Saboo_Shubham_)\n \u7684\u6559\u7a0b \n[@unwind_ai_](https://x.com/unwind_ai_)\n \u548c\u5f00\u6e90\u9879\u76ee\u5b9e\u6218\u5206\u4eab \ud83d\udc4f\ud83c\udffb\n\n\u7cfb\u7edf\u6784\u6210\n\u7cfb\u7edf\u57fa\u4e8e Agno Agent \u6846\u67b6\uff0c\u7531\u4e09\u4e2a\u4e13\u4e1a AI Agent \u7ec4\u6210\uff1a\u623f\u4ea7\u641c\u7d22 Agent\u3001\u5e02\u573a\u5206\u6790 Agent \u548c \u623f\u4ea7\u4f30\u503c Agent\u3002", "top": 0, "createdAt": 1754538657, "style": "", "script": "", "head": "", "ogImage": "https://github.githubassets.com/favicons/favicon.svg", "createdDate": "2025-08-07", "dateLabelColor": "#0969da"}, "P58": {"htmlDir": "docs/post/The Generative AI Data Scientist is NOW what companies want..html", "labels": ["AI", "blog"], "postTitle": "The Generative AI Data Scientist is NOW what companies want.", "postUrl": "post/The%20Generative%20AI%20Data%20Scientist%20is%20NOW%20what%20companies%20want..html", "postSourceUrl": "https://github.com/winnerineast/winnerineast.github.io/issues/58", "commentNum": 0, "wordCount": 652, "description": "<img width='1696' height='900' alt='Image' src='https://github.com/user-attachments/assets/4d956c7e-6c86-4a7f-8722-bca4a765994d' />\n\nCompanies are sitting on mountains of unstructured data. \n\nPDF\nWord docs\nMeeting notes\nEmails\nVideos\nAudio Transcripts\n\nThis is useful data. But it's unusable in its existing form.\nThe AI data scientist builds the systems to analyze information, gain business insights, and automates the process. \n\n- Models the system\n- Use AI to extract insights\n- Drives predictive business insights\n\n<img width='1201' height='649' alt='Image' src='https://github.com/user-attachments/assets/9953b5dd-6d28-4e37-af9b-981119753c5e' />\n\u3002", "top": 0, "createdAt": 1754839767, "style": "", "script": "", "head": "", "ogImage": "https://github.githubassets.com/favicons/favicon.svg", "createdDate": "2025-08-10", "dateLabelColor": "#0969da"}, "P59": {"htmlDir": "docs/post/Hugging Face -tuan-dui-kai-yuan-le-yi-ge-qiang-da-de- AI -gong-ju-\uff1aAI Sheets.html", "labels": ["AI", "Source Code"], "postTitle": "Hugging Face \u56e2\u961f\u5f00\u6e90\u4e86\u4e00\u4e2a\u5f3a\u5927\u7684 AI \u5de5\u5177\uff1aAI Sheets", "postUrl": "post/Hugging%20Face%20-tuan-dui-kai-yuan-le-yi-ge-qiang-da-de-%20AI%20-gong-ju-%EF%BC%9AAI%20Sheets.html", "postSourceUrl": "https://github.com/winnerineast/winnerineast.github.io/issues/59", "commentNum": 0, "wordCount": 366, "description": "\u63d0\u4f9b\u7c7b\u4f3c Excel \u8868\u683c\u7684\u754c\u9762\uff0c\u65e0\u9700\u7f16\u5199\u4ee3\u7801\uff0c\u5373\u53ef\u4f7f\u7528\u6570\u5343\u4e2a\u5f00\u6e90\u6a21\u578b\u6765\u6784\u5efa\u3001\u4e30\u5bcc\u548c\u8f6c\u6362\u6570\u636e\u96c6\uff0c\u652f\u6301\u672c\u5730\u90e8\u7f72\u6216\u76f4\u63a5\u5728 Hub \u4e0a\u4f7f\u7528\u3002", "top": 0, "createdAt": 1754839903, "style": "", "script": "", "head": "", "ogImage": "https://github.githubassets.com/favicons/favicon.svg", "createdDate": "2025-08-10", "dateLabelColor": "#0969da"}, "P60": {"htmlDir": "docs/post/A 23-page research paper reveals the number 1 method Hedge Funds use to beat the market.html", "labels": ["paper", "AI"], "postTitle": "A 23-page research paper reveals the number 1 method Hedge Funds use to beat the market", "postUrl": "post/A%2023-page%20research%20paper%20reveals%20the%20number%201%20method%20Hedge%20Funds%20use%20to%20beat%20the%20market.html", "postSourceUrl": "https://github.com/winnerineast/winnerineast.github.io/issues/60", "commentNum": 0, "wordCount": 130, "description": "<img width='550' height='720' alt='Image' src='https://github.com/user-attachments/assets/4666d2c4-adba-4e95-a4b3-e7c60862f57e' />\u3002", "top": 0, "createdAt": 1754840077, "style": "", "script": "", "head": "", "ogImage": "https://github.githubassets.com/favicons/favicon.svg", "createdDate": "2025-08-10", "dateLabelColor": "#0969da"}, "P61": {"htmlDir": "docs/post/Embedding Atlas.html", "labels": ["AI", "Source Code"], "postTitle": "Embedding Atlas", "postUrl": "post/Embedding%20Atlas.html", "postSourceUrl": "https://github.com/winnerineast/winnerineast.github.io/issues/61", "commentNum": 0, "wordCount": 964, "description": "https://github.com/apple/embedding-atlas\n\nEmbedding Atlas is a tool that provides interactive visualizations for large embeddings. It allows you to visualize, cross-filter, and search embeddings and metadata.\n\nFeatures\n\n\ud83c\udff7\ufe0f Automatic data clustering & labeling: Interactively visualize and navigate overall data structure.\n\n\ud83e\udee7 Kernel density estimation & density contours: Easily explore and distinguish between dense regions of data and outliers.\n\n\ud83e\uddca Order-independent transparency: Ensure clear, accurate rendering of overlapping points.\n\n\ud83d\udd0d Real-time search & nearest neighbors: Find similar data to a given query or existing data point.\n\n\ud83d\ude80 WebGPU implementation (with WebGL 2 fallback): Fast, smooth performance (up to few million points) with modern rendering stack.\n\n\ud83d\udcca Multi-coordinated views for metadata exploration: Interactively link and filter data across metadata columns.\n\nPlease visit https://apple.github.io/embedding-atlas for a demo and documentation.\u3002", "top": 0, "createdAt": 1754964670, "style": "", "script": "", "head": "", "ogImage": "https://github.githubassets.com/favicons/favicon.svg", "createdDate": "2025-08-12", "dateLabelColor": "#0969da"}, "P62": {"htmlDir": "docs/post/Routine\uff1a-ba- GPT-4o -zhun-que-lv-cong- 41% -la-dao- 96% -de-qi-ye-ji- Agent -wen-ding-qi-\uff08-wan-zi-\uff09.html", "labels": ["AI", "blog"], "postTitle": "Routine\uff1a\u628a GPT-4o \u51c6\u786e\u7387\u4ece 41% \u62c9\u5230 96% \u7684\u4f01\u4e1a\u7ea7 Agent \u7a33\u5b9a\u5668\uff08\u4e07\u5b57\uff09", "postUrl": "post/Routine%EF%BC%9A-ba-%20GPT-4o%20-zhun-que-lv-cong-%2041%25%20-la-dao-%2096%25%20-de-qi-ye-ji-%20Agent%20-wen-ding-qi-%EF%BC%88-wan-zi-%EF%BC%89.html", "postSourceUrl": "https://github.com/winnerineast/winnerineast.github.io/issues/62", "commentNum": 0, "wordCount": 13478, "description": "\u201c Routine \u6846\u67b6\u7528 400 \u884c JSON \u628a GPT-4o \u5728\u4f01\u4e1a\u573a\u666f\u7684\u7aef\u5230\u7aef\u51c6\u786e\u7387\u4ece 41% \u62c9\u5230 96%\uff0c14B \u5c0f\u6a21\u578b\u4e5f\u80fd\u903c\u8fd1 SOTA\u2014\u2014\u672c\u6587\u7ae0\u62c6\u89e3\u5b83\u5230\u5e95\u505a\u5bf9\u4e86\u4ec0\u4e48\u3002", "top": 0, "createdAt": 1755183637, "style": "", "script": "", "head": "", "ogImage": "https://github.githubassets.com/favicons/favicon.svg", "createdDate": "2025-08-14", "dateLabelColor": "#0969da"}}, "singeListJson": {}, "labelColorDict": {"AI": "#c21255", "blog": "#aaaaaa", "bug": "#d73a4a", "documentation": "#0075ca", "duplicate": "#cfd3d7", "enhancement": "#a2eeef", "good first issue": "#7057ff", "help wanted": "#008672", "invalid": "#e4e669", "paper": "#8ef190", "question": "#d876e3", "software": "#aaaaaa", "Source Code": "#25b1d1", "wontfix": "#ffffff"}, "displayTitle": "Winnerineast Blog", "faviconUrl": "https://github.githubassets.com/favicons/favicon.svg", "ogImage": "https://github.githubassets.com/favicons/favicon.svg", "primerCSS": "<link href='https://mirrors.sustech.edu.cn/cdnjs/ajax/libs/Primer/21.0.7/primer.css' rel='stylesheet' />", "homeUrl": "https://winnerineast.github.io", "prevUrl": "/page4.html", "nextUrl": "disabled"}