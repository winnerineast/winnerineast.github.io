{"singlePage": [], "startSite": "", "filingNum": "", "onePageListNum": 15, "commentLabelColor": "#006b75", "yearColorList": ["#bc4c00", "#0969da", "#1f883d", "#A333D0"], "i18n": "CN", "themeMode": "manual", "dayTheme": "light", "nightTheme": "dark", "urlMode": "pinyin", "script": "", "style": "", "head": "", "indexScript": "", "indexStyle": "", "bottomText": "", "showPostSource": 1, "iconList": {}, "UTC": 8, "rssSplit": "sentence", "exlink": {}, "needComment": 1, "allHead": "", "title": "Winnerineast Blog", "subTitle": "Humachine Studio", "avatarUrl": "https://github.githubassets.com/favicons/favicon.svg", "GMEEK_VERSION": "last", "postListJson": {"P1": {"htmlDir": "docs/post/\u3010-lun-wen-tui-jian-\u3011DeepResearch.html", "labels": ["paper"], "postTitle": "\u3010\u8bba\u6587\u63a8\u8350\u3011DeepResearch", "postUrl": "post/%E3%80%90-lun-wen-tui-jian-%E3%80%91DeepResearch.html", "postSourceUrl": "https://github.com/winnerineast/winnerineast.github.io/issues/1", "commentNum": 0, "wordCount": 901, "description": "\uff08\u597d\u4e00\u4e9b\u7684\u7efc\u8ff0\uff09[https://arxiv.org/abs/2506.18096](https://t.co/KqENoLuAiv) \n\uff08\u8fc7\u4e8e\u6c42\u5168\u53cd\u800c\u7f3a\u4e4f\u6d1e\u89c1\u7684\u7efc\u8ff0\uff09 [https://arxiv.org/pdf/2506.12594](https://t.co/xeWo94wXZq) \n\uff08\u8bc4\u4f30\u65b9\u6cd5\uff0c\u4ece\u62a5\u544a\u8d28\u91cf\u548c\u5f15\u7528\u7cbe\u5ea6\u4e24\u4e2a\u89d2\u5ea6\u8bc4\u4f30\uff09[https://arxiv.org/pdf/2506.11763](https://t.co/vKR0NO4HLO)\n\n\uff08\u63a8\u8350\uff0c\u5e26UI\uff0c\u5b57\u8282\u5f00\u6e90\u7684\u57fa\u4e8eLangGraph\u7684DeepFlow\uff09[https://github.com/bytedance/deer-flow\u2026](https://t.co/EtqKMqlKBf) \n\uff08\u63a8\u8350\uff0c\u5e26UI\uff0cGemini\u57fa\u4e8eLangGraph\u7684Deep Search\u5b9e\u73b0\uff0c\u6bd4\u8f83\u7b80\u5355\uff09 [https://github.com/google-gemini/gemini-fullstack-langgraph-quickstart\u2026](https://t.co/VTe064Fsx2) \n\uff08\u6ca1\u6709UI\uff0c\u4f46\u662f\u529f\u80fd\u6bd4\u8f83\u4e30\u5bcc\uff09[https://github.com/foreveryh/mentis/tree/main/super_agents/deep_research\u2026](https://t.co/9TJy0juGLG)\n[](https://t.co/VTe064Fsx2)\n\uff08\u57fa\u4e8esmolagents\u6846\u67b6\uff0c\u6bd4\u8f83\u7b80\u5355\uff09[https://github.com/huggingface/smolagents/tree/main/examples/open_deep_research\u2026](https://t.co/9wXwCsbxeL) \uff08\u57fa\u4e8eOpenAI Agents \u5e93\u5b9e\u73b0\uff0c\u6bd4\u8f83\u7b80\u5355\uff09 [https://huggingface.co/spaces/mallocode200/Deep_Research_Assistant/blob/main/research_manager.py](https://t.co/BaoHko5WT9)\u3002", "top": 0, "createdAt": 1752315239, "style": "", "script": "", "head": "", "ogImage": "https://github.githubassets.com/favicons/favicon.svg", "createdDate": "2025-07-12", "dateLabelColor": "#0969da"}, "P2": {"htmlDir": "docs/post/AI-zhi-hui-yu-jian-suo-de-dian-feng-rong-he-\uff1aRAG-jie-suo-xia-yi-dai-xin-xi-sheng-cheng-xin-jia-gou.html", "labels": ["AI"], "postTitle": "AI\u667a\u6167\u4e0e\u68c0\u7d22\u7684\u5dc5\u5cf0\u878d\u5408\uff1aRAG\u89e3\u9501\u4e0b\u4e00\u4ee3\u4fe1\u606f\u751f\u6210\u65b0\u67b6\u6784", "postUrl": "post/AI-zhi-hui-yu-jian-suo-de-dian-feng-rong-he-%EF%BC%9ARAG-jie-suo-xia-yi-dai-xin-xi-sheng-cheng-xin-jia-gou.html", "postSourceUrl": "https://github.com/winnerineast/winnerineast.github.io/issues/2", "commentNum": 0, "wordCount": 1894, "description": "**GPTDAOCN-e/acc** @GPTDAOCN [2024-12-11](https://x.com/GPTDAOCN/status/1866947666690838557)\n\nAI\u667a\u6167\u4e0e\u68c0\u7d22\u7684\u5dc5\u5cf0\u878d\u5408\uff1aRAG\u89e3\u9501\u4e0b\u4e00\u4ee3\u4fe1\u606f\u751f\u6210\u65b0\u67b6\u6784\n\n\u8fd9\u5f20\u56fe\u5c55\u793a\u4e86'\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08Retrieval-Augmented Generation\uff0cRAG\uff09'\u6280\u672f\u7684\u4e0d\u540c\u67b6\u6784\uff0c\u7528\u6765\u63cf\u8ff0\u5982\u4f55\u7ed3\u5408\u4fe1\u606f\u68c0\u7d22\u548c\u751f\u6210\u5f0fAI\u6a21\u578b\uff08\u5982GPT\uff09\u6765\u66f4\u597d\u5730\u56de\u7b54\u95ee\u9898\u6216\u751f\u6210\u5185\u5bb9\u3002", "top": 0, "createdAt": 1752321646, "style": "", "script": "", "head": "", "ogImage": "https://github.githubassets.com/favicons/favicon.svg", "createdDate": "2025-07-12", "dateLabelColor": "#0969da"}, "P3": {"htmlDir": "docs/post/Building effective agents.html", "labels": ["AI"], "postTitle": "Building effective agents", "postUrl": "post/Building%20effective%20agents.html", "postSourceUrl": "https://github.com/winnerineast/winnerineast.github.io/issues/3", "commentNum": 0, "wordCount": 18822, "description": "Over the past year, we've worked with dozens of teams building large language model (LLM) agents across industries. Consistently, the most successful implementations weren't using complex frameworks or specialized libraries. Instead, they were building with simple, composable patterns.\n\nIn this post, we share what we\u2019ve learned from working with our customers and building agents ourselves, and give practical advice for developers on building effective agents.\n\n## What are agents?\n\n'Agent' can be defined in several ways. Some customers define agents as fully autonomous systems that operate independently over extended periods, using various tools to accomplish complex tasks. Others use the term to describe more prescriptive implementations that follow predefined workflows. At Anthropic, we categorize all these variations as **agentic systems**, but draw an important architectural distinction between **workflows** and **agents**:\n\n- **Workflows** are systems where LLMs and tools are orchestrated through predefined code paths.\n- **Agents**, on the other hand, are systems where LLMs dynamically direct their own processes and tool usage, maintaining control over how they accomplish tasks.\n\nBelow, we will explore both types of agentic systems in detail. In Appendix 1 (\u201cAgents in Practice\u201d), we describe two domains where customers have found particular value in using these kinds of systems.\n\n## When (and when not) to use agents\n\nWhen building applications with LLMs, we recommend finding the simplest solution possible, and only increasing complexity when needed. This might mean not building agentic systems at all. Agentic systems often trade latency and cost for better task performance, and you should consider when this tradeoff makes sense.\n\nWhen more complexity is warranted, workflows offer predictability and consistency for well-defined tasks, whereas agents are the better option when flexibility and model-driven decision-making are needed at scale. For many applications, however, optimizing single LLM calls with retrieval and in-context examples is usually enough.\n\n## When and how to use frameworks\n\nThere are many frameworks that make agentic systems easier to implement, including:\n\n- [LangGraph](https://langchain-ai.github.io/langgraph/) from LangChain;\n- Amazon Bedrock's [AI Agent framework](https://aws.amazon.com/bedrock/agents/);\n- [Rivet](https://rivet.ironcladapp.com/), a drag and drop GUI LLM workflow builder; and\n- [Vellum](https://www.vellum.ai/), another GUI tool for building and testing complex workflows.\n\nThese frameworks make it easy to get started by simplifying standard low-level tasks like calling LLMs, defining and parsing tools, and chaining calls together. However, they often create extra layers of abstraction that can obscure the underlying prompts \u200b\u200band responses, making them harder to debug. They can also make it tempting to add complexity when a simpler setup would suffice.\n\nWe suggest that developers start by using LLM APIs directly: many patterns can be implemented in a few lines of code. If you do use a framework, ensure you understand the underlying code. Incorrect assumptions about what's under the hood are a common source of customer error.\n\nSee our [cookbook](https://github.com/anthropics/anthropic-cookbook/tree/main/patterns/agents) for some sample implementations.\n\n## Building blocks, workflows, and agents\n\nIn this section, we\u2019ll explore the common patterns for agentic systems we\u2019ve seen in production. We'll start with our foundational building block\u2014the augmented LLM\u2014and progressively increase complexity, from simple compositional workflows to autonomous agents.\n\n### Building block: The augmented LLM\n\nThe basic building block of agentic systems is an LLM enhanced with augmentations such as retrieval, tools, and memory. Our current models can actively use these capabilities\u2014generating their own search queries, selecting appropriate tools, and determining what information to retain.\n\nWe recommend focusing on two key aspects of the implementation: tailoring these capabilities to your specific use case and ensuring they provide an easy, well-documented interface for your LLM. While there are many ways to implement these augmentations, one approach is through our recently released [Model Context Protocol](https://www.anthropic.com/news/model-context-protocol), which allows developers to integrate with a growing ecosystem of third-party tools with a simple [client implementation](https://modelcontextprotocol.io/tutorials/building-a-client#building-mcp-clients).\n\nFor the remainder of this post, we'll assume each LLM call has access to these augmented capabilities.\n\n### Workflow: Prompt chaining\n\nPrompt chaining decomposes a task into a sequence of steps, where each LLM call processes the output of the previous one. You can add programmatic checks (see 'gate\u201d in the diagram below) on any intermediate steps to ensure that the process is still on track.\n\n**When to use this workflow:** This workflow is ideal for situations where the task can be easily and cleanly decomposed into fixed subtasks. The main goal is to trade off latency for higher accuracy, by making each LLM call an easier task.\n\n**Examples where prompt chaining is useful:**\n\n- Generating Marketing copy, then translating it into a different language.\n- Writing an outline of a document, checking that the outline meets certain criteria, then writing the document based on the outline.\n\n### Workflow: Routing\n\nRouting classifies an input and directs it to a specialized followup task. This workflow allows for separation of concerns, and building more specialized prompts. Without this workflow, optimizing for one kind of input can hurt performance on other inputs.\n\n**When to use this workflow:** Routing works well for complex tasks where there are distinct categories that are better handled separately, and where classification can be handled accurately, either by an LLM or a more traditional classification model/algorithm.\n\n**Examples where routing is useful:**\n\n- Directing different types of customer service queries (general questions, refund requests, technical support) into different downstream processes, prompts, and tools.\n- Routing easy/common questions to smaller models like Claude 3.5 Haiku and hard/unusual questions to more capable models like Claude 3.5 Sonnet to optimize cost and speed.\n\n### Workflow: Parallelization\n\nLLMs can sometimes work simultaneously on a task and have their outputs aggregated programmatically. This workflow, parallelization, manifests in two key variations:\n\n- **Sectioning**: Breaking a task into independent subtasks run in parallel.\n- **Voting:** Running the same task multiple times to get diverse outputs.\n\n**When to use this workflow:** Parallelization is effective when the divided subtasks can be parallelized for speed, or when multiple perspectives or attempts are needed for higher confidence results. For complex tasks with multiple considerations, LLMs generally perform better when each consideration is handled by a separate LLM call, allowing focused attention on each specific aspect.\n\n**Examples where parallelization is useful:**\n\n- **Sectioning**:\n- Implementing guardrails where one model instance processes user queries while another screens them for inappropriate content or requests. This tends to perform better than having the same LLM call handle both guardrails and the core response.\n- Automating evals for evaluating LLM performance, where each LLM call evaluates a different aspect of the model\u2019s performance on a given prompt.\n- **Voting**:\n- Reviewing a piece of code for vulnerabilities, where several different prompts review and flag the code if they find a problem.\n- Evaluating whether a given piece of content is inappropriate, with multiple prompts evaluating different aspects or requiring different vote thresholds to balance false positives and negatives.\n\n### Workflow: Orchestrator-workers\n\nIn the orchestrator-workers workflow, a central LLM dynamically breaks down tasks, delegates them to worker LLMs, and synthesizes their results.\n\n**When to use this workflow:** This workflow is well-suited for complex tasks where you can\u2019t predict the subtasks needed (in coding, for example, the number of files that need to be changed and the nature of the change in each file likely depend on the task). Whereas it\u2019s topographically similar, the key difference from parallelization is its flexibility\u2014subtasks aren't pre-defined, but determined by the orchestrator based on the specific input.\n\n**Example where orchestrator-workers is useful:**\n\n- Coding products that make complex changes to multiple files each time.\n- Search tasks that involve gathering and analyzing information from multiple sources for possible relevant information.\n\n### Workflow: Evaluator-optimizer\n\nIn the evaluator-optimizer workflow, one LLM call generates a response while another provides evaluation and feedback in a loop.\n\n**When to use this workflow:** This workflow is particularly effective when we have clear evaluation criteria, and when iterative refinement provides measurable value. The two signs of good fit are, first, that LLM responses can be demonstrably improved when a human articulates their feedback; and second, that the LLM can provide such feedback. This is analogous to the iterative writing process a human writer might go through when producing a polished document.\n\n**Examples where evaluator-optimizer is useful:**\n\n- Literary translation where there are nuances that the translator LLM might not capture initially, but where an evaluator LLM can provide useful critiques.\n- Complex search tasks that require multiple rounds of searching and analysis to gather comprehensive information, where the evaluator decides whether further searches are warranted.\n\n### Agents\n\nAgents are emerging in production as LLMs mature in key capabilities\u2014understanding complex inputs, engaging in reasoning and planning, using tools reliably, and recovering from errors. Agents begin their work with either a command from, or interactive discussion with, the human user. Once the task is clear, agents plan and operate independently, potentially returning to the human for further information or judgement. During execution, it's crucial for the agents to gain \u201cground truth\u201d from the environment at each step (such as tool call results or code execution) to assess its progress. Agents can then pause for human feedback at checkpoints or when encountering blockers. The task often terminates upon completion, but it\u2019s also common to include stopping conditions (such as a maximum number of iterations) to maintain control.\n\nAgents can handle sophisticated tasks, but their implementation is often straightforward. They are typically just LLMs using tools based on environmental feedback in a loop. It is therefore crucial to design toolsets and their documentation clearly and thoughtfully. We expand on best practices for tool development in Appendix 2 ('Prompt Engineering your Tools').\n\n**When to use agents:** Agents can be used for open-ended problems where it\u2019s difficult or impossible to predict the required number of steps, and where you can\u2019t hardcode a fixed path. The LLM will potentially operate for many turns, and you must have some level of trust in its decision-making. Agents' autonomy makes them ideal for scaling tasks in trusted environments.\n\nThe autonomous nature of agents means higher costs, and the potential for compounding errors. We recommend extensive testing in sandboxed environments, along with the appropriate guardrails.\n\n**Examples where agents are useful:**\n\nThe following examples are from our own implementations:\n\n- A coding Agent to resolve [SWE-bench tasks](https://www.anthropic.com/research/swe-bench-sonnet), which involve edits to many files based on a task description;\n- Our [\u201ccomputer use\u201d reference implementation](https://github.com/anthropics/anthropic-quickstarts/tree/main/computer-use-demo), where Claude uses a computer to accomplish tasks.\n\n## Combining and customizing these patterns\n\nThese building blocks aren't prescriptive. They're common patterns that developers can shape and combine to fit different use cases. The key to success, as with any LLM features, is measuring performance and iterating on implementations. To repeat: you should consider adding complexity *only* when it demonstrably improves outcomes.\n\n## Summary\n\nSuccess in the LLM space isn't about building the most sophisticated system. It's about building the *right* system for your needs. Start with simple prompts, optimize them with comprehensive evaluation, and add multi-step agentic systems only when simpler solutions fall short.\n\nWhen implementing agents, we try to follow three core principles:\n\n1. Maintain **simplicity** in your agent's design.\n2. Prioritize **transparency** by explicitly showing the agent\u2019s planning steps.\n3. Carefully craft your agent-computer interface (ACI) through thorough tool **documentation and testing**.\n\nFrameworks can help you get started quickly, but don't hesitate to reduce abstraction layers and build with basic components as you move to production. By following these principles, you can create agents that are not only powerful but also reliable, maintainable, and trusted by their users.\n\n### Acknowledgements\n\nWritten by Erik Schluntz and Barry Zhang. This work draws upon our experiences building agents at Anthropic and the valuable insights shared by our customers, for which we're deeply grateful.\n\n## Appendix 1: Agents in practice\n\nOur work with customers has revealed two particularly promising applications for AI agents that demonstrate the practical value of the patterns discussed above. Both applications illustrate how agents add the most value for tasks that require both conversation and action, have clear success criteria, enable feedback loops, and integrate meaningful human oversight.\n\n### A. Customer support\n\nCustomer support combines familiar chatbot interfaces with enhanced capabilities through tool integration. This is a natural fit for more open-ended agents because:\n\n- Support interactions naturally follow a conversation flow while requiring access to external information and actions;\n- Tools can be integrated to pull customer data, order history, and knowledge base articles;\n- Actions such as issuing refunds or updating tickets can be handled programmatically; and\n- Success can be clearly measured through user-defined resolutions.\n\nSeveral companies have demonstrated the viability of this approach through usage-based pricing models that charge only for successful resolutions, showing confidence in their agents' effectiveness.\n\n### B. Coding agents\n\nThe software development space has shown remarkable potential for LLM features, with capabilities evolving from code completion to autonomous problem-solving. Agents are particularly effective because:\n\n- Code solutions are verifiable through automated tests;\n- Agents can iterate on solutions using test results as feedback;\n- The problem space is well-defined and structured; and\n- Output quality can be measured objectively.\n\nIn our own implementation, agents can now solve real GitHub issues in the [SWE-bench Verified](https://www.anthropic.com/research/swe-bench-sonnet) benchmark based on the pull request description alone. However, whereas automated testing helps verify functionality, human review remains crucial for ensuring solutions align with broader system requirements.\n\n## Appendix 2: Prompt engineering your tools\n\nNo matter which agentic system you're building, tools will likely be an important part of your agent. [Tools](https://www.anthropic.com/news/tool-use-ga) enable Claude to interact with external services and APIs by specifying their exact structure and definition in our API. When Claude responds, it will include a [tool use block](https://docs.anthropic.com/en/docs/build-with-claude/tool-use#example-api-response-with-a-tool-use-content-block) in the API response if it plans to invoke a tool. Tool definitions and specifications should be given just as much prompt engineering attention as your overall prompts. In this brief appendix, we describe how to prompt engineer your tools.\n\nThere are often several ways to specify the same action. For instance, you can specify a file edit by writing a diff, or by rewriting the entire file. For structured output, you can return code inside markdown or inside JSON. In software engineering, differences like these are cosmetic and can be converted losslessly from one to the other. However, some formats are much more difficult for an LLM to write than others. Writing a diff requires knowing how many lines are changing in the chunk header before the new code is written. Writing code inside JSON (compared to markdown) requires extra escaping of newlines and quotes.\n\nOur suggestions for deciding on tool formats are the following:\n\n- Give the model enough tokens to 'think' before it writes itself into a corner.\n- Keep the format close to what the model has seen naturally occurring in text on the internet.\n- Make sure there's no formatting 'overhead' such as having to keep an accurate count of thousands of lines of code, or string-escaping any code it writes.\n\nOne rule of thumb is to think about how much effort goes into human-computer interfaces (HCI), and plan to invest just as much effort in creating good *agent*\\-computer interfaces (ACI). Here are some thoughts on how to do so:\n\n- Put yourself in the model's shoes. Is it obvious how to use this tool, based on the description and parameters, or would you need to think carefully about it? If so, then it\u2019s probably also true for the model. A good tool definition often includes example usage, edge cases, input format requirements, and clear boundaries from other tools.\n- How can you change parameter names or descriptions to make things more obvious? Think of this as writing a great docstring for a junior developer on your team. This is especially important when using many similar tools.\n- Test how the model uses your tools: Run many example inputs in our [workbench](https://console.anthropic.com/workbench) to see what mistakes the model makes, and iterate.\n- [Poka-yoke](https://en.wikipedia.org/wiki/Poka-yoke) your tools. Change the arguments so that it is harder to make mistakes.\n\nWhile building our agent for [SWE-bench](https://www.anthropic.com/research/swe-bench-sonnet), we actually spent more time optimizing our tools than the overall prompt. For example, we found that the model would make mistakes with tools using relative filepaths after the agent had moved out of the root directory. To fix this, we changed the tool to always require absolute filepaths\u2014and we found that the model used this method flawlessly.\u3002", "top": 0, "createdAt": 1752321675, "style": "", "script": "", "head": "", "ogImage": "https://github.githubassets.com/favicons/favicon.svg", "createdDate": "2025-07-12", "dateLabelColor": "#0969da"}, "P4": {"htmlDir": "docs/post/Deep Research,  WebDancer,  WebSailor.html", "labels": ["AI", "Source Code"], "postTitle": "Deep Research,  WebDancer,  WebSailor", "postUrl": "post/Deep%20Research%2C%20%20WebDancer%2C%20%20WebSailor.html", "postSourceUrl": "https://github.com/winnerineast/winnerineast.github.io/issues/4", "commentNum": 0, "wordCount": 748, "description": "\n- \u95ee\uff1a\u201c\u4e00\u90e8\u77e5\u540d\u7535\u89c6\u5267\uff1a\u5973\u4e8c 1993 \u5e74\u5165\u884c\uff1b\u5973\u4e00\u73b0\u4efb\u4e08\u592b\u662f\u6d59\u6c5f\u6e56\u5dde\u4eba\uff1b\u7537\u4e00\u516d\u5e74\u540e\u767b\u4e0a\u6625\u665a\u3002", "top": 0, "createdAt": 1752321703, "style": "", "script": "", "head": "", "ogImage": "https://github.githubassets.com/favicons/favicon.svg", "createdDate": "2025-07-12", "dateLabelColor": "#0969da"}, "P5": {"htmlDir": "docs/post/MUVERA--rang-duo-xiang-liang-jian-suo-yu-dan-xiang-liang-sou-suo-yi-yang-kuai.html", "labels": ["paper", "AI", "Source Code"], "postTitle": "MUVERA-\u8ba9\u591a\u5411\u91cf\u68c0\u7d22\u4e0e\u5355\u5411\u91cf\u641c\u7d22\u4e00\u6837\u5feb", "postUrl": "post/MUVERA--rang-duo-xiang-liang-jian-suo-yu-dan-xiang-liang-sou-suo-yi-yang-kuai.html", "postSourceUrl": "https://github.com/winnerineast/winnerineast.github.io/issues/5", "commentNum": 0, "wordCount": 786, "description": "\u505aRAG\u7684\u670b\u53cb\u4e00\u5b9a\u8981\u770b\u770b Google \u8fd9\u4e2a\u65b0\u8bba\u6587\u2014\u2014MUVERA\uff1a\u8ba9\u591a\u5411\u91cf\u68c0\u7d22\u4e0e\u5355\u5411\u91cf\u641c\u7d22\u4e00\u6837\u5feb \u5927\u5bb6\u5728RAG\u7684\u68c0\u7d22\u5185\u5bb9\u8fc7\u7a0b\u90fd\u4f1a\u9047\u5230\u8fd9\u79cd\u60c5\u51b5\uff0c\u5982\u679c\u7528\u4f20\u7edf\u641c\u7d22\uff08\u4f8b\u5982ElasticSearch\uff09\uff0c\u6587\u6863 = 1 \u4e2a\u5411\u91cf \u2192 \u5feb\u901f\u4f46\u4e0d\u51c6\u786e\u3002", "top": 0, "createdAt": 1752321737, "style": "", "script": "", "head": "", "ogImage": "https://github.githubassets.com/favicons/favicon.svg", "createdDate": "2025-07-12", "dateLabelColor": "#0969da"}, "P6": {"htmlDir": "docs/post/Optimizing ColPali for Retrieval at Scale, 13x Faster Results.html", "labels": ["AI", "Source Code"], "postTitle": "Optimizing ColPali for Retrieval at Scale, 13x Faster Results", "postUrl": "post/Optimizing%20ColPali%20for%20Retrieval%20at%20Scale%2C%2013x%20Faster%20Results.html", "postSourceUrl": "https://github.com/winnerineast/winnerineast.github.io/issues/6", "commentNum": 0, "wordCount": 6181, "description": "\nColPali is a fascinating leap in document retrieval. Its precision in handling visually rich PDFs is phenomenal, but scaling it to handle real-world datasets comes with its share of computational challenges.\n\nHere\u2019s how we solved these challenges to make ColPali 13x faster without sacrificing the precision it\u2019s known for.\n\n## [](https://qdrant.tech/blog/colpali-qdrant-optimization/#the-scaling-dilemma)The Scaling Dilemma\n\nColPali generates\u00a0**1,030 vectors for just one page of a PDF.**\u00a0While this is manageable for small-scale tasks, in a real-world production setting where you may need to store hundreds od thousands of PDFs, the challenge of scaling becomes significant.\n\nConsider this scenario:\n\n- **Dataset Size:**\u00a020,000 PDF pages.\n- **Number of Vectors:**\u00a0Each page generates ~1,000 vectors of 128 dimensions.\n\nThe total number of comparisons is calculated as:\n\n1,000\u22c51,000\u22c520,000\u22c5128=2.56\u00d71012\u00a0comparisons!\n\nThat\u2019s trillions of comparisons needed to build the index. Even advanced indexing algorithms like\u00a0**HNSW**\u00a0struggle with this scale, as computational costs grow quadratically with amount of multivectors per page.\n\nWe turned to a hybrid optimization strategy combining\u00a0**pooling**\u00a0(to reduce computational overhead) and\u00a0**reranking**\u00a0(to preserve accuracy).\n\nBefore we go any deeper, watch our\u00a0[Webinar video](https://www.youtube.com/live/_h6SN1WwnLs?si=n8gwiIjJ5dnfucXC)\u00a0for the full demo walkthrough.\n\nFor those eager to explore, the\u00a0[codebase is available here](https://github.com/qdrant/demo-colpali-optimized).\n\n## [](https://qdrant.tech/blog/colpali-qdrant-optimization/#two-stage-retrieval-process)Two-Stage Retrieval Process\n\n### [](https://qdrant.tech/blog/colpali-qdrant-optimization/#pooling)Pooling\n\nPooling is well-known in machine learning as a way to compress data while keeping important information. For ColPali, we reduced 1,030 vectors per page to just 38 vectors by pooling rows in the document\u2019s 32x32 grid.\n\n![](https://qdrant.tech/blog/colpali-optimization/rows.png)\n\nMax and mean pooling are the two most popular types, so we decided to test both approaches on the rows of the grid. Likewise, we could apply pooling on columns, which we plan to explore in the future.\n\n- **Mean Pooling:**\u00a0Averages values across rows.\n- **Max Pooling:**\u00a0Selects the maximum value for each feature.\n\n32 vectors represent the pooled rows, while 6 vectors encode contextual information derived from ColPali\u2019s special tokens (e.g.,\u00a0for the beginning of the sequence, and task-specific instructions like \u201cDescribe the image\u201d).\n\nFor our experiments, we chose to preserve these 6 additional vectors.\n\n### [](https://qdrant.tech/blog/colpali-qdrant-optimization/#the-colpali-as-a-reranker-experiment)The \u201cColPali as a Reranker\u201d Experiment\n\nPooling drastically reduces retrieval costs, but there\u2019s a risk of losing fine-grained precision. To address this, we implemented a\u00a0**two-stage retrieval system**, where embeddings generated with ColPali were max/mean pooled by grid rows to create lightweight vectors for the initial retrieval stage, followed by reranking with the original high-resolution embeddings:\n\n1. **Pooled Retrieval:**\u00a0Quickly retrieves the top 200 candidates using lightweight pooled embeddings.\n2. **Full Reranking:**\u00a0Refines these candidates using the original, high-resolution embeddings, delivering the final top 20 results.\n\n### [](https://qdrant.tech/blog/colpali-qdrant-optimization/#implementation)Implementation\n\nWe created a custom dataset with over 20,000 unique PDF pages by merging:\n\n- **ViDoRe Benchmark:**\u00a0Designed for PDF documents retrieval evaluation.\n- **UFO Dataset:**\u00a0Visually rich documents paired with synthetic queries\u00a0[generated by Daniel van Strien](https://huggingface.co/datasets/davanstrien/ufo-ColPali).\n- **DocVQA Dataset:**\u00a0A large set of document-derived Q&A pairs.\n\nEach document was processed into 32x32 grids, generating both full-resolution and pooled embeddings.\u00a0**Full-resolution**\u00a0embeddings consisted of 1,030 vectors per page, while\u00a0**pooled embeddings**\u00a0included mean and max pooling variants.\n\nAll embeddings were were stored and kept in RAM to avoid caching effects during retrieval speed experiments.\n\n### [](https://qdrant.tech/blog/colpali-qdrant-optimization/#experiment-setup)Experiment Setup\n\nWe evaluated retrieval quality with 1,000 queries. First, pooled embeddings retrieved the top 200 candidates. Then, full-resolution embeddings reranked them to produce the final top 20 results.\n\nTo measure performance, we used:\n\n- **NDCG@20:**\u00a0Measures ranking quality (how well the top results align with expectations).\n- **Recall@20:**\u00a0Measures the overlap between this method and the original ColPali retrieval.\n\n## [](https://qdrant.tech/blog/colpali-qdrant-optimization/#results)Results\n\nThe experiment showed promising improvements in speed and accuracy. Retrieval time improved\u00a0**13x**\u00a0compared to using full-resolution embeddings alone.\n\n### [](https://qdrant.tech/blog/colpali-qdrant-optimization/#metrics)Metrics\n\n|Pooling Type|NDCG@20|Recall@20|\n|---|---|---|\n|**Mean**|0.952|0.917|\n|**Max**|0.759|0.656|\n\nMean pooling preserved nearly identical quality to the original ColPali, with NDCG@20 = 0.952 and Recall@20 = 0.917. Max pooling did not perform well enough to be considered viable since it sacrificed significant accuracy without delivering a meaningful speed advantage.\n\n## [](https://qdrant.tech/blog/colpali-qdrant-optimization/#whats-next)What\u2019s Next?\n\nFuture experiments could push these results even further:\n\n- Investigating column-wise pooling for additional compression.\n- Testing half-precision (float16) vectors to balance memory use and speed.\n- Skipping special multivectors during prefetch to streamline retrieval.\n- Combining quantization with oversampling for even faster search.\n\n### [](https://qdrant.tech/blog/colpali-qdrant-optimization/#try-it-yourself)Try It Yourself\n\nCurious to see this in action? Explore the full codebase and experiment with ColPali optimizations:\n\n- **Demo Notebook:**\u00a0[GitHub Repository](https://github.com/qdrant/demo-colpali-optimized)\n- **Webinar Walkthrough:**\u00a0[Watch Here](https://www.youtube.com/live/_h6SN1WwnLs?si=n8gwiIjJ5dnfucXC)\u3002", "top": 0, "createdAt": 1752321794, "style": "", "script": "", "head": "", "ogImage": "https://github.githubassets.com/favicons/favicon.svg", "createdDate": "2025-07-12", "dateLabelColor": "#0969da"}, "P7": {"htmlDir": "docs/post/RAG -shi-zhan-fen-xiang-  - 50-60GB PDF -wen-jian-, 6000 -pian-yi-xue-lun-wen-, -neng-zuo- RAG -ma.html", "labels": ["paper", "AI", "Source Code"], "postTitle": "RAG \u5b9e\u6218\u5206\u4eab  - 50-60GB PDF \u6587\u4ef6, 6000 \u7bc7\u533b\u5b66\u8bba\u6587, \u80fd\u505a RAG \u5417", "postUrl": "post/RAG%20-shi-zhan-fen-xiang-%20%20-%2050-60GB%20PDF%20-wen-jian-%2C%206000%20-pian-yi-xue-lun-wen-%2C%20-neng-zuo-%20RAG%20-ma.html", "postSourceUrl": "https://github.com/winnerineast/winnerineast.github.io/issues/7", "commentNum": 0, "wordCount": 1430, "description": "\u4f5c\u8005 Scott \u662f\u5fae\u8f6f\u5f00\u53d1\u8005\u793e\u533a VP, \u8fd9\u5e94\u8be5\u4e5f\u662f\u4ed6\u5728\u5f00\u53d1\u8005\u4e2d\u9047\u5230\u7684\u5b9e\u9645\u95ee\u9898, \u8fd9\u4e2a\u95ee\u9898\u7684\u8ba8\u8bba\u975e\u5e38\u6fc0\u70c8\u3002", "top": 0, "createdAt": 1752321822, "style": "", "script": "", "head": "", "ogImage": "https://github.githubassets.com/favicons/favicon.svg", "createdDate": "2025-07-12", "dateLabelColor": "#0969da"}, "P8": {"htmlDir": "docs/post/Reliable Agentic RAG with LLM Trustworthiness Estimates.html", "labels": ["paper", "AI", "Source Code"], "postTitle": "Reliable Agentic RAG with LLM Trustworthiness Estimates", "postUrl": "post/Reliable%20Agentic%20RAG%20with%20LLM%20Trustworthiness%20Estimates.html", "postSourceUrl": "https://github.com/winnerineast/winnerineast.github.io/issues/8", "commentNum": 0, "wordCount": 14009, "description": "September 12, 2024\n\n- ![Chris Mauck](https://cleanlab.ai/_next/static/images/chris-2be9b6d6c9460c74034608339ff17c6d.jpg)Chris Mauck\n- ![Jonas Mueller](https://cleanlab.ai/_next/static/images/jonas-038f0156ce880eb9cff38bb91618ea1b.jpg)Jonas Mueller\n\nThis article demonstrates an agentic system to ensure reliable answers in Retrieval-Augmented Generation, while also ensuring that\u00a0**latency and compute costs do not exceed the processing needed to accurately respond to complex queries**. Our system relies on\u00a0_trustworthiness scores_\u00a0for LLM outputs, in order to dynamically adjust retrieval strategies until sufficient context has been retrieved to generate a trustworthy RAG answer.\n\n![Diagram of Agentic RAG with trustworthiness scores](https://cleanlab.ai/_next/static/images/RAG_diagram-cb17a3bb114e546c10eb3e8ddc68a415.png)\n\nBased on the trustworthiness score for a candidate response, the RAG Agent can choose more complex retrieval plans or approve the response for production.\n\n## Introduction\n\nRetrieval-Augmented Generation (RAG) combines the strengths of large language models (LLMs) with powerful retrieval systems to generate more accurate responses grounded in knowledge databases. Simple RAG systems retrieve relevant information to a query via semantic search based on vector embeddings of query and database contents, but this strategy fails for more complex queries.\n\n_Agentic RAG_\u00a0considers various Retrieval strategies as tools available to an LLM orchestrator that can iteratively decide which tools to call next based on what it\u2019s seen thus far. This Agent can plan, execute, and refine multi-step retrieval processes, but it is critical to ensure latency and compute costs do not exceed what is required to produce a good answer for a user\u2019s query. Despite advancements from LLMs \u2192 RAG \u2192 Agentic RAG with sophisticated Retrieval strategies, AI-generated responses still suffer from hallucinations today, producing incorrect or nonsensical information with unwarranted confidence.\n\nThis blog outlines an Agentic RAG system that can produce trustworthy answers even for complex queries, in a manner that keeps latency/costs in check. Our system relies on the\u00a0[Trustworthy Language Model](https://cleanlab.ai/blog/trustworthy-language-model/)\u00a0to score the trustworthiness of a candidate response (based on the query and currently retrieved context). When the current response is deemed untrustworthy, the Agent is tasked with orchestrating a better Retrieval strategy to improve the context. This system starts with cheaper Retrieval strategies, and dynamically tries strategies with greater runtime/costs\u00a0**only for complex queries where they are necessary to produce a trustworthy RAG answer**.\n\n## Trustworthy Language Model (TLM)\n\nFor a given user query, the RAG system will retrieve relevant context, which is then fed into a LLM to produce the response.\u00a0_But how do we know when the response is untrustworthy_? For instance, here is question incorrectly answered by ChatGPT with no indication it should not be trusted.\n\n![ChatGPT giving an incorrect answer](https://cleanlab.ai/_next/static/images/gpt-367eda0e9cc624b7c3497ea417db378b.png)\n\nTLM automates this determination, by producing a trustworthiness score (between 0-1) for responses from any LLM. For the above prompt & ChatGPT response:\n\n```text\ntlm.get_trustworthiness_score(prompt, response) = 0.413\n```\n\nindicating this response should not be trusted.\n\nThese scores\u00a0[have been found](https://cleanlab.ai/blog/trustworthy-language-model/)\u00a0to detect hallucinations and LLM errors with greater precision/recall than alternative approaches like asking the LLM to evaluate its own output or relying on token-probabilities. TLM internally estimates aleatoric and epistemic uncertainty of the LLM by combining techniques including self-reflection, probabilistic prediction, and observed consistency. You can\u00a0[present](https://help.cleanlab.ai/tutorials/tlm_rag/)\u00a0TLM trustworthiness scores to users of your RAG system to automatically help them understand which responses warrant further scrutiny.\n\n## Utilizing the trustworthiness score in Agentic RAG\n\nA user\u2019s query is processed by our RAG system as follows: the Retrieval Planner Agent orchestrates a series of Retrieval strategies in order to discover relevant context, that when appended to the query, leads to an LLM response with sufficiently high trustworthiness score. The Agent is encouraged to start with faster/cheaper Retrieval strategies and only escalate to more complex Retrieval when a high trustworthiness score has not yet been achieved. As soon as a trustworthy LLM response is produced, it is returned to the user.\n\nThis high-level system can work with many types of Retrieval Planner Agent implementations (e.g. via frameworks like LangGraph and tool-use algorithms like OpenAI Function Calling), as well as all sorts of Retrieval strategies. The goal is to minimize the runtime and costs required to process most queries, while still being able to produce trustworthy responses for complex queries that necessitate more compute.\n\n### Potential Retrieval Strategies\n\nAs a concrete example, our Retrieval Planner Agent might choose from the following Retrieval strategies, increasing in time and compute complexity:\n\n1. No Retrieval\n\n- Complexity:\u00a0_None_\n- The query is answerable with general knowledge the LLM already knows.\n\n2. Semantic Search (vector embedding similarity)\n\n- Complexity:\u00a0_Low_\n- Vector database (Pinecone, Qdrant, Weaviate, etc.) is searched using top similarities in space of embeddings (Sentence Transformers, Voyage, etc.)\n\n3. [Hybrid Search (vector + keyword search) with Reciprocal Rank Fusion](https://www.assembled.com/blog/better-rag-results-with-reciprocal-rank-fusion-and-hybrid-search)\n\n- Complexity:\u00a0_Low/Medium_\n- Knowledge database is searched via a combination of vector similarity and classical keyword search like BM25, with results rankings from different searches aggregated via the RRF method.\n\n4. [Re-Ranking](https://adasci.org/a-hands-on-guide-to-enhance-rag-with-re-ranking/)\u00a0retrieved results\n\n- Complexity:\u00a0_Medium_\n- A specialized re-ranker model is applied to the retrieved results from either vector or keyword search that more accurately estimates which ones are relevant to the query.\n\n5. [Query Expansion](https://docs.llamaindex.ai/en/stable/examples/query_transformations/query_transform_cookbook/)\n\n- Complexity:\u00a0_Medium/High_\n- User query is rewritten into possibly multiple queries before (possibly multiple steps of) Retrieval. This includes entity recognition, separate keyword searches, and methods like\u00a0[Hypothetical Document Embeddings (Hyde)](https://arxiv.org/abs/2212.10496)\u00a0or\u00a0[Step-Back Prompting](https://arxiv.org/abs/2310.06117).\n\n6. Chunk/Document Expansion ([Multi-Hop RAG](https://cobusgreyling.medium.com/multihop-rag-1c695794eeda),\u00a0[GraphRAG](https://neo4j.com/blog/graphrag-manifesto/))\n\n- Complexity:\u00a0_Medium/High_\n- Returned chunks from the vector database search are expanded by referring to the original document from which they came and traversing related documents or Knowledge Graphs like Neo4j. This may be required to find additional information that is needed for the retrieved context to be useful.\n\n## Examples of our Trustworthy Agentic RAG in action\n\nTo make things more concrete, we consider a RAG application intended to answer questions based on Nvidia\u2019s product documentation.\n\n### Simple Query\n\nLet\u2019s first consider a simple query that a user may pose:\n\n> **Query:**\u00a0_Which component of a computer is responsible for graphics rendering?_\n\nOn the first pass, our RAG Agent chooses the least complex retrieval plan: do not retrieve anything. The response and associated trustworthiness score are:\n\n> **Response:**\u00a0The component of a computer responsible for graphics rendering is the Graphics Processing Unit (GPU).\n> \n> **Trustworthiness Score:**\u00a00.986\n\nIn this case, the RAG Agent determines the response to be trustworthy and approves the response to be used downstream.\n\n### Complex Query\n\nNow for a more complex query:\n\n> **Query:**\u00a0_How many NVIDIA CUDA Cores does the RTX 4090 have?_\n\nOur RAG Agent might again initially choose to not retrieve anything, yielding:\n\n> **Response:**\u00a0As of my last update in October 2022, NVIDIA has not released an RTX 4090 graphics card. Therefore, there are no official specifications.\n> \n> **Trustworthiness Score:**\u00a00.725\n\nIt appears that the LLM used does not have access to this knowledge. This is quantified by the trustworthiness score, enabling our system to determine this is not a reliable response and then to automatically employ a more complex retrieval plan: vector embedding search. On the second pass, the context, response, and trustworthiness score are:\n\n> **Retrieved Context:**\n> \n> GeForce RTX 4090 GPU Engine Specs: NVIDIA CUDA Cores 16384 Shader Cores Ada Lovelace 83 TFLOPS Ray Tracing Cores 3rd Generation 191 TFLOPS Tensor Cores (AI) 4th Generation 1321 AI TOPS Boost Clock (GHz) 2.52 Base Clock (GHz) 2.23\n> \n> **Response:**\u00a0The NVIDIA GeForce RTX 4090 features 16,384 CUDA cores.\n> \n> **Trustworthiness Score:**\u00a00.988\n\nNice! The RAG Agent was able to dynamically up the complexity of the retrieval plan and now has a satisfactory answer with a high trustworthiness score.\n\n### More Complex Query\n\nLet\u2019s consider a more complex query:\n\n> **Query:**\u00a0_What were the key design challenges faced by NVIDIA in developing the RTX 4090, and how were they overcome?_\n\nOur RAG Agent might again initially chooses to not retrieve anything, yielding:\n\n> **Response:**\u00a0The NVIDIA RTX 4090 was developed to push the boundaries of GPU performance, requiring significant advancements in architecture, thermal management, and power efficiency.\n> \n> **Trustworthiness Score:**\u00a00.635\n\nTLM identifies this response is generic and not confidently containing the right information. In the next pass, our RAG Agent might opt for vector similarity search again, yielding:\n\n> **Retrieved Context:**\u00a0NVIDIA\u2019s RTX 4090 was anticipated to be a major leap in GPU technology. There were discussions about improving overall architecture and enhancing ray tracing capabilities. However, specific details about the challenges during its development are scarce. Some sources suggest that NVIDIA focused on maximizing performance for gaming and AI applications, but there is little concrete information on the exact hurdles faced.\n> \n> **Response:**\u00a0The NVIDIA RTX 4090 focused on improving performance and ray tracing capabilities, but specific details about the challenges encountered during development are not well-documented. It is believed that NVIDIA prioritized enhancements in gaming and AI, but the exact difficulties they faced remain unclear.\n> \n> **Trustworthiness Score:**\u00a00.805\n\nIn this pass, our RAG Agent retrieved unsatisfactory information. The corresponding moderate trustworthiness score reflects the lack of specific context. Our RAG Agent thus considers another Retrieval pass with a more complex Hybrid search strategy, yielding:\n\n> **Retrieved Context:**\u00a0The RTX 4090\u2019s development saw NVIDIA addressing several engineering hurdles. The thermal challenges were exacerbated by the sheer density of the CUDA cores, necessitating an innovative cooling solution. NVIDIA introduced a new fan design and a larger vapor chamber to dissipate heat more effectively. On the power side, the 4090 required a new power connector standard, the PCIe Gen 5 16-pin, which allowed for higher wattage delivery, ensuring stable operation under load. For AI integration, NVIDIA refined its Tensor Core technology to balance power and performance, ensuring that the 4090 could handle advanced AI workloads without bottlenecking other processes.\n> \n> **Response:**\u00a0NVIDIA\u2019s RTX 4090 development involved overcoming several key challenges, particularly around thermal management and power delivery. The GPU\u2019s high density of 16,384 CUDA cores required an innovative cooling system, leading to the introduction of a new fan design and larger vapor chamber. Additionally, the need for higher power delivery led to the adoption of the PCIe Gen 5 16-pin connector. To integrate AI features effectively, NVIDIA refined its Tensor Core technology, balancing power and performance to handle advanced AI workloads efficiently.\n> \n> **Trustworthiness Score:**\u00a00.975\n\nWith the expanded context, the RAG Agent produces a highly detailed and specific response that earns a high enough trustworthiness score to return to the user. That said, this higher quality answer did require additional time/costs to produce.\n\nThe same process continues for queries with greater levels of complexity. Our RAG Agent continues escalating Retrieval strategies until the trustworthiness score indicates a reliable response, or the maximum time limit is reached. If no Retrieval strategy yields a trustworthy LLM response, then our system responds that it cannot handle this question without additional clarifications/information.\n\n## Wrap Up\n\nIntegrating the Trustworthy Language Model enables Agentic RAG systems that can ensure accurate answers to complex queries while bounding latency/costs for regular queries. You can adopt this approach to navigate the delicate balance between speed, cost, and accuracy across diverse RAG applications\u2014from customer service to specialized fields like finance, law, and medicine.\n\nWhile traditional RAG systems generate responses of unknown quality based on predefined steps to process every query,\u00a0**the future of AI lies in systems that assess response trustworthiness and adapt processing plans to each query\u2019s complexity**. Agentic RAG with the TLM offers a promising step toward this future of\u00a0_reliable_\u00a0AI.\n\n## Next Steps\n\n1. Get started with the\u00a0[TLM tutorials](https://help.cleanlab.ai/tutorials/tlm/).\n2. Try it instantly via the\u00a0[TLM Playground](https://tlm.cleanlab.ai/).\n3. Read\u00a0[benchmarks](https://cleanlab.ai/blog/trustworthy-language-model/)\u00a0measuring the effectiveness of LLM trustworthiness scores.\u3002", "top": 0, "createdAt": 1752321863, "style": "", "script": "", "head": "", "ogImage": "https://github.githubassets.com/favicons/favicon.svg", "createdDate": "2025-07-12", "dateLabelColor": "#0969da"}, "P10": {"htmlDir": "docs/post/yi-kuan-PDF-zhuan-JSON,Markdown-de-gong-ju-\uff1aDocling.html", "labels": ["AI", "Source Code"], "postTitle": "\u4e00\u6b3ePDF\u8f6cJSON,Markdown\u7684\u5de5\u5177\uff1aDocling", "postUrl": "post/yi-kuan-PDF-zhuan-JSON%2CMarkdown-de-gong-ju-%EF%BC%9ADocling.html", "postSourceUrl": "https://github.com/winnerineast/winnerineast.github.io/issues/10", "commentNum": 0, "wordCount": 115, "description": "\u652f\u6301\u8be6\u7ec6\u9875\u9762\u5e03\u5c40\u548c\u9605\u8bfb\u987a\u5e8f\u7406\u89e3\u3001\u8868\u683c\u7ed3\u6784\u6062\u590d\uff0c\u5143\u6570\u636e\u63d0\u53d6\uff0c\u652f\u6301OCR\u529f\u80fd\uff0c\u53ef\u7528\u4e8e\u626b\u63cf\u7684PDF \n\ngithub\uff1a[https://github.com/DS4SD/docling](https://t.co/AOYqyN8JN5)\u3002", "top": 0, "createdAt": 1752321905, "style": "", "script": "", "head": "", "ogImage": "https://github.githubassets.com/favicons/favicon.svg", "createdDate": "2025-07-12", "dateLabelColor": "#0969da"}, "P11": {"htmlDir": "docs/post/wu-da-zhu-liu-Agentic-kuang-jia-you-lie-dui-bi-fen-xi.html", "labels": ["AI", "Source Code"], "postTitle": "\u4e94\u5927\u4e3b\u6d41Agentic\u6846\u67b6\u4f18\u52a3\u5bf9\u6bd4\u5206\u6790", "postUrl": "post/wu-da-zhu-liu-Agentic-kuang-jia-you-lie-dui-bi-fen-xi.html", "postSourceUrl": "https://github.com/winnerineast/winnerineast.github.io/issues/11", "commentNum": 0, "wordCount": 1080, "description": "\n\u300c\u5fae\u8f6f\u3001OpenAI \u7b49\u79d1\u6280\u5de8\u5934\u63a8\u51fa\u7684\u4e94\u5927\u591a AI \u4ee3\u7406\u6846\u67b6\u5404\u5177\u7279\u8272\uff0cAutoGen \u9002\u5408\u5f00\u53d1\u3001CrewAI \u6613\u4e0a\u624b\u3001LangGraph \u6700\u7075\u6d3b\u3001Swarm \u6700\u7b80\u5355\u3001Magnetic-One \u8f83\u5168\u80fd\uff0c\u5f00\u53d1\u8005\u8be5\u600e\u4e48\u9009\u5462\uff1f\u300d 1. AutoGen\n\n[@pyautogen](https://x.com/pyautogen)\n\n- \u4f18\u52bf: - \u6700\u65e9\u4e14\u6700\u6d41\u884c\u7684\u6846\u67b6 - \u7279\u522b\u9002\u5408\u8f6f\u4ef6\u5f00\u53d1\u4efb\u52a1 - \u6709\u5fae\u8f6f\u5f3a\u5927\u7684\u793e\u533a\u652f\u6301 - \u57fa\u4e8e\u7528\u6237\u4ee3\u7406\u548c\u52a9\u624b\u4ee3\u7406\u7684\u4ea4\u4e92\u6a21\u5f0f - \u5c40\u9650: - \u5bf9\u975e\u7a0b\u5e8f\u5458\u4e0d\u591f\u53cb\u597d - \u8bbe\u7f6e\u590d\u6742,\u7279\u522b\u662f\u4f7f\u7528\u672c\u5730LLM\u65f6 - \u5728\u975e\u8f6f\u4ef6\u5f00\u53d1\u4efb\u52a1\u4e0a\u8868\u73b0\u4e00\u822c 2. CrewAI\n\n[@crewAIInc](https://x.com/crewAIInc)\n\n- \u4f18\u52bf: - \u975e\u5e38\u76f4\u89c2,\u4e3b\u8981\u4f9d\u8d56\u63d0\u793a\u8bcd\u7f16\u5199 - \u5bb9\u6613\u521b\u5efa\u548c\u6dfb\u52a0\u65b0\u4ee3\u7406 - \u5bf9\u975e\u6280\u672f\u7528\u6237\u53cb\u597d - \u4e0e\u5927\u591a\u6570LLM\u63d0\u4f9b\u5546\u517c\u5bb9 - \u5c40\u9650: - \u7075\u6d3b\u6027\u548c\u81ea\u5b9a\u4e49\u6027\u6709\u9650 - \u4e3b\u8981\u9002\u5408\u57fa\u7840\u7528\u4f8b - \u4ee3\u7406\u4e4b\u95f4\u7684\u4ea4\u4e92\u5b58\u5728\u4e00\u4e9bbug - \u793e\u533a\u652f\u6301\u6709\u9650 3. LangGraph\n\n[@LangChainAI](https://x.com/LangChainAI)\n\n- \u4f18\u52bf: - \u5efa\u7acb\u5728LangChain\u4e4b\u4e0a,\u57fa\u4e8e\u6709\u5411\u5faa\u73af\u56fe - \u975e\u5e38\u7075\u6d3b\u548c\u53ef\u5b9a\u5236 - \u6709\u826f\u597d\u7684\u793e\u533a\u652f\u6301 - \u53ef\u4e0e\u5f00\u6e90LLM\u548c\u5404\u79cdAPI\u914d\u5408\u4f7f\u7528 - \u5c40\u9650: - \u6587\u6863\u4e0d\u591f\u5b8c\u5584 - \u5bf9\u975e\u7a0b\u5e8f\u5458\u4e0d\u591f\u53cb\u597d - \u9700\u8981\u8f83\u5f3a\u7684\u7f16\u7a0b\u6280\u80fd 4. OpenAI Swarm\n\n[@OpenAIDevs](https://x.com/OpenAIDevs)\n\n- \u4f18\u52bf: - \u6700\u5bb9\u6613\u4e0a\u624b\u7684\u6846\u67b6 - \u7b80\u5316\u4e86\u4ee3\u7406\u521b\u5efa\u548c\u5207\u6362 - \u9002\u5408\u5feb\u901fdemo - \u5c40\u9650: - \u4ec5\u652f\u6301OpenAI API - \u4e0d\u9002\u5408\u751f\u4ea7\u90e8\u7f72 - \u7075\u6d3b\u6027\u4e0d\u8db3 - \u793e\u533a\u652f\u6301\u8584\u5f31 5. Magnetic-One (\n\n[@OpenAtMicrosoft](https://x.com/OpenAtMicrosoft)\n\n) - \u4f18\u52bf: - \u9002\u5408\u975e\u7a0b\u5e8f\u5458 - \u9884\u88c55\u4e2a\u4ee3\u7406(\u542b1\u4e2a\u7ba1\u7406\u4ee3\u7406) - \u5efa\u7acb\u5728AutoGen\u4e4b\u4e0a - \u5c40\u9650: - \u5f00\u6e90LLM\u652f\u6301\u590d\u6742 - \u7075\u6d3b\u6027\u4e0d\u8db3 - \u6587\u6863\u548c\u793e\u533a\u652f\u6301\u51e0\u4e4e\u6ca1\u6709 \u4f5c\u8005\u7684\u6700\u7ec8\u5efa\u8bae: - \u8f6f\u4ef6\u5f00\u53d1\u4efb\u52a1\u9009\u62e9 AutoGen - \u65b0\u624b\u5165\u95e8\u9009\u62e9 OpenAI Swarm \u6216 CrewAI - \u590d\u6742\u4efb\u52a1\u9009\u62e9 LangGraph - \u5f00\u6e90LLM\u4f7f\u7528\u63a8\u8350 LangGraph - \u793e\u533a\u652f\u6301\u6700\u597d\u7684\u662f AutoGen - \u5feb\u901f\u542f\u52a8\u9009\u62e9 CrewAI - \u6210\u672c\u6548\u76ca\u8003\u8651 Magnetic-One\u3002", "top": 0, "createdAt": 1752321931, "style": "", "script": "", "head": "", "ogImage": "https://github.githubassets.com/favicons/favicon.svg", "createdDate": "2025-07-12", "dateLabelColor": "#0969da"}, "P12": {"htmlDir": "docs/post/quan-mian-jie-ma-MLOps - -cong-gou-xiang-dao-luo-di-de-ji-qi-xue-xi-zhi-lv.html", "labels": ["AI", "Source Code"], "postTitle": "\u5168\u9762\u89e3\u7801MLOps - \u4ece\u6784\u60f3\u5230\u843d\u5730\u7684\u673a\u5668\u5b66\u4e60\u4e4b\u65c5", "postUrl": "post/quan-mian-jie-ma-MLOps%20-%20-cong-gou-xiang-dao-luo-di-de-ji-qi-xue-xi-zhi-lv.html", "postSourceUrl": "https://github.com/winnerineast/winnerineast.github.io/issues/12", "commentNum": 0, "wordCount": 724, "description": "<img width='1818' height='1892' alt='Image' src='https://github.com/user-attachments/assets/792fc6af-5295-4faa-8570-fb3f33be16d6' />\n\n\u8fd9\u5f20\u56fe\u5c55\u793a\u4e86\u4e00\u4e2a\u7aef\u5230\u7aef\u7684MLOps\uff08\u673a\u5668\u5b66\u4e60\u8fd0\u7ef4\uff09\u67b6\u6784\uff0c\u5206\u4e3a\u51e0\u4e2a\u5173\u952e\u6b65\u9aa4\u548c\u533a\u57df\u3002", "top": 0, "createdAt": 1752321994, "style": "", "script": "", "head": "", "ogImage": "https://github.githubassets.com/favicons/favicon.svg", "createdDate": "2025-07-12", "dateLabelColor": "#0969da"}, "P13": {"htmlDir": "docs/post/ru-he-da-zao-shu-yu-ni-de-ding-zhi-hua-da-mo-xing-liao-tian-ji-qi-ren.html", "labels": ["AI", "Source Code"], "postTitle": "\u5982\u4f55\u6253\u9020\u5c5e\u4e8e\u4f60\u7684\u5b9a\u5236\u5316\u5927\u6a21\u578b\u804a\u5929\u673a\u5668\u4eba", "postUrl": "post/ru-he-da-zao-shu-yu-ni-de-ding-zhi-hua-da-mo-xing-liao-tian-ji-qi-ren.html", "postSourceUrl": "https://github.com/winnerineast/winnerineast.github.io/issues/13", "commentNum": 0, "wordCount": 782, "description": "\n1. \u6587\u6863\u62c6\u89e3\uff1a - \u9996\u5148\uff0c\u4f60\u5f97\u6709\u4e00\u4e9b\u6587\u672c\u8d44\u6599\uff08Documents\uff09\u3002", "top": 0, "createdAt": 1752322036, "style": "", "script": "", "head": "", "ogImage": "https://github.githubassets.com/favicons/favicon.svg", "createdDate": "2025-07-12", "dateLabelColor": "#0969da"}, "P14": {"htmlDir": "docs/post/mo-xing-zhu-ce-biao-de-quan-jing-yun-zuo.html", "labels": ["AI", "Source Code"], "postTitle": "\u6a21\u578b\u6ce8\u518c\u8868\u7684\u5168\u666f\u8fd0\u4f5c", "postUrl": "post/mo-xing-zhu-ce-biao-de-quan-jing-yun-zuo.html", "postSourceUrl": "https://github.com/winnerineast/winnerineast.github.io/issues/14", "commentNum": 0, "wordCount": 457, "description": "\n\u63ed\u5f00MLOps\u7684\u6838\u5fc3\u5965\u79d8\uff1a\u6a21\u578b\u6ce8\u518c\u8868\u7684\u5168\u666f\u8fd0\u4f5c\uff01 \u8fd9\u5f20\u56fe\u5c55\u793a\u4e86\u5728MLOps\uff08\u673a\u5668\u5b66\u4e60\u8fd0\u7ef4\uff09\u4e2d\uff0c\u6a21\u578b\u6ce8\u518c\u8868\u5982\u4f55\u4f5c\u4e3a\u6838\u5fc3\u73af\u8282\uff0c\u534f\u8c03\u548c\u7ba1\u7406\u6574\u4e2a\u673a\u5668\u5b66\u4e60\u6d41\u6c34\u7ebf\u3002", "top": 0, "createdAt": 1752322065, "style": "", "script": "", "head": "", "ogImage": "https://github.githubassets.com/favicons/favicon.svg", "createdDate": "2025-07-12", "dateLabelColor": "#0969da"}}, "singeListJson": {}, "labelColorDict": {"AI": "#c21255", "bug": "#d73a4a", "documentation": "#0075ca", "duplicate": "#cfd3d7", "enhancement": "#a2eeef", "good first issue": "#7057ff", "help wanted": "#008672", "invalid": "#e4e669", "paper": "#8ef190", "question": "#d876e3", "Source Code": "#25b1d1", "wontfix": "#ffffff"}, "displayTitle": "Winnerineast Blog", "faviconUrl": "https://github.githubassets.com/favicons/favicon.svg", "ogImage": "https://github.githubassets.com/favicons/favicon.svg", "primerCSS": "<link href='https://mirrors.sustech.edu.cn/cdnjs/ajax/libs/Primer/21.0.7/primer.css' rel='stylesheet' />", "homeUrl": "https://winnerineast.github.io", "prevUrl": "disabled", "nextUrl": "disabled"}