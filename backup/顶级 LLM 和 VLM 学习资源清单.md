
1. makemore by [Andrej Karpathy](https://youtu.be/PaCmpygFfXo?si=dZCY8sictT658_Sk) 
2. minbpe by [karpathy](https://youtu.be/kCc8FmEb1nY?si=W61JnqMT4gjzF37q) 
3. attention? attention! [Lilian Weng](https://lilianweng.github.io/posts/2018-06-24-attention) ![[Attention_Attention.pdf]]
4. gpt-2 again by [karpathy](https://youtu.be/kCc8FmEb1nY?si=454hxUDiqD-t4YU0) 
5. llama3 from scratch by [naklecha](https://github.com/naklecha/llama3-from-scratch) 
6. llm training in simple, raw by c/cuda [karpathy](https://github.com/karpathy/llm.c) 
7. decoding strategies in large language models [mlabonne](https://mlabonne.github.io/blog/posts/2023-06-07-Decoding_strategies.html) 
8. how to make llms go fast by [vgel](https://vgel.me/posts/faster-inference/#KV_caching) ![[How to make LLMs go fast.pdf]]
9. a visual guide to quantization [maarten](https://www.maartengrootendorst.com/blog/quantization/)![[A visual guide to quantization.pdf]]
10. extending the RoPE by [eleutherai](https://blog.eleuther.ai/yarn) ![[Extending the RoPE.pdf]]
11. the novice's llm training guide by [alpin](https://rentry.org/llm-training) ![[The Novice LLM Training Guide.pdf]]
12. a survey on evaluation of large language models [paper](https://arxiv.org/abs/2307.03109) ![[2307.03109v9.pdf]]
13. mixture of experts explained [huggingface](https://huggingface.co/blog/moe) ![[Mixture of Experts Explained.pdf]]
14. vision transformer by [aman-arora](https://amaarora.github.io/posts/2021-01-18-ViT.html) ![[Vision Transformer.pdf]]
15. clip, siglip and paligemma by [umar-jamil](https://youtu.be/vAmKB7iPkWw?si=p4d2eoVNz0Nqwkxs)