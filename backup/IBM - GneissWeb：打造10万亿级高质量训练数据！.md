Original *2025年03月07日 08:23*

![Image](https://mmbiz.qpic.cn/mmbiz_png/KQq0TwTibbBCkxtlNyIIqPxczibeCnD19H1xH4Ivok6zfqGxqkUI5UmgPjFPxhbzzzwIS4TGjUH11wtCJ60sJYwA/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

当下的AI热潮中，大家都在谈论ChatGPT、Claude这样的大语言模型有多么强大，但很少有人关注它们背后的"粮食"——训练数据。其实，数据的质量和数量对模型性能影响巨大！一个不为人知的事实：目前主流大模型使用的训练数据集都不对外公开，而开源社区能用的大多不足5万亿token，严重制约了开源大模型的发展。IBM最新发布的GneissWeb数据集或将改变这一局面！

1、大模型的"粮食危机"：高质量大规模数据从哪来？

![Image](https://mmbiz.qpic.cn/mmbiz_png/KQq0TwTibbBCkxtlNyIIqPxczibeCnD19HJoge5icDZFk4lwLjV6ia9gVQJHBMUkKw47kMiaeQjK4Hpic7z5hCicMqgmA/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

目前大语言模型（LLM）的发展日新月异，从Llama到Claude，从ChatGPT到Gemini，这些模型背后都有一个共同特点：它们都使用了超大规模的训练数据。根据公开信息，Llama3系列模型使用了15万亿token的训练数据，比Llama2增加了8倍多；Gemma2系列使用了13万亿token；Granite-3.0系列使用了12万亿token。

然而，这些领先模型使用的训练数据集却不对外公开，甚至连创建过程也鲜有透露。开源社区虽然开发了一些替代数据集，但这些数据集大多不超过5万亿token，严重限制了它们训练大型模型的适用性。特别是对于现代LLM通常采用的两阶段预训练策略：第一阶段需要海量数据以覆盖广泛知识，第二阶段则使用较小但质量更高的数据集进一步提升模型能力。

在这个背景下，IBM研究团队推出了GneissWeb数据集，这是一个包含约10万亿token的大规模高质量数据集，专为满足训练大型语言模型的需求而设计。

2、GneissWeb如何炼成：远不止简单的数据清洗

GneissWeb的构建过程颇为精巧，远非简单的数据清洗那么简单。研究团队开发了一套完整的"GneissWeb配方"，包含以下核心组件：

（1）分片式精确子字符串去重：不同于传统的文档级去重，GneissWeb使用长度阈值为50的精确子字符串去重，能够有效去除文档内部和跨文档的重复内容，同时保留首次出现的重复子字符串。

（2）多模型质量过滤器集成：不同于仅依赖单一模型过滤的方法，GneissWeb使用了多种过滤器的集成方案：

1）定制化fastText分类器：一个基于指令格式数据和高质量Reddit帖子训练的分类器，以及另一个基于高质量合成数据和经LLM标注的高教育价值数据训练的分类器

2）类别感知可读性评分过滤器：创新性地使用McAlpine-EFLAW可读性评分，并根据文档类别（如科学、教育、技术和医疗健康）调整阈值，避免误过滤有价值的专业文档

3）类别感知极端分词文档过滤器：结合分词前（文档字符长度）和分词后（token数量）信息，识别异常文档

这种多层次的过滤机制实现了数据质量和数量之间的最佳平衡，在保留约10万亿token的同时显著提升了数据质量。研究团队通过严格的消融实验，测试了各种组件组合和顺序，最终确定了能够在保留至少10万亿token的前提下最大化下游任务性能的最佳配方。

3、显著的性能提升：GneissWeb训练的模型更强大

GneissWeb数据集的优势在实验中得到了充分验证。研究团队训练了多种规模的模型，并在广泛的基准测试上进行了评估：

1）在11个常用基准测试（包括零样本和少样本两种评估模式）上，使用GneissWeb训练的1.4B参数模型比使用FineWeb-V1.1.0训练的模型平均得分高出2.73个百分点。

2）当评估扩展到20个基准测试时，GneissWeb训练的模型仍然保持了1.75个百分点的优势。

3）更大规模的实验也证实了这一优势，GneissWeb在3B和7B规模的模型训练中同样表现出色。

这些结果清晰地表明，GneissWeb数据集在训练大型语言模型方面具有显著优势，特别是对于第一阶段的长词元视域预训练。与其他开源大型数据集相比，GneissWeb在同等规模下提供了更高质量的训练数据。

4、开源社区的福音：数据准备工具包即将释出

IBM研究团队不仅分享了GneissWeb数据集，还计划开源用于构建这一数据集的IBM数据准备工具包，这无疑是对开源社区的重大贡献。

GneissWeb配方的一大特色是其多元化的质量标注器集合，用户可以根据具体用例在标注器级别调整阈值来过滤文档。这与其他高质量数据集（如FineWeb、DCLM等）仅依赖单一模型质量标注器并执行激进过滤（去除约90%数据）的方法形成鲜明对比。

虽然GneissWeb配方主要针对获取约10万亿高质量token以适用于第一阶段预训练，但通过调整过滤参数，也可以生成更小、质量更高的数据集，适用于第二阶段预训练。

随着大语言模型的发展，数据质量和数量的平衡变得越来越重要。GneissWeb的出现为开源社区提供了一个强有力的工具，有望推动开源大语言模型赶上或超越闭源模型的性能。期待看到更多基于GneissWeb训练的强大模型问世！

你对大模型训练数据有什么看法？欢迎在评论区分享你的想法！

论文标题：GneissWeb: Preparing High Quality Data for LLMs at Scale 

论文链接：https://arxiv.org/abs/2502.14907

预览时标签不可点

AI动态200

LLM548

暂无留言

已无更多数据

  写留言:

继续滑动看下一个

AI帝国

向上滑动看下一个

[Got It](https://mp.weixin.qq.com/s/)

 Scan with Weixin to  
use this Mini Program