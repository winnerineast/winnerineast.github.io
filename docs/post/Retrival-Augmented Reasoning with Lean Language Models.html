<!DOCTYPE html>
<html data-color-mode="light" data-dark-theme="dark" data-light-theme="light" lang="zh-CN">
<head>
    <meta content="text/html; charset=utf-8" http-equiv="content-type" />
    <meta name="viewport" content="width=device-width,initial-scale=1.0">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <link href='https://mirrors.sustech.edu.cn/cdnjs/ajax/libs/Primer/21.0.7/primer.css' rel='stylesheet' />
    
    <link rel="icon" href="https://github.githubassets.com/favicons/favicon.svg"><script>
        let theme = localStorage.getItem("meek_theme") || "light";
        document.documentElement.setAttribute("data-color-mode", theme);
    </script>
<meta name="description" content="Paper ‚Äì [https://arxiv.org/abs/2508.11386](https://t.co/E1F7XY1g8R)

Paper Title: 'Retrieval-augmented reasoning with lean language models'

A small Qwen2.5 model is fine-tuned to think over retrieved documents, so a single lean setup can answer domain questions on resource-constrained local hardware.

Using summarised NHS pages, retrieval hits the right condition among top‚Äë5 in 76% of queries, and the fine‚Äëtuned model predicts the exact condition correctly 56% of the time, close to larger frontier models. 

The whole pipeline is built for private deployments, so teams can run it without sending data to external APIs.

üîí The problem they tackle

Many teams cannot ship prompts or data outside their network, especially in health and government, so cloud LLM endpoints are off the table. 

They aim for a single lean model that can read retrieved evidence and reason over it, all running locally, so answers stay grounded and private. 

The target setting is messy queries over a closed corpus, where retrieval constrains facts and the reasoning step interprets symptoms and next actions.

üß© The pipeline in this paper.

The system indexes a corpus, retrieves the most relevant pieces for each query, then generates an answer that reasons over those pieces. 

They use a classic retriever plus generator design, with retrieval first then reasoning, which fits decision tasks better than free‚Äëform answering. 

The chat flow lets a conversational agent decide when to call retrieval, then passes the retrieved context to the reasoning model to produce the answer.

<img width='789' height='826' alt='Image' src='https://github.com/user-attachments/assets/d0c0b06a-d1bf-429e-9ca2-82b6e0b58be1' />

The retriever at work

Documents are split into overlapping chunks and embedded with a sentence transformer, then stored in a vector database for fast similarity search. 

They use sentence-transformers all‚Äëmpnet‚Äëbase‚Äëv2, which maps text into a 768‚Äëdimensional space with a max sequence of 384 tokens, and a Chroma store with L2 similarity. 

If any chunk from a document makes the top‚Äëk, the pipeline feeds the full original document to the LLM, so the model sees full context around the hit.

<img width='672' height='288' alt='Image' src='https://github.com/user-attachments/assets/f8f16b8d-0535-4757-b932-edea32e736af' />

Below image shows the whole training loop for their lean, retrieval-augmented reasoning setup. 

It starts with a private knowledge base of about 1,000 NHS condition pages. GPT-4o generates about 2,000 synthetic patient queries from those pages, so they have realistic questions tied to known answers. 

For each query, a retriever pulls the top 5 likely documents. DeepSeek-R1 reads those documents and the query, then produces a final label plus a step-by-step reasoning trace. That bundle becomes one training example. 

They then fine-tune Qwen-32B-Instruct on this data and distill it into a smaller t0-1 reasoning model. The result is a compact model that learns to reason over retrieved evidence from the approved corpus, so it can run locally and stay grounded.

<img width='720' height='426' alt='Image' src='https://github.com/user-attachments/assets/ae38a29c-09af-4eb5-9029-3e97e2d0f868' />

 How the data was built

They generate realistic patient queries plus demographics from the NHS Conditions pages using GPT‚Äë4o, covering basic, hypochondriac, and downplay styles. 

They create 1,000 queries for evaluation and 2,000 for training, checking there is no overlap in condition plus disposition pairs between the 2 sets. 

For each training query they retrieve k=5 candidate condition pages, then ask DeepSeek‚ÄëR1 to produce a reasoning trace and a final label, which becomes supervision.
<img width='679' height='376' alt='Image' src='https://github.com/user-attachments/assets/2bbe1de5-e30a-4202-b0aa-9c937b58d2a9' />

Keeping context short

Full documents made traces extremely long, averaging 74,641 tokens when retrieving 5 documents, which would blow up fine‚Äëtune compute. 

They summarise every document up front by 85%, then build traces from summaries, dropping the average trace length to 7,544 tokens, while keeping retrieval quality intact. 

Query‚Äëaware summarisation was considered, but it would add an extra LLM call per query, so they stick to static summaries for speed.
<img width='705' height='427' alt='Image' src='https://github.com/user-attachments/assets/b3327f4d-e1dd-46f0-a18f-336d046047dc' />

Fine‚Äëtuning and compute

They fine‚Äëtune Qwen2.5‚ÄëInstruct models from 1.5B to 32B parameters on next‚Äëtoken prediction of the traces, using 5 epochs, cosine LR at 1e‚Äë5, bf16, and FSDP sharding. 

The long‚Äëcontext setup uses block size 32,768, gradient checkpointing, and Adam with weight decay 1e‚Äë4, beta1 0.9, beta2 0.95. 

The main 32B run uses 16 A100 80GB GPUs, and the whole study spans Azure plus 2 UK HPC clusters, totalling roughly 3,700 GPU‚Äëhours.

<img width='653' height='621' alt='Image' src='https://github.com/user-attachments/assets/fe4dd20e-3a64-431e-ae23-199e3fd7109e' />

What retrieval recovered

On the 989 summarised pages, retrieval finds the correct condition in the top‚Äë5 for 76% of queries, and top‚Äë30 for 93%, which sets the ceiling for end‚Äëto‚Äëend accuracy when k=5. 

Summaries actually retrieve better than full chunks at the same k, because they remove boilerplate and keep the decision cues. 

They choose k=5 as a practical trade‚Äëoff, keeping prompts small enough for stable training and easier inspection in the UI.

„ÄÇ">
<meta property="og:title" content="Retrival-Augmented Reasoning with Lean Language Models">
<meta property="og:description" content="Paper ‚Äì [https://arxiv.org/abs/2508.11386](https://t.co/E1F7XY1g8R)

Paper Title: 'Retrieval-augmented reasoning with lean language models'

A small Qwen2.5 model is fine-tuned to think over retrieved documents, so a single lean setup can answer domain questions on resource-constrained local hardware.

Using summarised NHS pages, retrieval hits the right condition among top‚Äë5 in 76% of queries, and the fine‚Äëtuned model predicts the exact condition correctly 56% of the time, close to larger frontier models. 

The whole pipeline is built for private deployments, so teams can run it without sending data to external APIs.

üîí The problem they tackle

Many teams cannot ship prompts or data outside their network, especially in health and government, so cloud LLM endpoints are off the table. 

They aim for a single lean model that can read retrieved evidence and reason over it, all running locally, so answers stay grounded and private. 

The target setting is messy queries over a closed corpus, where retrieval constrains facts and the reasoning step interprets symptoms and next actions.

üß© The pipeline in this paper.

The system indexes a corpus, retrieves the most relevant pieces for each query, then generates an answer that reasons over those pieces. 

They use a classic retriever plus generator design, with retrieval first then reasoning, which fits decision tasks better than free‚Äëform answering. 

The chat flow lets a conversational agent decide when to call retrieval, then passes the retrieved context to the reasoning model to produce the answer.

<img width='789' height='826' alt='Image' src='https://github.com/user-attachments/assets/d0c0b06a-d1bf-429e-9ca2-82b6e0b58be1' />

The retriever at work

Documents are split into overlapping chunks and embedded with a sentence transformer, then stored in a vector database for fast similarity search. 

They use sentence-transformers all‚Äëmpnet‚Äëbase‚Äëv2, which maps text into a 768‚Äëdimensional space with a max sequence of 384 tokens, and a Chroma store with L2 similarity. 

If any chunk from a document makes the top‚Äëk, the pipeline feeds the full original document to the LLM, so the model sees full context around the hit.

<img width='672' height='288' alt='Image' src='https://github.com/user-attachments/assets/f8f16b8d-0535-4757-b932-edea32e736af' />

Below image shows the whole training loop for their lean, retrieval-augmented reasoning setup. 

It starts with a private knowledge base of about 1,000 NHS condition pages. GPT-4o generates about 2,000 synthetic patient queries from those pages, so they have realistic questions tied to known answers. 

For each query, a retriever pulls the top 5 likely documents. DeepSeek-R1 reads those documents and the query, then produces a final label plus a step-by-step reasoning trace. That bundle becomes one training example. 

They then fine-tune Qwen-32B-Instruct on this data and distill it into a smaller t0-1 reasoning model. The result is a compact model that learns to reason over retrieved evidence from the approved corpus, so it can run locally and stay grounded.

<img width='720' height='426' alt='Image' src='https://github.com/user-attachments/assets/ae38a29c-09af-4eb5-9029-3e97e2d0f868' />

 How the data was built

They generate realistic patient queries plus demographics from the NHS Conditions pages using GPT‚Äë4o, covering basic, hypochondriac, and downplay styles. 

They create 1,000 queries for evaluation and 2,000 for training, checking there is no overlap in condition plus disposition pairs between the 2 sets. 

For each training query they retrieve k=5 candidate condition pages, then ask DeepSeek‚ÄëR1 to produce a reasoning trace and a final label, which becomes supervision.
<img width='679' height='376' alt='Image' src='https://github.com/user-attachments/assets/2bbe1de5-e30a-4202-b0aa-9c937b58d2a9' />

Keeping context short

Full documents made traces extremely long, averaging 74,641 tokens when retrieving 5 documents, which would blow up fine‚Äëtune compute. 

They summarise every document up front by 85%, then build traces from summaries, dropping the average trace length to 7,544 tokens, while keeping retrieval quality intact. 

Query‚Äëaware summarisation was considered, but it would add an extra LLM call per query, so they stick to static summaries for speed.
<img width='705' height='427' alt='Image' src='https://github.com/user-attachments/assets/b3327f4d-e1dd-46f0-a18f-336d046047dc' />

Fine‚Äëtuning and compute

They fine‚Äëtune Qwen2.5‚ÄëInstruct models from 1.5B to 32B parameters on next‚Äëtoken prediction of the traces, using 5 epochs, cosine LR at 1e‚Äë5, bf16, and FSDP sharding. 

The long‚Äëcontext setup uses block size 32,768, gradient checkpointing, and Adam with weight decay 1e‚Äë4, beta1 0.9, beta2 0.95. 

The main 32B run uses 16 A100 80GB GPUs, and the whole study spans Azure plus 2 UK HPC clusters, totalling roughly 3,700 GPU‚Äëhours.

<img width='653' height='621' alt='Image' src='https://github.com/user-attachments/assets/fe4dd20e-3a64-431e-ae23-199e3fd7109e' />

What retrieval recovered

On the 989 summarised pages, retrieval finds the correct condition in the top‚Äë5 for 76% of queries, and top‚Äë30 for 93%, which sets the ceiling for end‚Äëto‚Äëend accuracy when k=5. 

Summaries actually retrieve better than full chunks at the same k, because they remove boilerplate and keep the decision cues. 

They choose k=5 as a practical trade‚Äëoff, keeping prompts small enough for stable training and easier inspection in the UI.

„ÄÇ">
<meta property="og:type" content="article">
<meta property="og:url" content="https://winnerineast.github.io/post/Retrival-Augmented%20Reasoning%20with%20Lean%20Language%20Models.html">
<meta property="og:image" content="https://github.githubassets.com/favicons/favicon.svg">
<title>Retrival-Augmented Reasoning with Lean Language Models</title>



</head>
<style>
body{box-sizing: border-box;min-width: 200px;max-width: 900px;margin: 20px auto;padding: 45px;font-size: 16px;font-family: sans-serif;line-height: 1.25;}
#header{display:flex;padding-bottom:8px;border-bottom: 1px solid var(--borderColor-muted, var(--color-border-muted));margin-bottom: 16px;}
#footer {margin-top:64px; text-align: center;font-size: small;}

</style>

<style>
.postTitle{margin: auto 0;font-size:40px;font-weight:bold;}
.title-right{display:flex;margin:auto 0 0 auto;}
.title-right .circle{padding: 14px 16px;margin-right:8px;}
#postBody{border-bottom: 1px solid var(--color-border-default);padding-bottom:36px;}
#postBody hr{height:2px;}
#cmButton{height:48px;margin-top:48px;}
#comments{margin-top:64px;}
.g-emoji{font-size:24px;}
@media (max-width: 600px) {
    body {padding: 8px;}
    .postTitle{font-size:24px;}
}

</style>




<body>
    <div id="header">
<h1 class="postTitle">Retrival-Augmented Reasoning with Lean Language Models</h1>
<div class="title-right">
    <a href="https://winnerineast.github.io" id="buttonHome" class="btn btn-invisible circle" title="È¶ñÈ°µ">
        <svg class="octicon" width="16" height="16">
            <path id="pathHome" fill-rule="evenodd"></path>
        </svg>
    </a>
    
    <a href="https://github.com/winnerineast/winnerineast.github.io/issues/73" target="_blank" class="btn btn-invisible circle" title="Issue">
        <svg class="octicon" width="16" height="16">
            <path id="pathIssue" fill-rule="evenodd"></path>
        </svg>
    </a>
    

    <a class="btn btn-invisible circle" onclick="modeSwitch();" title="ÂàáÊç¢‰∏ªÈ¢ò">
        <svg class="octicon" width="16" height="16" >
            <path id="themeSwitch" fill-rule="evenodd"></path>
        </svg>
    </a>

</div>
</div>
    <div id="content">
<div class="markdown-body" id="postBody"><p>Paper ‚Äì <a href="https://t.co/E1F7XY1g8R" rel="nofollow">https://arxiv.org/abs/2508.11386</a></p>
<p>Paper Title: "Retrieval-augmented reasoning with lean language models"</p>
<p>A small Qwen2.5 model is fine-tuned to think over retrieved documents, so a single lean setup can answer domain questions on resource-constrained local hardware.</p>
<p>Using summarised NHS pages, retrieval hits the right condition among top‚Äë5 in 76% of queries, and the fine‚Äëtuned model predicts the exact condition correctly 56% of the time, close to larger frontier models.</p>
<p>The whole pipeline is built for private deployments, so teams can run it without sending data to external APIs.</p>
<p>üîí The problem they tackle</p>
<p>Many teams cannot ship prompts or data outside their network, especially in health and government, so cloud LLM endpoints are off the table.</p>
<p>They aim for a single lean model that can read retrieved evidence and reason over it, all running locally, so answers stay grounded and private.</p>
<p>The target setting is messy queries over a closed corpus, where retrieval constrains facts and the reasoning step interprets symptoms and next actions.</p>
<p>üß© The pipeline in this paper.</p>
<p>The system indexes a corpus, retrieves the most relevant pieces for each query, then generates an answer that reasons over those pieces.</p>
<p>They use a classic retriever plus generator design, with retrieval first then reasoning, which fits decision tasks better than free‚Äëform answering.</p>
<p>The chat flow lets a conversational agent decide when to call retrieval, then passes the retrieved context to the reasoning model to produce the answer.</p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/user-attachments/assets/d0c0b06a-d1bf-429e-9ca2-82b6e0b58be1"><img width="789" height="826" alt="Image" src="https://github.com/user-attachments/assets/d0c0b06a-d1bf-429e-9ca2-82b6e0b58be1" style="max-width: 100%; height: auto; max-height: 826px;"></a></p>
<p>The retriever at work</p>
<p>Documents are split into overlapping chunks and embedded with a sentence transformer, then stored in a vector database for fast similarity search.</p>
<p>They use sentence-transformers all‚Äëmpnet‚Äëbase‚Äëv2, which maps text into a 768‚Äëdimensional space with a max sequence of 384 tokens, and a Chroma store with L2 similarity.</p>
<p>If any chunk from a document makes the top‚Äëk, the pipeline feeds the full original document to the LLM, so the model sees full context around the hit.</p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/user-attachments/assets/f8f16b8d-0535-4757-b932-edea32e736af"><img width="672" height="288" alt="Image" src="https://github.com/user-attachments/assets/f8f16b8d-0535-4757-b932-edea32e736af" style="max-width: 100%; height: auto; max-height: 288px;"></a></p>
<p>Below image shows the whole training loop for their lean, retrieval-augmented reasoning setup.</p>
<p>It starts with a private knowledge base of about 1,000 NHS condition pages. GPT-4o generates about 2,000 synthetic patient queries from those pages, so they have realistic questions tied to known answers.</p>
<p>For each query, a retriever pulls the top 5 likely documents. DeepSeek-R1 reads those documents and the query, then produces a final label plus a step-by-step reasoning trace. That bundle becomes one training example.</p>
<p>They then fine-tune Qwen-32B-Instruct on this data and distill it into a smaller t0-1 reasoning model. The result is a compact model that learns to reason over retrieved evidence from the approved corpus, so it can run locally and stay grounded.</p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/user-attachments/assets/ae38a29c-09af-4eb5-9029-3e97e2d0f868"><img width="720" height="426" alt="Image" src="https://github.com/user-attachments/assets/ae38a29c-09af-4eb5-9029-3e97e2d0f868" style="max-width: 100%; height: auto; max-height: 426px;"></a></p>
<p>How the data was built</p>
<p>They generate realistic patient queries plus demographics from the NHS Conditions pages using GPT‚Äë4o, covering basic, hypochondriac, and downplay styles.</p>
<p>They create 1,000 queries for evaluation and 2,000 for training, checking there is no overlap in condition plus disposition pairs between the 2 sets.</p>
<p>For each training query they retrieve k=5 candidate condition pages, then ask DeepSeek‚ÄëR1 to produce a reasoning trace and a final label, which becomes supervision.<br>
<a target="_blank" rel="noopener noreferrer" href="https://github.com/user-attachments/assets/2bbe1de5-e30a-4202-b0aa-9c937b58d2a9"><img width="679" height="376" alt="Image" src="https://github.com/user-attachments/assets/2bbe1de5-e30a-4202-b0aa-9c937b58d2a9" style="max-width: 100%; height: auto; max-height: 376px;"></a></p>
<p>Keeping context short</p>
<p>Full documents made traces extremely long, averaging 74,641 tokens when retrieving 5 documents, which would blow up fine‚Äëtune compute.</p>
<p>They summarise every document up front by 85%, then build traces from summaries, dropping the average trace length to 7,544 tokens, while keeping retrieval quality intact.</p>
<p>Query‚Äëaware summarisation was considered, but it would add an extra LLM call per query, so they stick to static summaries for speed.<br>
<a target="_blank" rel="noopener noreferrer" href="https://github.com/user-attachments/assets/b3327f4d-e1dd-46f0-a18f-336d046047dc"><img width="705" height="427" alt="Image" src="https://github.com/user-attachments/assets/b3327f4d-e1dd-46f0-a18f-336d046047dc" style="max-width: 100%; height: auto; max-height: 427px;"></a></p>
<p>Fine‚Äëtuning and compute</p>
<p>They fine‚Äëtune Qwen2.5‚ÄëInstruct models from 1.5B to 32B parameters on next‚Äëtoken prediction of the traces, using 5 epochs, cosine LR at 1e‚Äë5, bf16, and FSDP sharding.</p>
<p>The long‚Äëcontext setup uses block size 32,768, gradient checkpointing, and Adam with weight decay 1e‚Äë4, beta1 0.9, beta2 0.95.</p>
<p>The main 32B run uses 16 A100 80GB GPUs, and the whole study spans Azure plus 2 UK HPC clusters, totalling roughly 3,700 GPU‚Äëhours.</p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/user-attachments/assets/fe4dd20e-3a64-431e-ae23-199e3fd7109e"><img width="653" height="621" alt="Image" src="https://github.com/user-attachments/assets/fe4dd20e-3a64-431e-ae23-199e3fd7109e" style="max-width: 100%; height: auto; max-height: 621px;"></a></p>
<p>What retrieval recovered</p>
<p>On the 989 summarised pages, retrieval finds the correct condition in the top‚Äë5 for 76% of queries, and top‚Äë30 for 93%, which sets the ceiling for end‚Äëto‚Äëend accuracy when k=5.</p>
<p>Summaries actually retrieve better than full chunks at the same k, because they remove boilerplate and keep the decision cues.</p>
<p>They choose k=5 as a practical trade‚Äëoff, keeping prompts small enough for stable training and easier inspection in the UI.</p></div>
<div style="font-size:small;margin-top:8px;float:right;"></div>

<button class="btn btn-block" type="button" onclick="openComments()" id="cmButton">ËØÑËÆ∫</button>
<div class="comments" id="comments"></div>

</div>
    <div id="footer"><div id="footer1">Copyright ¬© <span id="copyrightYear"></span> <a href="https://winnerineast.github.io">Winnerineast Blog</a></div>
<div id="footer2">
    <span id="runday"></span><span>Powered by <a href="https://meekdai.com/Gmeek.html" target="_blank">Gmeek</a></span>
</div>

<script>
var now=new Date();
document.getElementById("copyrightYear").innerHTML=now.getFullYear();

if(""!=""){
    var startSite=new Date("");
    var diff=now.getTime()-startSite.getTime();
    var diffDay=Math.floor(diff/(1000*60*60*24));
    document.getElementById("runday").innerHTML="ÁΩëÁ´ôËøêË°å"+diffDay+"Â§©"+" ‚Ä¢ ";
}
</script></div>
</body>
<script>
var IconList={'sun': 'M8 10.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5zM8 12a4 4 0 100-8 4 4 0 000 8zM8 0a.75.75 0 01.75.75v1.5a.75.75 0 01-1.5 0V.75A.75.75 0 018 0zm0 13a.75.75 0 01.75.75v1.5a.75.75 0 01-1.5 0v-1.5A.75.75 0 018 13zM2.343 2.343a.75.75 0 011.061 0l1.06 1.061a.75.75 0 01-1.06 1.06l-1.06-1.06a.75.75 0 010-1.06zm9.193 9.193a.75.75 0 011.06 0l1.061 1.06a.75.75 0 01-1.06 1.061l-1.061-1.06a.75.75 0 010-1.061zM16 8a.75.75 0 01-.75.75h-1.5a.75.75 0 010-1.5h1.5A.75.75 0 0116 8zM3 8a.75.75 0 01-.75.75H.75a.75.75 0 010-1.5h1.5A.75.75 0 013 8zm10.657-5.657a.75.75 0 010 1.061l-1.061 1.06a.75.75 0 11-1.06-1.06l1.06-1.06a.75.75 0 011.06 0zm-9.193 9.193a.75.75 0 010 1.06l-1.06 1.061a.75.75 0 11-1.061-1.06l1.06-1.061a.75.75 0 011.061 0z', 'moon': 'M9.598 1.591a.75.75 0 01.785-.175 7 7 0 11-8.967 8.967.75.75 0 01.961-.96 5.5 5.5 0 007.046-7.046.75.75 0 01.175-.786zm1.616 1.945a7 7 0 01-7.678 7.678 5.5 5.5 0 107.678-7.678z', 'sync': 'M1.705 8.005a.75.75 0 0 1 .834.656 5.5 5.5 0 0 0 9.592 2.97l-1.204-1.204a.25.25 0 0 1 .177-.427h3.646a.25.25 0 0 1 .25.25v3.646a.25.25 0 0 1-.427.177l-1.38-1.38A7.002 7.002 0 0 1 1.05 8.84a.75.75 0 0 1 .656-.834ZM8 2.5a5.487 5.487 0 0 0-4.131 1.869l1.204 1.204A.25.25 0 0 1 4.896 6H1.25A.25.25 0 0 1 1 5.75V2.104a.25.25 0 0 1 .427-.177l1.38 1.38A7.002 7.002 0 0 1 14.95 7.16a.75.75 0 0 1-1.49.178A5.5 5.5 0 0 0 8 2.5Z', 'home': 'M6.906.664a1.749 1.749 0 0 1 2.187 0l5.25 4.2c.415.332.657.835.657 1.367v7.019A1.75 1.75 0 0 1 13.25 15h-3.5a.75.75 0 0 1-.75-.75V9H7v5.25a.75.75 0 0 1-.75.75h-3.5A1.75 1.75 0 0 1 1 13.25V6.23c0-.531.242-1.034.657-1.366l5.25-4.2Zm1.25 1.171a.25.25 0 0 0-.312 0l-5.25 4.2a.25.25 0 0 0-.094.196v7.019c0 .138.112.25.25.25H5.5V8.25a.75.75 0 0 1 .75-.75h3.5a.75.75 0 0 1 .75.75v5.25h2.75a.25.25 0 0 0 .25-.25V6.23a.25.25 0 0 0-.094-.195Z', 'github': 'M8 0c4.42 0 8 3.58 8 8a8.013 8.013 0 0 1-5.45 7.59c-.4.08-.55-.17-.55-.38 0-.27.01-1.13.01-2.2 0-.75-.25-1.23-.54-1.48 1.78-.2 3.65-.88 3.65-3.95 0-.88-.31-1.59-.82-2.15.08-.2.36-1.02-.08-2.12 0 0-.67-.22-2.2.82-.64-.18-1.32-.27-2-.27-.68 0-1.36.09-2 .27-1.53-1.03-2.2-.82-2.2-.82-.44 1.1-.16 1.92-.08 2.12-.51.56-.82 1.28-.82 2.15 0 3.06 1.86 3.75 3.64 3.95-.23.2-.44.55-.51 1.07-.46.21-1.61.55-2.33-.66-.15-.24-.6-.83-1.23-.82-.67.01-.27.38.01.53.34.19.73.9.82 1.13.16.45.68 1.31 2.69.94 0 .67.01 1.3.01 1.49 0 .21-.15.45-.55.38A7.995 7.995 0 0 1 0 8c0-4.42 3.58-8 8-8Z'};
var utterancesLoad=0;

let themeSettings={
    "dark": ["dark","moon","#00f0ff","dark-blue"],
    "light": ["light","sun","#ff5000","github-light"],
    "auto": ["auto","sync","","preferred-color-scheme"]
};
function changeTheme(mode, icon, color, utheme){
    document.documentElement.setAttribute("data-color-mode",mode);
    document.getElementById("themeSwitch").setAttribute("d",value=IconList[icon]);
    document.getElementById("themeSwitch").parentNode.style.color=color;
    if(utterancesLoad==1){utterancesTheme(utheme);}
}
function modeSwitch(){
    let currentMode=document.documentElement.getAttribute('data-color-mode');
    let newMode = currentMode === "light" ? "dark" : currentMode === "dark" ? "auto" : "light";
    localStorage.setItem("meek_theme", newMode);
    if(themeSettings[newMode]){
        changeTheme(...themeSettings[newMode]);
    }
}
function utterancesTheme(theme){
    const message={type:'set-theme',theme: theme};
    const iframe=document.getElementsByClassName('utterances-frame')[0];
    iframe.contentWindow.postMessage(message,'https://utteranc.es');
}
if(themeSettings[theme]){changeTheme(...themeSettings[theme]);}
console.log("\n %c Gmeek last https://github.com/Meekdai/Gmeek \n","padding:5px 0;background:#02d81d;color:#fff");
</script>

<script>
document.getElementById("pathHome").setAttribute("d",IconList["home"]);
document.getElementById("pathIssue").setAttribute("d",IconList["github"]);



function openComments(){
    cm=document.getElementById("comments");
    cmButton=document.getElementById("cmButton");
    cmButton.innerHTML="loading";
    span=document.createElement("span");
    span.setAttribute("class","AnimatedEllipsis");
    cmButton.appendChild(span);

    script=document.createElement("script");
    script.setAttribute("src","https://utteranc.es/client.js");
    script.setAttribute("repo","winnerineast/winnerineast.github.io");
    script.setAttribute("issue-term","title");
    
    if(localStorage.getItem("meek_theme")=="dark"){script.setAttribute("theme","dark-blue");}
    else if(localStorage.getItem("meek_theme")=="light") {script.setAttribute("theme","github-light");}
    else{script.setAttribute("theme","preferred-color-scheme");}
    
    script.setAttribute("crossorigin","anonymous");
    script.setAttribute("async","");
    cm.appendChild(script);

    int=self.setInterval("iFrameLoading()",200);
}

function iFrameLoading(){
    var utterances=document.getElementsByClassName('utterances');
    if(utterances.length==1){
        if(utterances[0].style.height!=""){
            utterancesLoad=1;
            int=window.clearInterval(int);
            document.getElementById("cmButton").style.display="none";
            console.log("utterances Load OK");
        }
    }
}



</script>


</html>
