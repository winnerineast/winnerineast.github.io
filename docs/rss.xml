<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Winnerineast Blog</title><link>https://winnerineast.github.io</link><description>Humachine Studio</description><copyright>Winnerineast Blog</copyright><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><image><url>https://github.githubassets.com/favicons/favicon.svg</url><title>avatar</title><link>https://winnerineast.github.io</link></image><lastBuildDate>Sat, 12 Jul 2025 12:16:14 +0000</lastBuildDate><managingEditor>Winnerineast Blog</managingEditor><ttl>60</ttl><webMaster>Winnerineast Blog</webMaster><item><title>MiniMindï¼šå·²å¼€æºï¼åªè¦3å°æ—¶å°±èƒ½è®­ç»ƒä¸€ä¸ª26Mçš„å¤§æ¨¡å‹ï¼Œæ•™ç¨‹éå¸¸æ¸…æ™°æˆ‘æ„Ÿè§‰æˆ‘åˆè¡Œäº†ï¼</title><link>https://winnerineast.github.io/post/MiniMind%EF%BC%9A-yi-kai-yuan-%EF%BC%81-zhi-yao-3-xiao-shi-jiu-neng-xun-lian-yi-ge-26M-de-da-mo-xing-%EF%BC%8C-jiao-cheng-fei-chang-qing-xi-wo-gan-jue-wo-you-xing-le-%EF%BC%81.html</link><description>å¾ˆå¤šäººéƒ½è§‰å¾—è®­ç»ƒå¤§æ¨¡å‹æ˜¯ä¸ªå¾ˆéš¾çš„äº‹æƒ…ï¼ŒåŒ…æ‹¬å¤§éƒ¨åˆ†çš„ç¨‹åºå‘˜è‡ªå·±ä¹Ÿæä¸å‡ºæ¥ã€‚</description><guid isPermaLink="true">https://winnerineast.github.io/post/MiniMind%EF%BC%9A-yi-kai-yuan-%EF%BC%81-zhi-yao-3-xiao-shi-jiu-neng-xun-lian-yi-ge-26M-de-da-mo-xing-%EF%BC%8C-jiao-cheng-fei-chang-qing-xi-wo-gan-jue-wo-you-xing-le-%EF%BC%81.html</guid><pubDate>Sat, 12 Jul 2025 12:15:48 +0000</pubDate></item><item><title>DeepSearcherå¼€æºï¼šå‘Šåˆ«ä¼ ç»ŸRAGï¼Œç§æœ‰æ•°æ®+Deepseekï¼Œæ‰“é€ æœ¬åœ°ç‰ˆDeep Research</title><link>https://winnerineast.github.io/post/DeepSearcher-kai-yuan-%EF%BC%9A-gao-bie-chuan-tong-RAG%EF%BC%8C-si-you-shu-ju-%2BDeepseek%EF%BC%8C-da-zao-ben-di-ban-Deep%20Research.html</link><description>
åŸåˆ›Â å’Œä½ ä¸€èµ·è¿›æ­¥çš„Â Zilliz

Â _2025å¹´02æœˆ13æ—¥ 19:06_

![å›¾ç‰‡](https://mmbiz.qpic.cn/mmbiz_png/MqgA8Ylgeh69MGal1BGtDcKwvTTfp5d0DHtYLLGMqYNKG5qwPU0VWeyZibj6YxULOqb3M2heiaUflhftAyLmniciag/640?wx_fmt=other&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1&amp;tp=wxpic)

![å›¾ç‰‡](https://mmbiz.qpic.cn/mmbiz_jpg/MqgA8Ylgeh5IGBMTYqBrAcxy5ZYQ8PyPustqwVcWc7JZ44y2ibSqYia0apqCylI22ia3goM9kmP8CHvwef4CgOLicw/640?wx_fmt=jpeg&amp;tp=wxpic&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1)

# **å‰è¨€**

è¿‘æ—¥ï¼ŒOpen AIçš„Deep Researchï¼ˆæ·±åº¦ç ”ç©¶ï¼‰åŠŸèƒ½ä¸€ç»æ¨å‡ºï¼Œè¿…é€Ÿå—åˆ°è¯¸å¤šå…³æ³¨ï¼Œé€šè¿‡å°†å¤§æ¨¡å‹+è¶…çº§æœç´¢+ç ”ç©¶åŠ©ç†çš„ä¸‰åˆä¸€ï¼Œé‡‘èæœºæ„ä¸€é”®ç”ŸæˆæŠ¥å‘Šã€ç§‘ç ”å…šä¸€é”®ç”Ÿæˆç»¼è¿°æˆä¸ºå¯èƒ½ã€‚</description><guid isPermaLink="true">https://winnerineast.github.io/post/DeepSearcher-kai-yuan-%EF%BC%9A-gao-bie-chuan-tong-RAG%EF%BC%8C-si-you-shu-ju-%2BDeepseek%EF%BC%8C-da-zao-ben-di-ban-Deep%20Research.html</guid><pubDate>Sat, 12 Jul 2025 12:14:48 +0000</pubDate></item><item><title>ç»Ÿæ²»è½¯ä»¶å¼€å‘çš„è‘—åå®šå¾‹</title><link>https://winnerineast.github.io/post/tong-zhi-ruan-jian-kai-fa-de-zhu-ming-ding-lv.html</link><description>
åŸæ–‡ï¼šhttps://www.timsommer.be/famous-laws-of-software-development/

ç¿»è¯‘| ç å†œç¿»èº«

å’Œå…¶ä»–é¢†åŸŸä¸€æ ·ï¼Œåœ¨è½¯ä»¶å¼€å‘çš„ä¸–ç•Œä¸­ä¹Ÿæœ‰ä¸€äº›æœ‰è¶£è€Œè‘—åçš„å®šå¾‹ï¼Œå¼€å‘äººå‘˜ã€ç®¡ç†äººå‘˜è¿˜æ˜¯æ¶æ„å¸ˆï¼Œéƒ½ç»å¸¸åœ¨ä¼šè®®æˆ–é—²è°ˆä¸­æåˆ°ä»–ä»¬ï¼Œå¾ˆå¤šæ—¶å€™æˆ‘ä»¬éƒ½åªæ˜¯ç‚¹å¤´é™„å’Œï¼Œå…å¾—è®©äººçŸ¥é“è‡ªå·±å…¶å®æ ¹æœ¬æ²¡å¬è¯´è¿‡å¸ƒé²å…‹æ–¯(Brooks)ã€æ‘©å°”(Moore)æˆ–åº·å¨(Conway)è¿™äº›å¤§ä½¬ã€‚</description><guid isPermaLink="true">https://winnerineast.github.io/post/tong-zhi-ruan-jian-kai-fa-de-zhu-ming-ding-lv.html</guid><pubDate>Sat, 12 Jul 2025 12:13:45 +0000</pubDate></item><item><title>Mitigating LLM Hallucinations - a multifaceted approach</title><link>https://winnerineast.github.io/post/Mitigating%20LLM%20Hallucinations%20-%20a%20multifaceted%20approach.html</link><description>
(I recently turned this guide into a paper. You can find itÂ [here](https://amatria.in/blog/hallucination-paper))

- [Introduction](https://amatria.in/blog/hallucinations#introduction)
- [What we talk about when we talk about Hallucinations](https://amatria.in/blog/hallucinations#definition)
- [How to Measure](https://amatria.in/blog/hallucinations#measurement)
- [Mitigating Hallucinations: a multifacted approach](https://amatria.in/blog/hallucinations#mitigation)
    - [Product design approaches](https://amatria.in/blog/hallucinations#product)
    - [Prompt Engineering solutions](https://amatria.in/blog/hallucinations#promptengineering)
    - [Grounding with RAG](https://amatria.in/blog/hallucinations#rag)
    - [Advanced Prompt Engineering methods](https://amatria.in/blog/hallucinations#advancedprompting)
    - [Model Choices](https://amatria.in/blog/hallucinations#modeling)
    - [Reinforcement Learning from Human Feedback (RLHF)](https://amatria.in/blog/hallucinations#rlhf)
    - [Domain adaptation through Fine-Tuning](https://amatria.in/blog/hallucinations#finetuning)
- [Conclusion: Yann vs. Ilya](https://amatria.in/blog/hallucinations#conclusion)

# Introduction[Permalink](https://amatria.in/blog/hallucinations#introduction 'Permalink')

Ever been curious about the complexities of integrating large language models (LLMs) into user-facing products? One challenge that has been gaining attention is the occurrence of â€˜hallucinationsâ€™â€”situations where these advanced AI systems produce misleading or incorrect information. This is a real-world issue that many of us in the tech industry are actively working to address as we develop new features and services.

In this blog post, youâ€™ll find a comprehensive guide to the most effective strategies for mitigating these hallucinations in user-facing products. The field is fast-evolving, so while I donâ€™t plan on continuously updating this post, I hope it serves as a valuable snapshot of current best practices. Iâ€™m also open to your insights and ideas, so feel free to reach out with any suggestions or questions you might have.

![](https://amatria.in/blog/images/106-0.png)Â _A multifaceted approach to mitigating LLM hallucinations_

# What we talk about when we talk about hallucinations[Permalink](https://amatria.in/blog/hallucinations#what-we-talk-about-when-we-talk-about-hallucinations 'Permalink')

In the context of Large Language Models (LLMs), the term â€œhallucinationsâ€ often surfaces. As defined by the â€œSurvey of Hallucination in Natural Language Generationâ€ paper, a hallucination in an LLM refers to â€œthe generation of content that is nonsensical or unfaithful to the provided source.â€

## A Controversial Term: Unpacking the Use of â€œHallucinationâ€ in AI[Permalink](https://amatria.in/blog/hallucinations#a-controversial-term-unpacking-the-use-of-hallucination-in-ai 'Permalink')

According toÂ [Wikipedia](https://en.wikipedia.org/wiki/Hallucination), a hallucination is defined as â€œa perception in the absence of an external stimulus that has the qualities of a real perception.â€ Such a description might evoke images of mysterious visions or imagined sounds. However, the term has taken on a different, though not uncontroversial, shade of meaning in the realm of artificial intelligence.

There are three main concerns Iâ€™ve come across regarding the use of â€œhallucinationâ€ to describe phenomena in AI systems:

- **Misattribution of Properties:**Â The application of â€œhallucinationâ€ might inadvertently suggest that LLMs possess some form of consciousness or perception, which they certainly donâ€™t. LLMs generate text based on patterns in their training data, not because they â€œperceiveâ€ or â€œimagineâ€ in the way living creatures do.
- **Misunderstanding of Dynamics:**Â Such terminology might cloud understanding about how LLMs function. They donâ€™t â€œseeâ€ or â€œimagine.â€ Instead, they churn out text based on statistical patterns from their training data.
- **Ethical Implications:**Â Thereâ€™s a fear that describing AI outputs as â€œhallucinationsâ€ trivializes the potential risks of LLMs providing incorrect or misleading information, especially if users over-rely on these models without proper fact-checking.

However, the AI context for â€œhallucinationâ€ has even been acknowledged by dictionaries. For instance,Â [Merriam-Webster](https://www.merriam-webster.com/dictionary/hallucination)Â defines it in the context of AI as â€œa plausible but false or misleading response generated by an artificial intelligence algorithm.â€

Interestingly, this term isnâ€™t freshly minted. Andrej KarpathyÂ [suggested](https://twitter.com/karpathy/status/1702916988891193460)Â that he might have popularized the term in his enlighteningÂ [blog post](https://karpathy.github.io/2015/05/21/rnn-effectiveness/)Â from 2015. But a little digging reveals earlier uses. Notably, anÂ [ACL conference paper](https://aclanthology.org/W14-1617.pdf)Â from 2014 discussed â€œhallucinatingâ€ translations. Even further back, a 2009 paper titledÂ [â€œReview Sentiment Scoring via a Parse-and-Paraphrase Paradigmâ€](https://aclanthology.org/D09-1017.pdf)Â used the term in the context of â€œhallucinatingâ€ topics. But perhaps the most ancient reference I found was in a 1996 paper,Â [â€œText Databases and Information Retrievalâ€](https://dl.acm.org/doi/pdf/10.1145/234313.234371), which talked about systems that could â€œhallucinateâ€ words not present in the original document.

In my view, itâ€™s clear that â€œhallucinationâ€ in AI has been in the lexicon for some time, carving out a niche meaning, distinct from its psychological roots.

Itâ€™s also worth noting that AI is replete with terms borrowed from human analogies - take â€œneural networksâ€ for instance. Despite initial reservations, these terms have become integral, largely uncontroversial components of AI discourse.

## Types of Hallucinations[Permalink](https://amatria.in/blog/hallucinations#types-of-hallucinations 'Permalink')

Hallucinations can be categorized into two main types:

- **Intrinsic Hallucinations:**Â These directly contradict the source material, introducing factual inaccuracies or logical inconsistencies.
- **Extrinsic Hallucinations:**Â These do not contradict but also cannot be verified against the source, adding elements that could be considered speculative or unconfirmable.

## The Nuanced Role of the â€˜Sourceâ€™[Permalink](https://amatria.in/blog/hallucinations#the-nuanced-role-of-the-source 'Permalink')

The concept of a â€˜sourceâ€™ varies depending on the specific task an LLM is performing. In dialogue-based tasks, the source can be considered as â€˜world knowledge.â€™ However, when the task involves text summarization, the source is the input text itself. This is a critical nuance that significantly impacts both the evaluation and interpretation of hallucinations.

## Contextual Importance of Hallucinations[Permalink](https://amatria.in/blog/hallucinations#contextual-importance-of-hallucinations 'Permalink')

The implications of hallucinations are highly context-dependent. For example, in creative applications such as poem-writing, the presence of hallucinations may not only be acceptable but could potentially enrich the output.

## Why do LLMs hallucinate[Permalink](https://amatria.in/blog/hallucinations#why-do-llms-hallucinate 'Permalink')

It is important to first keep in mind that LLMs have been pre-trained to predict tokens. They do not have a notion of true/false or correct/incorrect, but rather base their text generation on probabilities. While that leads to some unexpected reasoning abilities (such as being able to pass the legal BAR exam or the medical USMLE), that is only a result of this probabilistic token by token reasoning. To be fair, the additional training steps of instruct tuning and RLHF that most modern LLMs have do introduce a bit more â€œbias towards factualityâ€, but they do not change the overal underlying mechanism and its pitfalls.

LLMs have been trained on the whole internet, book collections, question/answers, and Wikipedia, among many others. They have good and not-so-good knowledge in their training set. Their responses are biased towards whatever they have seen the most. If you ask an LLM a medical question and you are not careful on how you prompt it, you might get an answer that is mostly aligned to the best medical literature or to random Reddit threads.

In a recent paper entitledÂ [â€œSources of Hallucination by Large Language Models on Inference Tasksâ€](https://arxiv.org/abs/2305.14552), the authors show how hallucinations are originated by two aspects of the LLMâ€™s training dataset: veracity prior and the relative frequency heuristic.

# How to Measure Hallucinations in Large Language Models[Permalink](https://amatria.in/blog/hallucinations#how-to-measure-hallucinations-in-large-language-models 'Permalink')

Understanding hallucinations is one thing, but quantifying them? Thatâ€™s where things get really interesting. Quantitative metrics are essential for assessing the effectiveness of mitigation strategies. In this section, Iâ€™ll guide you through the recommended methodologies for measuring hallucinations.

## A Five-Step Approach to Quantitative Measurement[Permalink](https://amatria.in/blog/hallucinations#a-five-step-approach-to-quantitative-measurement 'Permalink')

Based on best practices in the field, hereâ€™s a systematic five-step approach to accurately measure hallucinations:

**1. Identify Grounding Data:**Â Grounding data serves as the benchmark for what the LLM should produce. The choice of grounding data varies by use-case. For instance, actual resumes could serve as grounding data when generating resume-related information. On the other hand, search engine results could be used for web-based queries.

**2. Create Measurement Test Sets:**Â These sets usually consist of input/output pairs and may include human-LLM conversations, depending on the application. Ideally, youâ€™d have at least two types of test sets: * A generic or random test set * An adversarial test set, generated from red-teaming exercises to include challenging or high-risk edge cases.

**3. Extract Claims:**Â After preparing the test sets, the next step is to extract claims made by the LLM. This can be done manually, through rule-based methods, or even using machine learning models. Each technique has its pros and cons, which we will explore in detail.

**4. Validate Against Grounding Data:**Â Validation ensures that the LLMâ€™s generated content aligns with the grounding data. This step often mirrors the extraction methods used previously.

**5. Report Metrics:**Â The â€œGrounding Defect Rateâ€ is a fundamental metric that quantifies the ratio of ungrounded responses to the total number of generated outputs. Additional metrics will be discussed later for a more nuanced evaluation.

## Evaluating Hallucinations: Common Metrics and Methodologies[Permalink](https://amatria.in/blog/hallucinations#evaluating-hallucinations-common-metrics-and-methodologies 'Permalink')

Quantifying hallucinations in Large Language Models isnâ€™t just about recognizing that they existâ€”itâ€™s about measuring them rigorously. In this section, Iâ€™ll delve into the different types of metrics commonly employed for this purpose.

### Statistical Metrics[Permalink](https://amatria.in/blog/hallucinations#statistical-metrics 'Permalink')

Metrics like ROUGE and BLEU are often the go-to choices for text similarity evaluations. They focus on the intrinsic type of hallucinations by comparing the generated output against a source. Advanced metrics such as PARENT, PARENT-T, and Knowledge F1 come into play when a structured knowledge source is available. However, these metrics have limitations: they primarily focus on intrinsic hallucinations and can falter when capturing syntactic and semantic nuances.

### Model-Based Metrics[Permalink](https://amatria.in/blog/hallucinations#model-based-metrics 'Permalink')

Model-based metrics leverage neural networks, making them more adaptable to syntactic and semantic complexities. They come in various flavors:

**IE-based Metrics:**Â These use Information Extraction (IE) models to distill the knowledge into a simpler relational tuple formatâ€”think subject, relation, object. The model then validates these tuples against those extracted from the source or reference.

- **QA-based Metrics:**Â These implicitly measure the overlap or consistency between the generated content and the source. If the content is factually consistent with the source, similar answers will be generated to the same questions. (see e.g. â€œEvaluating Factual Consistency in Knowledge-Grounded Dialogues via Question Generation and Question Answeringâ€)
- **NLI-based Metrics:**Â Utilizing Natural Language Inference (NLI) datasets, these metrics determine if a generated â€œhypothesisâ€ is true, false, or undetermined given a â€œpremiseâ€.(see e.g. â€œEvaluating Groundedness in Dialogue Systems: The BEGIN Benchmarkâ€).
- **Faithfulness Classification Metrics:**Â These improve upon NLI-based metrics by creating task-specific datasets, thereby providing a more nuanced evaluation. . (see e.g. â€œRome was built in 1776: A Case Study on Factual Correctness in Knowledge-Grounded Response Generationâ€).

### The Role of Human Evaluation[Permalink](https://amatria.in/blog/hallucinations#the-role-of-human-evaluation 'Permalink')

Despite the sophistication of automated metrics, human evaluation still holds significant value. Two primary approaches are commonly employed:

1. **Scoring:**Â Human annotators assign scores within a defined range to rate the level of hallucination.
2. **Comparing:**Â Here, human annotators evaluate the generated content against baselines or ground-truth references, providing an additional layer of validation.

### The example of FActScore[Permalink](https://amatria.in/blog/hallucinations#the-example-of-factscore 'Permalink')

[FActScore](https://arxiv.org/abs/2305.14251)Â is a recent example of a metric that can be used both for human and model-based evaluation. The metric breaks an LLM generation into â€œatomic factsâ€. The final score is computed as the sum of the accuracy of each atomic fact, giving each of them equal weight. Accuracy is a binary number that simply states whether the atomic fact is supported by the source. The authors implement different automation strategies that use LLMs to estimate this metric.

![](https://amatria.in/blog/images/106-9.png)

# The Art of Red Teaming: Best Practices for Stress-Testing LLMs[Permalink](https://amatria.in/blog/hallucinations#the-art-of-red-teaming-best-practices-for-stress-testing-llms 'Permalink')

While statistical and model-based metrics are indispensable for measuring hallucinations in LLMs, itâ€™s equally important to put these models through the rigor of human evaluation. Red teaming provides an essential layer of scrutiny that complements systematic measurement. Here are some best practices to follow:

**Keep Red Teaming Complementary:**Â Although red teaming and stress-testing are invaluable tools, they should not replace systematic measurement. They are meant to augment, not substitute.

**Test in Real-world Conditions:**Â Whenever possible, conduct your testing on the production endpoint. This allows for a more realistic assessment of how the model behaves under actual conditions.

**Define Harms and Guidelines:**Â Clearly outline the potential harms and provide specific guidelines to the testers. This ensures that everyone is aligned on what to look for during testing.

**Prioritize Your Focus Areas:**Â Identify the key features, harms, and scenarios that should be prioritized in the red teaming exercise. This focused approach yields more actionable insights.

**Diverse and Skilled Testers:**Â A diverse set of testers with different areas of expertise can provide a multi-faceted evaluation. Diversity here can mean different domains of knowledge, different cultural backgrounds, or even different biases.

**Documentation is Key:**Â Decide in advance what kinds of data or findings youâ€™d like your testers to document. Clear documentation aids in a more structured evaluation process.

**Manage Tester Time and Well-being:**Â Determine how much time each tester should ideally dedicate to the task. Moreover, be cognizant of potential burnout or a decline in creativity over time, and plan accordingly.

New approaches to red teaming include using an LLM to read team another LLM. See e.g. Deepmindâ€™sÂ [â€œRed Teaming Language Models with Language Modelsâ€](https://arxiv.org/abs/2202.03286)

# Mitigating Hallucinations in Large Language Models: A Multifaceted Approach[Permalink](https://amatria.in/blog/hallucinations#mitigating-hallucinations-in-large-language-models-a-multifaceted-approach 'Permalink')

The road to minimizing hallucinations is paved with both challenges and opportunities. In this section, weâ€™ll explore various mitigation strategies that can be customized to fit the unique demands of different applications of large language models.

## Leverage Product Design to Minimize Impact[Permalink](https://amatria.in/blog/hallucinations#leverage-product-design-to-minimize-impact 'Permalink')

The first piece of advice is straightforward: if possible, design your use case in such a way that hallucinations become a non-issue. For instance, in applications that generate written content, focusing on opinion pieces rather than factual articles may naturally lower the risk of problematic hallucinations.

### Product-Level Recommendations[Permalink](https://amatria.in/blog/hallucinations#product-level-recommendations 'Permalink')

- **User Editability:**Â Allow users to edit AI-generated outputs. This not only adds an extra layer of scrutiny but also improves the overall reliability of the content.
- **User Responsibility:**Â Make it clear that users are ultimately responsible for the content that is generated and published.
- **Citations and References:**Â Enabling a feature that incorporates citations can serve as a safety net, helping users verify the information before disseminating it.
- **User Optionality:**Â Offer various operational modes, such as a â€œprecisionâ€ mode that uses a more accurate (but computationally expensive) model.
- **User Feedback:**Â Implement a feedback mechanism where users can flag generated content as inaccurate, harmful, or incomplete. This data can be invaluable for refining the model in future iterations.
- **Limit Output and Turns:**Â Be mindful of the length and complexity of generated responses, as longer and more complex outputs have a higher chance of producing hallucinations.
- **Structured Input/Output:**Â Consider using structured fields instead of free-form text to lower the risk of hallucinations. For example, if the application involves resume generation, predefined fields for educational background, work experience, and skills could be beneficial.

### Data Practices for Continuous Improvement[Permalink](https://amatria.in/blog/hallucinations#data-practices-for-continuous-improvement 'Permalink')

- **Maintain a Tracking Set:**Â A dynamic database should be maintained to log different types of hallucinations along with the necessary information to reproduce them. This can serve as a powerful tool for regression testing.
- **Privacy and Trust:**Â Given that the tracking set may contain sensitive data, adhere to best practices for data privacy and security.

## Prompt Engineering: Mastering the Art of Metaprompt Design[Permalink](https://amatria.in/blog/hallucinations#prompt-engineering-mastering-the-art-of-metaprompt-design 'Permalink')

Although large language models (LLMs) have come a long way, they are not yet perfectâ€”especially when it comes to grounding their responses. Thatâ€™s why understanding and effectively utilizing metaprompts can make a world of difference. A study revealed that simply instructing the LLM on what not to do could lower hallucination rates dramatically. Even better, guiding the model towards alternative actions slashed these rates further.

## General Guidelines to Curb Hallucinations[Permalink](https://amatria.in/blog/hallucinations#general-guidelines-to-curb-hallucinations 'Permalink')

- **Simplify Complex Tasks:**Â Break down intricate actions into simpler steps.
- **Harness Affordances:**Â Utilize built-in functions within your metaprompt.
- **Use Few-Shot Learning:**Â Include examples when you can.
- **Iterative Refinement:**Â Donâ€™t hesitate to tweak the modelâ€™s output.

One important thing to note is that while these techniques improve grounding, they also come at a computational cost. Anyone leveraging LLMs in product design will need to balance this trade-off carefully.

## Fine-Tuning Your Metaprompts[Permalink](https://amatria.in/blog/hallucinations#fine-tuning-your-metaprompts 'Permalink')

- **Assertive Tone:**Â Using ALL CAPS and highlighting certain directives can improve model compliance.
- **Context is King:**Â Providing more background information can better ground the model.
- **Refinement Steps:**Â Reevaluate the initial output and make necessary adjustments.
- **Inline Citations:**Â Ask the model to substantiate its claims.
- **Framing:**Â Approaching tasks as summarization often yields more grounded results compared to question-answering.
- **Selective Grounding:**Â Ascertain scenarios where grounding is a must versus where it may be optional.
- **Reiterate Key Points:**Â Repeating essential instructions at the end of the prompt can underline their importance.
- **Echoing Input:**Â Request the model to recap vital input details, ensuring alignment with the source data.
- **Algorithmic Filtering:**Â Utilize algorithms to sift through and prioritize the most relevant information.

In upcoming sections, weâ€™ll dissect advanced metaprompting techniques, such as the â€œchain of thoughtâ€ approach, and delve into how Retrieval-Augmented Generation (RAG) can be leveraged for better grounding.

## Chain of Thought[Permalink](https://amatria.in/blog/hallucinations#chain-of-thought 'Permalink')

Chain of thought was initially described in theÂ [â€œChain-of-Thought Prompting Elicits Reasoning in Large Language Modelsâ€](https://arxiv.org/abs/2201.11903)Â paper by Google researchers. The simple idea here is that given that LLMs have been trained to predict tokens and not explicitly reason, you can get them closer to reasoning if you specify those required reasoning steps. Here is a simple example from the original paper:

![](https://amatria.in/blog/images/106-1.png)

Note that in this case the â€œrequired reasoning stepsâ€ are given in the example in blue. This is the so-called â€œManual CoTâ€. There are in fact two ways of doing basic chain of thought prompting (see below). In the basic one, called zero-shot CoT, you simply ask the LLM to â€œthink step by stepâ€. In the more complex version, called â€œmanual CoTâ€ you have to give the LLM examples of thinking step by step to illustrate how to reason. Manual prompting is more effective, but harder to scale and maintain.

![](https://amatria.in/blog/images/106-2.png)

CoT is just a more structured approach to the â€œsimplify complex tasksâ€ generic recommendation above and is known to mitigate hallucinations in many situations.

## Grounding with RAG[Permalink](https://amatria.in/blog/hallucinations#grounding-with-rag 'Permalink')

Retrieval-Augmented Generation, commonly known as RAG, is a technique aimed at augmenting the capabilities of Large Language Models (LLMs). InitiallyÂ [presented](https://ai.facebook.com/blog/retrieval-augmented-generation-streamlining-the-creation-of-intelligent-natural-language-processing-models/)Â by Facebook in 2020 in the context of their BART model, RAG has since been incorporated as a feature in theÂ [Hugging Face library](https://huggingface.co/docs/transformers/model_doc/rag).

### The Core Concept[Permalink](https://amatria.in/blog/hallucinations#the-core-concept 'Permalink')

The fundamental idea behind RAG is straightforward: it merges a retrieval component with a generative component, allowing the two to complement each other. This process is visually explained in the diagram below, extracted from the original research paper.

![](https://amatria.in/blog/images/106-3.png)

By combining these two elements, RAG enables the LLM to access and incorporate external information, thereby grounding the generated content more effectively. The retrieval component fetches relevant data, while the generative aspect of the model synthesizes this data into coherent and contextually appropriate responses.

RAG has evolved to become an indispensable part of the prompt engineerâ€™s toolkit. Over time, it has expanded into more complex applications, effectively serving as a concrete example within the broader framework of Toolkits, where the â€œtoolâ€ is typically a straightforward retriever or query engine.

Because RAGÂ **grounds**Â the response to the LLM to external data, it is known to be a very effective technique to mitigate hallucinations. However, there are some caveats.

### RAG known caveats and guardrails[Permalink](https://amatria.in/blog/hallucinations#rag-known-caveats-and-guardrails 'Permalink')

**The Pitfall of Over-Reliance**. One significant drawback of using RAG is a pronounced over-reliance on the retrieval results, which can, in certain cases, lead to hallucinations. Itâ€™s crucial to understand that retrieval might produce results that are either empty, incorrect, or require further disambiguation. Below are strategies to handle each of these scenarios.

**Empty Results: Be Prepared for Voids**. When the retrieval engine returns empty results, it could either be due to a lack of relevant data in the document source or an incorrect query formulation. Meta-prompts should be designed to anticipate and guard against this scenario. If the retrieval engine returns no results, the system should opt for caution and decline to answer, stating something along the lines of, â€œSorry, we donâ€™t have enough information on this topic. Could you please rephrase your question?â€ More advanced strategies might involve internally reformulating the query to handle issues like user misspellings, which can lead to void results.

**Ambiguous Results: Seek Clarification**. For ambiguous queries such as â€œWhat is a good restaurant in Portland?â€, where Portland could refer to multiple locations, itâ€™s advisable to seek further clarification from the user. For example, â€œDid you mean Portland, OR, or Portland, ME?â€

**Wrong Results: Navigate Carefully**. Incorrect retrieval results are particularly challenging to address because they are difficult to identify without an external ground truth. While improving the accuracy of retrieval engines is a complex problem thatâ€™s beyond the scope of this document, we recommend analyzing the performance of your retrieval solution within your applicationâ€™s specific use cases. Design your prompts to be extra cautious in areas where the retrieval engine has been identified to be less accurate.

## Advanced Prompt Engineering methods[Permalink](https://amatria.in/blog/hallucinations#advanced-prompt-engineering-methods 'Permalink')

Over the past few months, significant efforts have been directed towards mitigating the issues of hallucinations and grounding in Large Language Models (LLMs). These endeavors have led to a variety of innovative approaches that tackle the problem from a prompt engineering perspective. Itâ€™s important to note that these advanced methods are distinctly different from the more straightforward â€œdesign tricksâ€ discussed earlier. I will give a few examples of advanced prompt engineering methods that are relevant in the context of preventing hallucination. If you are interested in a more comprehensive catalog, check my previous postÂ [â€œPrompt Engineering 201: Advanced methods and toolkitsâ€](https://amatriain.net/blog/prompt201)

**Complexity, Latency, and Cost**: Advanced prompt engineering techniques often introduce additional complexity, latency, and cost, primarily because they frequently involve making multiple calls to the LLM. However, itâ€™s crucial to grasp their functionality and to have these advanced methods in your prompt engineering toolbox.

**Trade-offs and Opportunities**: In some cases, the incremental costs and latency might be justifiable, given the improvement in grounding and reduction in hallucinations. Additionally, you may find opportunities to implement some of these advanced methods using smaller, more cost-effective models. This could offer a valuable compromise between performance and expense.

By understanding these advanced prompt engineering methods, you can make more informed decisions about when and how to apply them, and whether their benefits outweigh their costs for your specific application.

### Self-consistency[Permalink](https://amatria.in/blog/hallucinations#self-consistency 'Permalink')

Self consistency, introduced in the paperÂ [â€œSelfCheckGPT: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Modelsâ€](https://arxiv.org/abs/2303.08896), is a method to use an LLM to fact-check itself. The idea is a simple ensemble-based approach where the LLM is asked to generate several responses to the same prompt. The consistency between those responses indicates how accurate the response may be.

![](https://amatria.in/blog/images/106-4.png)

The diagram above illustrates the approach in a QA scenario. In this case, the â€œconsistencyâ€ is measured by the number of answers to passages that agree with the overall answer. However, the authors introduce two other measures of consistency (BERT-scores, and n-gram), and a fourth one that combines the three.

### Reason and act (React)[Permalink](https://amatria.in/blog/hallucinations#reason-and-act-react 'Permalink')

React is a specific approach to designing agents introduced by Google inÂ [â€œReAct: Synergizing Reasoning and Acting in Language Modelsâ€](https://www.promptingguide.ai/techniques/react). This method prompts the LLM to generate both verbal reasoning traces and actions in an interleaved manner, which allows the model to perform dynamic reasoning. Importantly, the authors find that the React approach reduces hallucination from CoT. However, this increase in groundedness and trustworthiness, also comes at the cost of slightly reduced flexibility in reasoning steps (see the paper for more details).

![](https://amatria.in/blog/images/106-5.png)

### Reflection[Permalink](https://amatria.in/blog/hallucinations#reflection 'Permalink')

In the Self-consistency approach we saw how LLMs can be used to infer the confidence in a response. In that approach, confidence is measured as a by-product of how similar several responses to the same question are. Reflection goes a step further and tries to answer the question of whether (or how) we can ask an LLM directly about the confidence in its response. AsÂ [Eric Jang puts it](https://evjang.com/2023/03/26/self-reflection.html), there is â€œsome preliminary evidence that GPT-4 possesses some ability to edit own prior generations based on reasoning whether their output makes senseâ€.

The ReflexionÂ [paper](https://arxiv.org/abs/2303.11366)Â proposes an approach defined as â€œreinforcement via verbal reflectionâ€ with different components. The actor, an LLM itself, produces a trajectory (hypothesis). The evaluator produces a score on how good that hypothesis is. The self reflection component produces a summary that is stored in memory. The process is repeated iteratively until the Evaluator decides it has a â€œgood enoughâ€ answer. The authors show through experiments how reflection greatly improves the ability of detecting hallucinations even when compared to a ReAct agent.

![](https://amatria.in/blog/images/106-6.png)

### Dialog-Enabled Resolving Agents (DERA)[Permalink](https://amatria.in/blog/hallucinations#dialog-enabled-resolving-agents-dera 'Permalink')

[DERA](https://arxiv.org/abs/2303.17071), developed by my former team at Curai Health for their specific healthcare approach, defines different agents that, in the context of a dialog, take different roles. In the case of high stakes situations like a medical conversation, it pays off to define a set of â€œResearchersâ€ and a â€œDeciderâ€. The main difference here is that the Researchers operate in parallel vs. the Reflexion Actors that operate sequentially only if the Evaluator decides.

![](https://amatria.in/blog/images/106-7.png)

### Chain-of-Verification (COVE)[Permalink](https://amatria.in/blog/hallucinations#chain-of-verification-cove 'Permalink')

[COVE](https://arxiv.org/abs/2309.11495), recently presented by Meta, presents yet another variation on using different instances of the LLM to produce several responses and self-validate. In their approach, illustrated in the figure below, the model first (i) drafts an initial response; then (ii) plans verification questions to fact-check its draft; (iii) answers those questions independently so the answers are not biased by other responses; and (iv) generates its final verified response.

![](https://amatria.in/blog/images/106-8.png)

### Rails[Permalink](https://amatria.in/blog/hallucinations#rails 'Permalink')

AÂ [rail](https://github.com/NVIDIA/NeMo-Guardrails/blob/main/docs/README.md)Â is simply a programmable way to control the output of an LLM. Rails are specified using Colang, a simple modeling language, and Canonical Forms, templates to standardize natural language sentences (seeÂ [here](https://github.com/NVIDIA/NeMo-Guardrails/blob/main/docs/getting_started/hello-world.md))

Using rails, one can implement ways to have the LLM behave in a particular way. Of particular interest to our discussion, there is a rail to minimize hallucination (Fact checking rail).

### Guidance (Constrained Prompting)[Permalink](https://amatria.in/blog/hallucinations#guidance-constrained-prompting 'Permalink')

â€œConstrained Promptingâ€ is a term recentlyÂ [introduced](https://youtu.be/bZQun8Y4L2A?t=2093)Â by Andrej Karpathy to describe approaches and languages that allow us to interleave generation, prompting, and logical control in an LLM flow.

[Guidance](https://github.com/microsoft/guidance)Â is the only example of such an approach that I know although one could argue that React is also a constrained prompting approach. The tool is not so much a prompting approach but rather a â€œprompting languageâ€. Using guidance templates, you can pretty much implement most if not all the approaches in this post. Guidance uses a syntax based onÂ [Handlebars](https://handlebarsjs.com/)Â that allows to interleave prompting and generation, as well as manage logical control flow and variables. Because Guidance programs are declared in the exact linear order that they will be executed, the LLM can, at any point, be used to generate text or make logical decisions.

## Model Choices for Mitigating Hallucinations[Permalink](https://amatria.in/blog/hallucinations#model-choices-for-mitigating-hallucinations 'Permalink')

### Size and Model Complexity as a General Heuristic[Permalink](https://amatria.in/blog/hallucinations#size-and-model-complexity-as-a-general-heuristic 'Permalink')

A well-accepted guideline within the field suggests that larger, more complex models typically offer superior grounding capabilities. For example, empirical evaluations have shown that GPT-4 substantially outperforms its predecessor, GPT-3.5, in reducing the occurrence of hallucinations.

### The Significance of Model Temperature[Permalink](https://amatria.in/blog/hallucinations#the-significance-of-model-temperature 'Permalink')

Model temperature serves as a critical hyperparameter that influences the stochastic behavior of the modelâ€™s output. In a nutshell, it determines the level of randomness when predicting subsequent tokens. Higher temperatures increase the selection probabilities for tokens that are less likely, making the modelâ€™s output more diverse but potentially less grounded. Conversely, a lower temperature, approaching zero, results in the model sticking more closely to high-probability tokens, generally yielding more reliable and grounded outputs.

### Leveraging Reinforcement Learning from Human Feedback (RLHF)[Permalink](https://amatria.in/blog/hallucinations#leveraging-reinforcement-learning-from-human-feedback-rlhf 'Permalink')

RLHF methods can be applied during the later stages of training to optimize for more accurate and grounded outputs. These methods have shown marked improvements in hallucination mitigation, especially for models that have undergone domain-specific fine-tuning.

### Domain adaptation through Fine-Tuning[Permalink](https://amatria.in/blog/hallucinations#domain-adaptation-through-fine-tuning 'Permalink')

Lastly, if youâ€™re developing for a specific application, you might want to consider fine-tuning your internal models. Fine-tuning to your own data and examples can make a world of difference in grounding your outputs and minimizing those pesky hallucinations, particularly if you want to use a smaller and more efficient LLM. As of this writing, OpenAI offers fine-tuning for GPT-3.5 Turbo and acknowledges that in some applications this can yield better results than using the much larger and expensive GPT-4.

# Conclusion[Permalink](https://amatria.in/blog/hallucinations#conclusion 'Permalink')

As we have seen in this discussion of hallucinations, the problem is not an easy one to solve. In fact, Yann Lecun argues thatÂ [it cannot be solved](https://spectrum.ieee.org/ai-hallucination)Â without a complete redesign of the underlying models (although Ilya Sutskever disagrees). I stand somewhere in between: with the current underlying technology, hallucinations are just an expected side-effect and are hard to completely rule out. However, a combination of techniques can mitigate them and make them completely acceptable for most if not all use cases. After all, as I explained inÂ [a previous blog post](https://amatriain.net/blog/llmsdoctors), even medical doctors hallucinate!ã€‚</description><guid isPermaLink="true">https://winnerineast.github.io/post/Mitigating%20LLM%20Hallucinations%20-%20a%20multifaceted%20approach.html</guid><pubDate>Sat, 12 Jul 2025 12:13:12 +0000</pubDate></item><item><title>FLOAT</title><link>https://winnerineast.github.io/post/FLOAT.html</link><description>https://deepbrainai-research.github.io/float/
### Generative Motion Latent Flow Matching for Audio-driven Talking Portrait

[Taekyung Ki1](https://taekyungki.github.io/)Â [Dongchan Min2](https://kevinmin95.github.io/)Â [Gyeongsu Chae1](https://www.aistudios.com/ko/home)

1DeepBrain AI Inc.Â 2Graduate School of AI, KAIST

ArXiv 2024

[Paper (Soon)](https://deepbrainai-research.github.io/float/)Â [Video (Soon)](https://deepbrainai-research.github.io/float/)Â [Code (Soon)](https://deepbrainai-research.github.io/float/)

## Abstract

![](https://deepbrainai-research.github.io/float/src/img/float-abstract.png)  

With the rapid advancement of diffusion-based generative models, portrait image animation has achieved remarkable results. However, it still faces challenges in temporally consistent video generation and fast sampling due to its iterative sampling nature. This paper presents FLOAT, an audio-driven talking portrait video generation method based on flow matching generative model. We shift the generative modeling from the pixel-based latent space to a learned motion latent space, enabling efficient design of temporally consistent motion. To achieve this, we introduce a transformer-based vector field predictor with a simple yet effective frame-wise conditioning mechanism. Additionally, our method supports speech-driven emotion enhancement, enabling a natural incorporation of expressive motions. Extensive experiments demonstrate that our method outperforms state-of-the-art audio-driven talking portrait methods in terms of visual quality, motion fidelity, and efficiency.

TL;DR:Â FLOAT is a flow matching based audio-driven talking portrait video generation method, which can enhance the speech-driven emotional motion.

## Method Overview

![Responsive image](https://deepbrainai-research.github.io/float/src/img/overview.png)

Audio-driven talking portrait aims to synthesize talking portrait videos using a single source portrait image and driving audio. FLOAT is built upon a motion latent auto-encoder that encodes the given portrait image into an (identity-motion) latent representation. We generates audio-conditioned talking portrait motion latents through the flow matching (with optimal transport trajectories). To enhace the naturalness of generated talking motion, we incorporate the speech-driven emotion labels (ğŸ˜€), providing a natural approach of emotion-aware talking portrait motion generation.

## Results

  

#### Results with Out-of-distribution Datas

FLOAT can generate realistic talking portrait videos using OOD portrait images and audio.

  

  

  
  

#### Emotion Redirection

Since FLOAT is trained with speech-driven emotion labels, it can re-direct the emotion of the talking portrait during the inference phase. Specifically, we can manipulate the predicted speech-driven emotion label with a simple one-hot emotion label, which can be further refined through classifier-free vector fields. This enables users to refine emotion even when the driving speech conveys ambiguious or mixed emotions.

  

  
  

#### Comparison with State-of-the-art Methods

We compare with state-of-the-art non-diffusion-based methods and diffusion-based methods. For non-diffusion-based methods, we choose SadTalker and EDTalk. For diffusion-based methods, we choose AniTalker, Hallo, and EchoMimic.

[Previous](https://deepbrainai-research.github.io/float/#carouselExampleControls)[Next](https://deepbrainai-research.github.io/float/#carouselExampleControls)

  
  

#### Ablation Studies on Frame-wise AdaLN and Flow Matching

We conduct ablation studies on frame-wise AdaLN (and gating) and flow matching. For frame-wise AdaLN (and gating), we compare it with cross-attention mechanism, which is widely used in conditional generation. For flow matching, we compare it with two types of diffusion models (ÏµÏµ-prediction andÂ x0x0-prediction). We observe that frame-wise AdaLN (and gating) can generate more diverse head motions than cross-attention. We also observe that flow mathcing can generate more temporally consistent videos with accurate lip-synchronization than diffusion models.

[Previous](https://deepbrainai-research.github.io/float/#carouselExampleControls1)[Next](https://deepbrainai-research.github.io/float/#carouselExampleControls1)

  
  

#### Different Number of Function Evaluations (NFEs)

The small number of function evaluations (NFEs) affects temporal consistency. This is because we generate the motion latents, not the content itself. FLOAT is capable of generating reasonable video results with approximately 10 NFEs.

[Previous](https://deepbrainai-research.github.io/float/#carouselExampleControls2)[Next](https://deepbrainai-research.github.io/float/#carouselExampleControls2)

  
  

#### Emotion Guidance Scales

We can control the intensity of the emotion by adjusting the emotion guidance scale. Note that the predicted speech-driven emotion label isÂ **Disgust**Â with a 99.99% probability.

  
  

#### Additional Driving Conditions

We also investigate another types of driving conditions, such as 3DMM head pose parameters, to improve the controllability and naturalness. Here, 3DPose, S2E, and I2E are 3DMM head pose parameters, Speech-to-emotion label, and Image-to-emotion label, resepctively.

  

  
  

#### Ablation Study on Facial Component Perceptual Loss in Phase 1

The proposed facial component perceptual loss for the motion latent auto-encoder significantly improves visual quality (e.g., teeth and eyes), as well as fine-grained motion (e.g., eyeball movement and lip motion).

  

## Citation

If you want to cite our work, please use:

          TBA
          
      

## Acknowledgement

The source images and audio are collected from the internet and other baselines, such as SadTalker, EMO, VASA-1, Hallo, LivePortrait, Loopy, and others. We appreciate their valuable contributions to this field. This project page is based on the project page ofÂ [RegNeRF](https://m-niemeyer.github.io/regnerf). You can easily use it fromÂ [the github repository](https://github.com/m-niemeyer/regnerf).ã€‚</description><guid isPermaLink="true">https://winnerineast.github.io/post/FLOAT.html</guid><pubDate>Sat, 12 Jul 2025 12:12:06 +0000</pubDate></item><item><title>A statistical approach to model evaluations</title><link>https://winnerineast.github.io/post/A%20statistical%20approach%20to%20model%20evaluations.html</link><description>

Nov 20, 2024

[Read the paper](https://arxiv.org/abs/2411.00640)

Suppose an AI model outperforms another model on a benchmark of interestâ€”testing its general knowledge, for example, or its ability to solve computer-coding questions. Is the difference in capabilities real, or could one model simply have gotten lucky in the choice of questions on the benchmark?

With the amount of public interest in AI model evaluationsâ€”informally called â€œevalsâ€â€”this question remains surprisingly understudied among the AI research community. This month, we published aÂ [new research paper](https://arxiv.org/abs/2411.00640)Â that attempts to answer the question rigorously. Drawing on statistical theory and the experiment design literature, the paper makes a number of recommendations to the AI research community for reporting eval results in a scientifically informative way. In this post, we briefly go over the reporting recommendations, and the logic behind them.

### Recommendation #1: Use the Central Limit Theorem

Evals often consist of hundreds or thousands of unrelated questions.Â [MMLU](https://arxiv.org/abs/2009.03300v3), for instance, contains questions as diverse as:

- Who discovered the first virus?
- What is the inverse of ğ‘“(ğ‘¥)=4âˆ’5ğ‘¥?
- Who said that â€œJurisprudence is the eye of lawâ€?

To compute an overall eval score, each question is separately scored, and then the overall score is (usually) a simple average of these question scores. Typically, researchers focus their attention on this observed average. But in our paper, we argue that the real object of interest should not be theÂ _observed_Â average, but rather theÂ _theoretical_Â average across all possible questions. So if we imagine that eval questions were drawn from an unseen â€œquestion universe,â€ we can learn about the average score in that universeâ€”that is, we can measure the underlyingÂ _skill_, independent of the â€œluck of the drawâ€â€”using statistical theory.

![](https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2Fb34871a36ad66fa0330e3ad6488ee87eb96bddda-2401x1260.png&amp;w=3840&amp;q=75)

If we imagine that eval questions were drawn from a â€œquestion universe,â€ then eval scores will tend to follow a normal distribution, centered around the average score of all possible questions.

This formulation buys us analytic robustness: if a new eval were to be created with questions having the same difficulty distribution as the original eval, we should generally expect our original conclusions to hold.

In technical terms: under the fairly mild conditions of theÂ [Central Limit Theorem](https://en.wikipedia.org/wiki/Central_limit_theorem), the mean values of several random samples taken from the same underlying distribution will tend to follow aÂ [normal distribution](https://en.wikipedia.org/wiki/Normal_distribution). The standard deviation (or width) of that normal distribution is commonly known as theÂ [standard error of the mean](https://en.wikipedia.org/wiki/Standard_error), or SEM. In our paper, we encourage researchers to report the SEM, derived from the Central Limit Theorem, alongside each calculated eval scoreâ€”and we show researchers how to use the SEM to quantify the difference in theoretical means between two models. A 95%Â [confidence interval](https://en.wikipedia.org/wiki/Confidence_interval)Â can be calculated from the SEM by adding and subtracting 1.96 Ã— SEM from the mean score.

### Recommendation #2: Cluster standard errors

Many evals violate the above assumption of independently selected questions, and instead consist of groups of closely related questions. For example, several questions in a reading-comprehension eval may ask about the same passage of text. Popular evals that follow this pattern includeÂ [DROP](https://aclanthology.org/N19-1246/),Â [QuAC](https://arxiv.org/abs/1808.07036),Â [RACE](https://aclanthology.org/D17-1082/), andÂ [SQuAD](https://arxiv.org/abs/1806.03822).

For these evals, each questionâ€™s selection from the â€œquestion universeâ€ is no longer independent. Because including several questions about the same passage of text will yield less information than selecting the same number of questions about different passages of text, a naive application of the Central Limit Theorem to the case of non-independent questions will lead us to underestimate the standard errorâ€”and potentially mislead analysts into drawing incorrect conclusions from the data.

Fortunately, the problem ofÂ [clustered standard errors](https://en.wikipedia.org/wiki/Clustered_standard_errors)Â has been extensively studied in the social sciences. When the inclusion of questions is non-independent, we recommend clustering standard errors on the unit ofÂ [randomization](https://en.wikipedia.org/wiki/Randomization)Â (for example, passage of text), and we provide applicable formulas in our paper.

![](https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2Ff6f90f93dc66380904709a3ef4d63b92332871fd-2401x1260.png&amp;w=3840&amp;q=75)

If questions arrive in related clustersâ€”a common pattern in reading-comprehension evalsâ€”eval scores will be more spread-out compared to the non-clustered case.

In practice, we have found that clustered standard errors on popular evals can be over three times as large as naive standard errors. Ignoring question clustering may lead researchers to inadvertently detect a difference in model capabilities when in fact none exists.

### Recommendation #3: Reduce variance within questions

Variance is a measurement of how spread-out a random variable is. The variance of an eval score is the square of the standard error of the mean, discussed above; this quantity depends on the amount of variance in the score on each individual eval question.

A key insight of our paper is to decompose a modelâ€™s score on a particular question into two terms that are added together:

- The mean score (the average score that the model would achieve if asked the same question an infinite number of timesâ€”even if the model might produce a different answer each time); and
- A random component (the difference between a realized question score and the mean score for that question).

Thanks to theÂ [law of total variance](https://en.wikipedia.org/wiki/Law_of_total_variance), reducing the variance in the random component directly leads to a smaller standard error of the overall mean, and thus greater statistical precision. Our paper highlights two strategies for reducing variance in the random component depending on whether or not the model is asked to think step by step before answering (a prompting technique known as CoT, or chain-of-thought reasoning).

If an eval uses chain-of-thought reasoning, we recommend resampling answers from the same model several times, and using the question-level averages as the question scores fed into the Central Limit Theorem. We note that theÂ [Inspect framework](https://github.com/UKGovernmentBEIS/inspect_ai/)Â correctly computes standard errors in this way via itsÂ [_epochs_Â parameter](https://inspect.ai-safety-institute.org.uk/scorers.html#sec-reducing-epochs).

![](https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2Fefef59a06ddeb530fa15f31dc0937f28f70f655b-2401x1260.png&amp;w=3840&amp;q=75)

If a model produces answers non-deterministically, then generating (and grading) several answers per question will result in less spread-out eval scores.

If the eval does not use chain-of-thought reasoning (i.e., its answers are not â€œpath dependentâ€), we note that the random component in the score may often be eliminated altogether using next-token probabilities from the language model. For example, if the correct answer to a multiple-choice question is â€œBâ€, we would simply use the probability of the model producing the token â€œBâ€ as the question score. We are not aware of an open-source evals framework which implements this technique.

### Recommendation #4: Analyze paired differences

Eval scores donâ€™t have any meaning on their own; they only make sense in relation to one another (one model outperforms another model, or ties another model, or outperforms a person). But could a measured difference between two models be due to the specific choice of questions in the eval, and randomness in the modelsâ€™ answers? We can find out with aÂ [two-sampleÂ _t_-test](https://en.wikipedia.org/wiki/Student%27s_t-test), using only the standard errors of the mean calculated from both eval scores.

However, a two-sample test ignores the hidden structure inside eval data. Since the question list is shared across models, conducting aÂ [paired-differences test](https://en.wikipedia.org/wiki/Paired_difference_test)Â lets us eliminate the variance in question difficulty and focus on the variance in responses. In our paper, we show how the result of a paired-differences test will be related to theÂ [Pearson correlation coefficient](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient)Â between two modelsâ€™ question scores. When the correlation coefficient is higher, the standard error of the mean difference will be smaller.

In practice, we find the correlation of question scores on popular evals between frontier models to be substantialâ€”between 0.3 and 0.7 on a scale of âˆ’1 to +1. Put another way, frontier models have an overall tendency to get the same questions right and wrong. Paired-difference analysis thus represents a â€œfreeâ€ variance reduction technique that is very well suited for AI model evals. Therefore, in the interest of extracting the clearest signal from the data, our paper recommends reporting pairwise informationâ€”mean differences, standard errors, confidence intervals, and correlationsâ€”whenever two or more models are being compared.

### Recommendation #5: Use power analysis

The flip side of the statistical significance coin is statistical power, which is the ability of a statistical test to detect a difference between two models, assuming such a difference exists. If an eval doesnâ€™t have very many questions, confidence intervals associated with any statistical tests will tend to be wide. This means that models will need to have a large underlying difference in capabilities in order to register a statistically significant resultâ€”and that small differences will likely go undetected. Power analysis refers to the mathematical relationship between observation count,Â [statistical power](https://en.wikipedia.org/wiki/Power_(statistics)), theÂ [false positive rate](https://en.wikipedia.org/wiki/False_positive_rate), and theÂ [effect size](https://en.wikipedia.org/wiki/Effect_size)Â of interest.

In our paper, we show how to apply concepts from power analysis to evals. Specifically, we show researchers how to formulate a hypothesis (such asÂ _Model A outperforms Model B by 3 percentage points_) and calculate the number of questions that an eval should have in order to test this hypothesis against the null hypothesis (such asÂ _Model A and Model B are tied_).

We believe that power analysis will prove helpful to researchers in a number of situations. Our power formula will inform evaluators of models about the number of times to re-sample answers from questions (see Recommendation #3 above), as well as the number of questions that may be included in a random subsample while retaining the desired power properties. Researchers might use the power formula to conclude that an eval with a limited number of available questions is not worth running on a particular pair of models. Developers of new evals may wish to use the formula to help decide how many questions to include.

### Conclusion

Statistics is the science of measurement in the presence of noise. Evals present a number of practicalÂ [challenges](https://www.anthropic.com/news/evaluating-ai-systems), and a trueÂ [science of evals](https://www.apolloresearch.ai/blog/we-need-a-science-of-evals)Â remains underdeveloped. Statistics can only form one aspect of a science of evalsâ€”but a critical one, as an empirical science is only as good as its measuring tools. We hope that the recommendations in our paperÂ [**Adding Error Bars to Evals: A Statistical Approach to Language Model Evaluations**](https://arxiv.org/abs/2411.00640)Â will help AI researchers calculate, interpret, and communicate eval numbers with greater precision and clarity than beforeâ€”and we encourage researchers in the AI community to explore other techniques from experiment design so that theyÂ may understand more exactly all the things that they want to measure.

  
ã€‚</description><guid isPermaLink="true">https://winnerineast.github.io/post/A%20statistical%20approach%20to%20model%20evaluations.html</guid><pubDate>Sat, 12 Jul 2025 12:10:19 +0000</pubDate></item><item><title>é¡¶çº§ LLM å’Œ VLM å­¦ä¹ èµ„æºæ¸…å•</title><link>https://winnerineast.github.io/post/ding-ji-%20LLM%20-he-%20VLM%20-xue-xi-zi-yuan-qing-dan.html</link><description>
1. makemore by [Andrej Karpathy](https://youtu.be/PaCmpygFfXo?si=dZCY8sictT658_Sk) 
2. minbpe by [karpathy](https://youtu.be/kCc8FmEb1nY?si=W61JnqMT4gjzF37q) 
3. attention? attention! [Lilian Weng](https://lilianweng.github.io/posts/2018-06-24-attention) ![[Attention_Attention.pdf]]
4. gpt-2 again by [karpathy](https://youtu.be/kCc8FmEb1nY?si=454hxUDiqD-t4YU0) 
5. llama3 from scratch by [naklecha](https://github.com/naklecha/llama3-from-scratch) 
6. llm training in simple, raw by c/cuda [karpathy](https://github.com/karpathy/llm.c) 
7. decoding strategies in large language models [mlabonne](https://mlabonne.github.io/blog/posts/2023-06-07-Decoding_strategies.html) 
8. how to make llms go fast by [vgel](https://vgel.me/posts/faster-inference/#KV_caching) ![[How to make LLMs go fast.pdf]]
9. a visual guide to quantization [maarten](https://www.maartengrootendorst.com/blog/quantization/)![[A visual guide to quantization.pdf]]
10. extending the RoPE by [eleutherai](https://blog.eleuther.ai/yarn) ![[Extending the RoPE.pdf]]
11. the novice's llm training guide by [alpin](https://rentry.org/llm-training) ![[The Novice LLM Training Guide.pdf]]
12. a survey on evaluation of large language models [paper](https://arxiv.org/abs/2307.03109) ![[2307.03109v9.pdf]]
13. mixture of experts explained [huggingface](https://huggingface.co/blog/moe) ![[Mixture of Experts Explained.pdf]]
14. vision transformer by [aman-arora](https://amaarora.github.io/posts/2021-01-18-ViT.html) ![[Vision Transformer.pdf]]
15. clip, siglip and paligemma by [umar-jamil](https://youtu.be/vAmKB7iPkWw?si=p4d2eoVNz0Nqwkxs)ã€‚</description><guid isPermaLink="true">https://winnerineast.github.io/post/ding-ji-%20LLM%20-he-%20VLM%20-xue-xi-zi-yuan-qing-dan.html</guid><pubDate>Sat, 12 Jul 2025 12:09:50 +0000</pubDate></item><item><title>è§£æ„ AI åŸç”Ÿåº”ç”¨ - ä¼ä¸šè½¯ä»¶çš„æœªæ¥å›¾æ™¯</title><link>https://winnerineast.github.io/post/jie-gou-%20AI%20-yuan-sheng-ying-yong-%20-%20-qi-ye-ruan-jian-de-wei-lai-tu-jing.html</link><description>
&lt;img width='2000' height='1125' alt='Image' src='https://github.com/user-attachments/assets/2a7cad0a-f31b-4ff1-b153-7c14a45da4ba' /&gt;

ã€Œæ¥è‡ª Sapphire Ventures çš„é‡è¦æ–‡ç« ï¼Œé€šè¿‡è®¾è®¡ã€æ•°æ®ã€ä¸“ä¸šé¢†åŸŸã€åŠ¨æ€ç‰¹æ€§ã€å•†ä¸šåˆ†å‘äº”å¤§ç»´åº¦ï¼Œæ¢è®¨ AI å¦‚ä½•ä»'åŠŸèƒ½é™„åŠ 'è½¬å‘'æ ¸å¿ƒé©±åŠ¨'ï¼Œé‡å¡‘ä¸‹ä¸€ä»£ä¼ä¸šè½¯ä»¶çš„å‘å±•æ–¹å‘ã€ ã€ŒAI åŸç”Ÿåº”ç”¨çš„å®šä¹‰ã€ - AI æ˜¯åº”ç”¨çš„æ ¸å¿ƒä½“éªŒï¼Œè€Œä¸åªæ˜¯é™„åŠ åŠŸèƒ½ - å»ºç«‹åœ¨åŸºç¡€ AI èƒ½åŠ›ä¹‹ä¸Š - èƒ½çªç ´ä¼ ç»Ÿçš„é€Ÿåº¦ã€è§„æ¨¡å’Œæˆæœ¬é™åˆ¶ - èƒ½æŒç»­æ”¹è¿›å’Œä¼˜åŒ– - åŒ…å«ä¸€å®šç¨‹åº¦çš„ä¸“æœ‰ AI æŠ€æœ¯ ã€Œå¸‚åœºç°çŠ¶ã€ - 2024å¹´ AI åŸç”Ÿåº”ç”¨çš„èèµ„è¾¾åˆ°85äº¿ç¾å…ƒ - å·²æœ‰è‡³å°‘47å®¶ AI åŸç”Ÿåº”ç”¨å…¬å¸å¹´æ”¶å…¥è¶…è¿‡2500ä¸‡ç¾å…ƒ - é¢„è®¡æ˜å¹´å°†æœ‰åŒç­‰æ•°é‡å…¬å¸å¹´æ”¶å…¥è¶…è¿‡5000ä¸‡ç¾å…ƒ ã€Œè¯„ä¼°æ¡†æ¶(5ä¸ªç»´åº¦)ã€ A. è®¾è®¡(Design): - åˆ›å»ºæ–°çš„äº¤äº’æ¨¡å¼ - åŠ é€Ÿåé¦ˆå¾ªç¯ - å¼€å‘ AI åŸç”Ÿç³»ç»Ÿæ¶æ„ B. æ•°æ®(Data): - æé«˜ç«¯åˆ°ç«¯æ•°æ®ç®¡ç†çš„ä¸¥è°¨æ€§ - åˆ©ç”¨æ½œåœ¨æ•°æ®ä»·å€¼ - åˆ›å»ºæ–°çš„ä¸“æœ‰æ•°æ®é›† C. é¢†åŸŸä¸“é•¿(Domain Expertise): - å°†ç‰¹å®šé¢†åŸŸæ´»åŠ¨è½¬åŒ–ä¸º AI åŠ é€Ÿçš„å·¥ä½œæµ - å¤§è§„æ¨¡å¿«é€Ÿåˆæˆ - ç»“åˆå…¨å±€å’Œæœ¬åœ°çŸ¥è¯† D. åŠ¨æ€æ€§(Dynamism): - å®æ—¶ä¼˜åŒ–æ€§èƒ½ä¸æˆæœ¬æƒè¡¡ - åˆ›å»ºç”Ÿæˆå¼å®¢æˆ·æ—…ç¨‹ - å®ç°å¤šå±‚æ¬¡è¶…ä¸ªæ€§åŒ– E. åˆ†å‘(Distribution): - å¢åŠ å®šä»·å’ŒåŒ…è£…çµæ´»æ€§ - å®ç°æ–°å•†ä¸šæ¨¡å¼ã€‚</description><guid isPermaLink="true">https://winnerineast.github.io/post/jie-gou-%20AI%20-yuan-sheng-ying-yong-%20-%20-qi-ye-ruan-jian-de-wei-lai-tu-jing.html</guid><pubDate>Sat, 12 Jul 2025 12:08:55 +0000</pubDate></item><item><title>æ¨¡å‹æ³¨å†Œè¡¨çš„å…¨æ™¯è¿ä½œ</title><link>https://winnerineast.github.io/post/mo-xing-zhu-ce-biao-de-quan-jing-yun-zuo.html</link><description>
æ­å¼€MLOpsçš„æ ¸å¿ƒå¥¥ç§˜ï¼šæ¨¡å‹æ³¨å†Œè¡¨çš„å…¨æ™¯è¿ä½œï¼ è¿™å¼ å›¾å±•ç¤ºäº†åœ¨MLOpsï¼ˆæœºå™¨å­¦ä¹ è¿ç»´ï¼‰ä¸­ï¼Œæ¨¡å‹æ³¨å†Œè¡¨å¦‚ä½•ä½œä¸ºæ ¸å¿ƒç¯èŠ‚ï¼Œåè°ƒå’Œç®¡ç†æ•´ä¸ªæœºå™¨å­¦ä¹ æµæ°´çº¿ã€‚</description><guid isPermaLink="true">https://winnerineast.github.io/post/mo-xing-zhu-ce-biao-de-quan-jing-yun-zuo.html</guid><pubDate>Sat, 12 Jul 2025 12:07:45 +0000</pubDate></item><item><title>å¦‚ä½•æ‰“é€ å±äºä½ çš„å®šåˆ¶åŒ–å¤§æ¨¡å‹èŠå¤©æœºå™¨äºº</title><link>https://winnerineast.github.io/post/ru-he-da-zao-shu-yu-ni-de-ding-zhi-hua-da-mo-xing-liao-tian-ji-qi-ren.html</link><description>
1. æ–‡æ¡£æ‹†è§£ï¼š - é¦–å…ˆï¼Œä½ å¾—æœ‰ä¸€äº›æ–‡æœ¬èµ„æ–™ï¼ˆDocumentsï¼‰ã€‚</description><guid isPermaLink="true">https://winnerineast.github.io/post/ru-he-da-zao-shu-yu-ni-de-ding-zhi-hua-da-mo-xing-liao-tian-ji-qi-ren.html</guid><pubDate>Sat, 12 Jul 2025 12:07:16 +0000</pubDate></item><item><title>å…¨é¢è§£ç MLOps - ä»æ„æƒ³åˆ°è½åœ°çš„æœºå™¨å­¦ä¹ ä¹‹æ—…</title><link>https://winnerineast.github.io/post/quan-mian-jie-ma-MLOps%20-%20-cong-gou-xiang-dao-luo-di-de-ji-qi-xue-xi-zhi-lv.html</link><description>&lt;img width='1818' height='1892' alt='Image' src='https://github.com/user-attachments/assets/792fc6af-5295-4faa-8570-fb3f33be16d6' /&gt;

è¿™å¼ å›¾å±•ç¤ºäº†ä¸€ä¸ªç«¯åˆ°ç«¯çš„MLOpsï¼ˆæœºå™¨å­¦ä¹ è¿ç»´ï¼‰æ¶æ„ï¼Œåˆ†ä¸ºå‡ ä¸ªå…³é”®æ­¥éª¤å’ŒåŒºåŸŸã€‚</description><guid isPermaLink="true">https://winnerineast.github.io/post/quan-mian-jie-ma-MLOps%20-%20-cong-gou-xiang-dao-luo-di-de-ji-qi-xue-xi-zhi-lv.html</guid><pubDate>Sat, 12 Jul 2025 12:06:34 +0000</pubDate></item><item><title>äº”å¤§ä¸»æµAgenticæ¡†æ¶ä¼˜åŠ£å¯¹æ¯”åˆ†æ</title><link>https://winnerineast.github.io/post/wu-da-zhu-liu-Agentic-kuang-jia-you-lie-dui-bi-fen-xi.html</link><description>
ã€Œå¾®è½¯ã€OpenAI ç­‰ç§‘æŠ€å·¨å¤´æ¨å‡ºçš„äº”å¤§å¤š AI ä»£ç†æ¡†æ¶å„å…·ç‰¹è‰²ï¼ŒAutoGen é€‚åˆå¼€å‘ã€CrewAI æ˜“ä¸Šæ‰‹ã€LangGraph æœ€çµæ´»ã€Swarm æœ€ç®€å•ã€Magnetic-One è¾ƒå…¨èƒ½ï¼Œå¼€å‘è€…è¯¥æ€ä¹ˆé€‰å‘¢ï¼Ÿã€ 1. AutoGen

[@pyautogen](https://x.com/pyautogen)

- ä¼˜åŠ¿: - æœ€æ—©ä¸”æœ€æµè¡Œçš„æ¡†æ¶ - ç‰¹åˆ«é€‚åˆè½¯ä»¶å¼€å‘ä»»åŠ¡ - æœ‰å¾®è½¯å¼ºå¤§çš„ç¤¾åŒºæ”¯æŒ - åŸºäºç”¨æˆ·ä»£ç†å’ŒåŠ©æ‰‹ä»£ç†çš„äº¤äº’æ¨¡å¼ - å±€é™: - å¯¹éç¨‹åºå‘˜ä¸å¤Ÿå‹å¥½ - è®¾ç½®å¤æ‚,ç‰¹åˆ«æ˜¯ä½¿ç”¨æœ¬åœ°LLMæ—¶ - åœ¨éè½¯ä»¶å¼€å‘ä»»åŠ¡ä¸Šè¡¨ç°ä¸€èˆ¬ 2. CrewAI

[@crewAIInc](https://x.com/crewAIInc)

- ä¼˜åŠ¿: - éå¸¸ç›´è§‚,ä¸»è¦ä¾èµ–æç¤ºè¯ç¼–å†™ - å®¹æ˜“åˆ›å»ºå’Œæ·»åŠ æ–°ä»£ç† - å¯¹éæŠ€æœ¯ç”¨æˆ·å‹å¥½ - ä¸å¤§å¤šæ•°LLMæä¾›å•†å…¼å®¹ - å±€é™: - çµæ´»æ€§å’Œè‡ªå®šä¹‰æ€§æœ‰é™ - ä¸»è¦é€‚åˆåŸºç¡€ç”¨ä¾‹ - ä»£ç†ä¹‹é—´çš„äº¤äº’å­˜åœ¨ä¸€äº›bug - ç¤¾åŒºæ”¯æŒæœ‰é™ 3. LangGraph

[@LangChainAI](https://x.com/LangChainAI)

- ä¼˜åŠ¿: - å»ºç«‹åœ¨LangChainä¹‹ä¸Š,åŸºäºæœ‰å‘å¾ªç¯å›¾ - éå¸¸çµæ´»å’Œå¯å®šåˆ¶ - æœ‰è‰¯å¥½çš„ç¤¾åŒºæ”¯æŒ - å¯ä¸å¼€æºLLMå’Œå„ç§APIé…åˆä½¿ç”¨ - å±€é™: - æ–‡æ¡£ä¸å¤Ÿå®Œå–„ - å¯¹éç¨‹åºå‘˜ä¸å¤Ÿå‹å¥½ - éœ€è¦è¾ƒå¼ºçš„ç¼–ç¨‹æŠ€èƒ½ 4. OpenAI Swarm

[@OpenAIDevs](https://x.com/OpenAIDevs)

- ä¼˜åŠ¿: - æœ€å®¹æ˜“ä¸Šæ‰‹çš„æ¡†æ¶ - ç®€åŒ–äº†ä»£ç†åˆ›å»ºå’Œåˆ‡æ¢ - é€‚åˆå¿«é€Ÿdemo - å±€é™: - ä»…æ”¯æŒOpenAI API - ä¸é€‚åˆç”Ÿäº§éƒ¨ç½² - çµæ´»æ€§ä¸è¶³ - ç¤¾åŒºæ”¯æŒè–„å¼± 5. Magnetic-One (

[@OpenAtMicrosoft](https://x.com/OpenAtMicrosoft)

) - ä¼˜åŠ¿: - é€‚åˆéç¨‹åºå‘˜ - é¢„è£…5ä¸ªä»£ç†(å«1ä¸ªç®¡ç†ä»£ç†) - å»ºç«‹åœ¨AutoGenä¹‹ä¸Š - å±€é™: - å¼€æºLLMæ”¯æŒå¤æ‚ - çµæ´»æ€§ä¸è¶³ - æ–‡æ¡£å’Œç¤¾åŒºæ”¯æŒå‡ ä¹æ²¡æœ‰ ä½œè€…çš„æœ€ç»ˆå»ºè®®: - è½¯ä»¶å¼€å‘ä»»åŠ¡é€‰æ‹© AutoGen - æ–°æ‰‹å…¥é—¨é€‰æ‹© OpenAI Swarm æˆ– CrewAI - å¤æ‚ä»»åŠ¡é€‰æ‹© LangGraph - å¼€æºLLMä½¿ç”¨æ¨è LangGraph - ç¤¾åŒºæ”¯æŒæœ€å¥½çš„æ˜¯ AutoGen - å¿«é€Ÿå¯åŠ¨é€‰æ‹© CrewAI - æˆæœ¬æ•ˆç›Šè€ƒè™‘ Magnetic-Oneã€‚</description><guid isPermaLink="true">https://winnerineast.github.io/post/wu-da-zhu-liu-Agentic-kuang-jia-you-lie-dui-bi-fen-xi.html</guid><pubDate>Sat, 12 Jul 2025 12:05:31 +0000</pubDate></item><item><title>ä¸€æ¬¾PDFè½¬JSON,Markdownçš„å·¥å…·ï¼šDocling</title><link>https://winnerineast.github.io/post/yi-kuan-PDF-zhuan-JSON%2CMarkdown-de-gong-ju-%EF%BC%9ADocling.html</link><description>æ”¯æŒè¯¦ç»†é¡µé¢å¸ƒå±€å’Œé˜…è¯»é¡ºåºç†è§£ã€è¡¨æ ¼ç»“æ„æ¢å¤ï¼Œå…ƒæ•°æ®æå–ï¼Œæ”¯æŒOCRåŠŸèƒ½ï¼Œå¯ç”¨äºæ‰«æçš„PDF 

githubï¼š[https://github.com/DS4SD/docling](https://t.co/AOYqyN8JN5)ã€‚</description><guid isPermaLink="true">https://winnerineast.github.io/post/yi-kuan-PDF-zhuan-JSON%2CMarkdown-de-gong-ju-%EF%BC%9ADocling.html</guid><pubDate>Sat, 12 Jul 2025 12:05:05 +0000</pubDate></item><item><title>Reliable Agentic RAG with LLM Trustworthiness Estimates</title><link>https://winnerineast.github.io/post/Reliable%20Agentic%20RAG%20with%20LLM%20Trustworthiness%20Estimates.html</link><description>September 12, 2024

- ![Chris Mauck](https://cleanlab.ai/_next/static/images/chris-2be9b6d6c9460c74034608339ff17c6d.jpg)Chris Mauck
- ![Jonas Mueller](https://cleanlab.ai/_next/static/images/jonas-038f0156ce880eb9cff38bb91618ea1b.jpg)Jonas Mueller

This article demonstrates an agentic system to ensure reliable answers in Retrieval-Augmented Generation, while also ensuring thatÂ **latency and compute costs do not exceed the processing needed to accurately respond to complex queries**. Our system relies onÂ _trustworthiness scores_Â for LLM outputs, in order to dynamically adjust retrieval strategies until sufficient context has been retrieved to generate a trustworthy RAG answer.

![Diagram of Agentic RAG with trustworthiness scores](https://cleanlab.ai/_next/static/images/RAG_diagram-cb17a3bb114e546c10eb3e8ddc68a415.png)

Based on the trustworthiness score for a candidate response, the RAG Agent can choose more complex retrieval plans or approve the response for production.

## Introduction

Retrieval-Augmented Generation (RAG) combines the strengths of large language models (LLMs) with powerful retrieval systems to generate more accurate responses grounded in knowledge databases. Simple RAG systems retrieve relevant information to a query via semantic search based on vector embeddings of query and database contents, but this strategy fails for more complex queries.

_Agentic RAG_Â considers various Retrieval strategies as tools available to an LLM orchestrator that can iteratively decide which tools to call next based on what itâ€™s seen thus far. This Agent can plan, execute, and refine multi-step retrieval processes, but it is critical to ensure latency and compute costs do not exceed what is required to produce a good answer for a userâ€™s query. Despite advancements from LLMs â†’ RAG â†’ Agentic RAG with sophisticated Retrieval strategies, AI-generated responses still suffer from hallucinations today, producing incorrect or nonsensical information with unwarranted confidence.

This blog outlines an Agentic RAG system that can produce trustworthy answers even for complex queries, in a manner that keeps latency/costs in check. Our system relies on theÂ [Trustworthy Language Model](https://cleanlab.ai/blog/trustworthy-language-model/)Â to score the trustworthiness of a candidate response (based on the query and currently retrieved context). When the current response is deemed untrustworthy, the Agent is tasked with orchestrating a better Retrieval strategy to improve the context. This system starts with cheaper Retrieval strategies, and dynamically tries strategies with greater runtime/costsÂ **only for complex queries where they are necessary to produce a trustworthy RAG answer**.

## Trustworthy Language Model (TLM)

For a given user query, the RAG system will retrieve relevant context, which is then fed into a LLM to produce the response.Â _But how do we know when the response is untrustworthy_? For instance, here is question incorrectly answered by ChatGPT with no indication it should not be trusted.

![ChatGPT giving an incorrect answer](https://cleanlab.ai/_next/static/images/gpt-367eda0e9cc624b7c3497ea417db378b.png)

TLM automates this determination, by producing a trustworthiness score (between 0-1) for responses from any LLM. For the above prompt &amp; ChatGPT response:

```text
tlm.get_trustworthiness_score(prompt, response) = 0.413
```

indicating this response should not be trusted.

These scoresÂ [have been found](https://cleanlab.ai/blog/trustworthy-language-model/)Â to detect hallucinations and LLM errors with greater precision/recall than alternative approaches like asking the LLM to evaluate its own output or relying on token-probabilities. TLM internally estimates aleatoric and epistemic uncertainty of the LLM by combining techniques including self-reflection, probabilistic prediction, and observed consistency. You canÂ [present](https://help.cleanlab.ai/tutorials/tlm_rag/)Â TLM trustworthiness scores to users of your RAG system to automatically help them understand which responses warrant further scrutiny.

## Utilizing the trustworthiness score in Agentic RAG

A userâ€™s query is processed by our RAG system as follows: the Retrieval Planner Agent orchestrates a series of Retrieval strategies in order to discover relevant context, that when appended to the query, leads to an LLM response with sufficiently high trustworthiness score. The Agent is encouraged to start with faster/cheaper Retrieval strategies and only escalate to more complex Retrieval when a high trustworthiness score has not yet been achieved. As soon as a trustworthy LLM response is produced, it is returned to the user.

This high-level system can work with many types of Retrieval Planner Agent implementations (e.g. via frameworks like LangGraph and tool-use algorithms like OpenAI Function Calling), as well as all sorts of Retrieval strategies. The goal is to minimize the runtime and costs required to process most queries, while still being able to produce trustworthy responses for complex queries that necessitate more compute.

### Potential Retrieval Strategies

As a concrete example, our Retrieval Planner Agent might choose from the following Retrieval strategies, increasing in time and compute complexity:

1. No Retrieval

- Complexity:Â _None_
- The query is answerable with general knowledge the LLM already knows.

2. Semantic Search (vector embedding similarity)

- Complexity:Â _Low_
- Vector database (Pinecone, Qdrant, Weaviate, etc.) is searched using top similarities in space of embeddings (Sentence Transformers, Voyage, etc.)

3. [Hybrid Search (vector + keyword search) with Reciprocal Rank Fusion](https://www.assembled.com/blog/better-rag-results-with-reciprocal-rank-fusion-and-hybrid-search)

- Complexity:Â _Low/Medium_
- Knowledge database is searched via a combination of vector similarity and classical keyword search like BM25, with results rankings from different searches aggregated via the RRF method.

4. [Re-Ranking](https://adasci.org/a-hands-on-guide-to-enhance-rag-with-re-ranking/)Â retrieved results

- Complexity:Â _Medium_
- A specialized re-ranker model is applied to the retrieved results from either vector or keyword search that more accurately estimates which ones are relevant to the query.

5. [Query Expansion](https://docs.llamaindex.ai/en/stable/examples/query_transformations/query_transform_cookbook/)

- Complexity:Â _Medium/High_
- User query is rewritten into possibly multiple queries before (possibly multiple steps of) Retrieval. This includes entity recognition, separate keyword searches, and methods likeÂ [Hypothetical Document Embeddings (Hyde)](https://arxiv.org/abs/2212.10496)Â orÂ [Step-Back Prompting](https://arxiv.org/abs/2310.06117).

6. Chunk/Document Expansion ([Multi-Hop RAG](https://cobusgreyling.medium.com/multihop-rag-1c695794eeda),Â [GraphRAG](https://neo4j.com/blog/graphrag-manifesto/))

- Complexity:Â _Medium/High_
- Returned chunks from the vector database search are expanded by referring to the original document from which they came and traversing related documents or Knowledge Graphs like Neo4j. This may be required to find additional information that is needed for the retrieved context to be useful.

## Examples of our Trustworthy Agentic RAG in action

To make things more concrete, we consider a RAG application intended to answer questions based on Nvidiaâ€™s product documentation.

### Simple Query

Letâ€™s first consider a simple query that a user may pose:

&gt; **Query:**Â _Which component of a computer is responsible for graphics rendering?_

On the first pass, our RAG Agent chooses the least complex retrieval plan: do not retrieve anything. The response and associated trustworthiness score are:

&gt; **Response:**Â The component of a computer responsible for graphics rendering is the Graphics Processing Unit (GPU).
&gt; 
&gt; **Trustworthiness Score:**Â 0.986

In this case, the RAG Agent determines the response to be trustworthy and approves the response to be used downstream.

### Complex Query

Now for a more complex query:

&gt; **Query:**Â _How many NVIDIA CUDA Cores does the RTX 4090 have?_

Our RAG Agent might again initially choose to not retrieve anything, yielding:

&gt; **Response:**Â As of my last update in October 2022, NVIDIA has not released an RTX 4090 graphics card. Therefore, there are no official specifications.
&gt; 
&gt; **Trustworthiness Score:**Â 0.725

It appears that the LLM used does not have access to this knowledge. This is quantified by the trustworthiness score, enabling our system to determine this is not a reliable response and then to automatically employ a more complex retrieval plan: vector embedding search. On the second pass, the context, response, and trustworthiness score are:

&gt; **Retrieved Context:**
&gt; 
&gt; GeForce RTX 4090 GPU Engine Specs: NVIDIA CUDA Cores 16384 Shader Cores Ada Lovelace 83 TFLOPS Ray Tracing Cores 3rd Generation 191 TFLOPS Tensor Cores (AI) 4th Generation 1321 AI TOPS Boost Clock (GHz) 2.52 Base Clock (GHz) 2.23
&gt; 
&gt; **Response:**Â The NVIDIA GeForce RTX 4090 features 16,384 CUDA cores.
&gt; 
&gt; **Trustworthiness Score:**Â 0.988

Nice! The RAG Agent was able to dynamically up the complexity of the retrieval plan and now has a satisfactory answer with a high trustworthiness score.

### More Complex Query

Letâ€™s consider a more complex query:

&gt; **Query:**Â _What were the key design challenges faced by NVIDIA in developing the RTX 4090, and how were they overcome?_

Our RAG Agent might again initially chooses to not retrieve anything, yielding:

&gt; **Response:**Â The NVIDIA RTX 4090 was developed to push the boundaries of GPU performance, requiring significant advancements in architecture, thermal management, and power efficiency.
&gt; 
&gt; **Trustworthiness Score:**Â 0.635

TLM identifies this response is generic and not confidently containing the right information. In the next pass, our RAG Agent might opt for vector similarity search again, yielding:

&gt; **Retrieved Context:**Â NVIDIAâ€™s RTX 4090 was anticipated to be a major leap in GPU technology. There were discussions about improving overall architecture and enhancing ray tracing capabilities. However, specific details about the challenges during its development are scarce. Some sources suggest that NVIDIA focused on maximizing performance for gaming and AI applications, but there is little concrete information on the exact hurdles faced.
&gt; 
&gt; **Response:**Â The NVIDIA RTX 4090 focused on improving performance and ray tracing capabilities, but specific details about the challenges encountered during development are not well-documented. It is believed that NVIDIA prioritized enhancements in gaming and AI, but the exact difficulties they faced remain unclear.
&gt; 
&gt; **Trustworthiness Score:**Â 0.805

In this pass, our RAG Agent retrieved unsatisfactory information. The corresponding moderate trustworthiness score reflects the lack of specific context. Our RAG Agent thus considers another Retrieval pass with a more complex Hybrid search strategy, yielding:

&gt; **Retrieved Context:**Â The RTX 4090â€™s development saw NVIDIA addressing several engineering hurdles. The thermal challenges were exacerbated by the sheer density of the CUDA cores, necessitating an innovative cooling solution. NVIDIA introduced a new fan design and a larger vapor chamber to dissipate heat more effectively. On the power side, the 4090 required a new power connector standard, the PCIe Gen 5 16-pin, which allowed for higher wattage delivery, ensuring stable operation under load. For AI integration, NVIDIA refined its Tensor Core technology to balance power and performance, ensuring that the 4090 could handle advanced AI workloads without bottlenecking other processes.
&gt; 
&gt; **Response:**Â NVIDIAâ€™s RTX 4090 development involved overcoming several key challenges, particularly around thermal management and power delivery. The GPUâ€™s high density of 16,384 CUDA cores required an innovative cooling system, leading to the introduction of a new fan design and larger vapor chamber. Additionally, the need for higher power delivery led to the adoption of the PCIe Gen 5 16-pin connector. To integrate AI features effectively, NVIDIA refined its Tensor Core technology, balancing power and performance to handle advanced AI workloads efficiently.
&gt; 
&gt; **Trustworthiness Score:**Â 0.975

With the expanded context, the RAG Agent produces a highly detailed and specific response that earns a high enough trustworthiness score to return to the user. That said, this higher quality answer did require additional time/costs to produce.

The same process continues for queries with greater levels of complexity. Our RAG Agent continues escalating Retrieval strategies until the trustworthiness score indicates a reliable response, or the maximum time limit is reached. If no Retrieval strategy yields a trustworthy LLM response, then our system responds that it cannot handle this question without additional clarifications/information.

## Wrap Up

Integrating the Trustworthy Language Model enables Agentic RAG systems that can ensure accurate answers to complex queries while bounding latency/costs for regular queries. You can adopt this approach to navigate the delicate balance between speed, cost, and accuracy across diverse RAG applicationsâ€”from customer service to specialized fields like finance, law, and medicine.

While traditional RAG systems generate responses of unknown quality based on predefined steps to process every query,Â **the future of AI lies in systems that assess response trustworthiness and adapt processing plans to each queryâ€™s complexity**. Agentic RAG with the TLM offers a promising step toward this future ofÂ _reliable_Â AI.

## Next Steps

1. Get started with theÂ [TLM tutorials](https://help.cleanlab.ai/tutorials/tlm/).
2. Try it instantly via theÂ [TLM Playground](https://tlm.cleanlab.ai/).
3. ReadÂ [benchmarks](https://cleanlab.ai/blog/trustworthy-language-model/)Â measuring the effectiveness of LLM trustworthiness scores.ã€‚</description><guid isPermaLink="true">https://winnerineast.github.io/post/Reliable%20Agentic%20RAG%20with%20LLM%20Trustworthiness%20Estimates.html</guid><pubDate>Sat, 12 Jul 2025 12:04:23 +0000</pubDate></item><item><title>RAG å®æˆ˜åˆ†äº«  - 50-60GB PDF æ–‡ä»¶, 6000 ç¯‡åŒ»å­¦è®ºæ–‡, èƒ½åš RAG å—</title><link>https://winnerineast.github.io/post/RAG%20-shi-zhan-fen-xiang-%20%20-%2050-60GB%20PDF%20-wen-jian-%2C%206000%20-pian-yi-xue-lun-wen-%2C%20-neng-zuo-%20RAG%20-ma.html</link><description>ä½œè€… Scott æ˜¯å¾®è½¯å¼€å‘è€…ç¤¾åŒº VP, è¿™åº”è¯¥ä¹Ÿæ˜¯ä»–åœ¨å¼€å‘è€…ä¸­é‡åˆ°çš„å®é™…é—®é¢˜, è¿™ä¸ªé—®é¢˜çš„è®¨è®ºéå¸¸æ¿€çƒˆã€‚</description><guid isPermaLink="true">https://winnerineast.github.io/post/RAG%20-shi-zhan-fen-xiang-%20%20-%2050-60GB%20PDF%20-wen-jian-%2C%206000%20-pian-yi-xue-lun-wen-%2C%20-neng-zuo-%20RAG%20-ma.html</guid><pubDate>Sat, 12 Jul 2025 12:03:42 +0000</pubDate></item><item><title>Optimizing ColPali for Retrieval at Scale, 13x Faster Results</title><link>https://winnerineast.github.io/post/Optimizing%20ColPali%20for%20Retrieval%20at%20Scale%2C%2013x%20Faster%20Results.html</link><description>
ColPali is a fascinating leap in document retrieval. Its precision in handling visually rich PDFs is phenomenal, but scaling it to handle real-world datasets comes with its share of computational challenges.

Hereâ€™s how we solved these challenges to make ColPali 13x faster without sacrificing the precision itâ€™s known for.

## [](https://qdrant.tech/blog/colpali-qdrant-optimization/#the-scaling-dilemma)The Scaling Dilemma

ColPali generatesÂ **1,030 vectors for just one page of a PDF.**Â While this is manageable for small-scale tasks, in a real-world production setting where you may need to store hundreds od thousands of PDFs, the challenge of scaling becomes significant.

Consider this scenario:

- **Dataset Size:**Â 20,000 PDF pages.
- **Number of Vectors:**Â Each page generates ~1,000 vectors of 128 dimensions.

The total number of comparisons is calculated as:

1,000â‹…1,000â‹…20,000â‹…128=2.56Ã—1012Â comparisons!

Thatâ€™s trillions of comparisons needed to build the index. Even advanced indexing algorithms likeÂ **HNSW**Â struggle with this scale, as computational costs grow quadratically with amount of multivectors per page.

We turned to a hybrid optimization strategy combiningÂ **pooling**Â (to reduce computational overhead) andÂ **reranking**Â (to preserve accuracy).

Before we go any deeper, watch ourÂ [Webinar video](https://www.youtube.com/live/_h6SN1WwnLs?si=n8gwiIjJ5dnfucXC)Â for the full demo walkthrough.

For those eager to explore, theÂ [codebase is available here](https://github.com/qdrant/demo-colpali-optimized).

## [](https://qdrant.tech/blog/colpali-qdrant-optimization/#two-stage-retrieval-process)Two-Stage Retrieval Process

### [](https://qdrant.tech/blog/colpali-qdrant-optimization/#pooling)Pooling

Pooling is well-known in machine learning as a way to compress data while keeping important information. For ColPali, we reduced 1,030 vectors per page to just 38 vectors by pooling rows in the documentâ€™s 32x32 grid.

![](https://qdrant.tech/blog/colpali-optimization/rows.png)

Max and mean pooling are the two most popular types, so we decided to test both approaches on the rows of the grid. Likewise, we could apply pooling on columns, which we plan to explore in the future.

- **Mean Pooling:**Â Averages values across rows.
- **Max Pooling:**Â Selects the maximum value for each feature.

32 vectors represent the pooled rows, while 6 vectors encode contextual information derived from ColPaliâ€™s special tokens (e.g.,Â for the beginning of the sequence, and task-specific instructions like â€œDescribe the imageâ€).

For our experiments, we chose to preserve these 6 additional vectors.

### [](https://qdrant.tech/blog/colpali-qdrant-optimization/#the-colpali-as-a-reranker-experiment)The â€œColPali as a Rerankerâ€ Experiment

Pooling drastically reduces retrieval costs, but thereâ€™s a risk of losing fine-grained precision. To address this, we implemented aÂ **two-stage retrieval system**, where embeddings generated with ColPali were max/mean pooled by grid rows to create lightweight vectors for the initial retrieval stage, followed by reranking with the original high-resolution embeddings:

1. **Pooled Retrieval:**Â Quickly retrieves the top 200 candidates using lightweight pooled embeddings.
2. **Full Reranking:**Â Refines these candidates using the original, high-resolution embeddings, delivering the final top 20 results.

### [](https://qdrant.tech/blog/colpali-qdrant-optimization/#implementation)Implementation

We created a custom dataset with over 20,000 unique PDF pages by merging:

- **ViDoRe Benchmark:**Â Designed for PDF documents retrieval evaluation.
- **UFO Dataset:**Â Visually rich documents paired with synthetic queriesÂ [generated by Daniel van Strien](https://huggingface.co/datasets/davanstrien/ufo-ColPali).
- **DocVQA Dataset:**Â A large set of document-derived Q&amp;A pairs.

Each document was processed into 32x32 grids, generating both full-resolution and pooled embeddings.Â **Full-resolution**Â embeddings consisted of 1,030 vectors per page, whileÂ **pooled embeddings**Â included mean and max pooling variants.

All embeddings were were stored and kept in RAM to avoid caching effects during retrieval speed experiments.

### [](https://qdrant.tech/blog/colpali-qdrant-optimization/#experiment-setup)Experiment Setup

We evaluated retrieval quality with 1,000 queries. First, pooled embeddings retrieved the top 200 candidates. Then, full-resolution embeddings reranked them to produce the final top 20 results.

To measure performance, we used:

- **NDCG@20:**Â Measures ranking quality (how well the top results align with expectations).
- **Recall@20:**Â Measures the overlap between this method and the original ColPali retrieval.

## [](https://qdrant.tech/blog/colpali-qdrant-optimization/#results)Results

The experiment showed promising improvements in speed and accuracy. Retrieval time improvedÂ **13x**Â compared to using full-resolution embeddings alone.

### [](https://qdrant.tech/blog/colpali-qdrant-optimization/#metrics)Metrics

|Pooling Type|NDCG@20|Recall@20|
|---|---|---|
|**Mean**|0.952|0.917|
|**Max**|0.759|0.656|

Mean pooling preserved nearly identical quality to the original ColPali, with NDCG@20 = 0.952 and Recall@20 = 0.917. Max pooling did not perform well enough to be considered viable since it sacrificed significant accuracy without delivering a meaningful speed advantage.

## [](https://qdrant.tech/blog/colpali-qdrant-optimization/#whats-next)Whatâ€™s Next?

Future experiments could push these results even further:

- Investigating column-wise pooling for additional compression.
- Testing half-precision (float16) vectors to balance memory use and speed.
- Skipping special multivectors during prefetch to streamline retrieval.
- Combining quantization with oversampling for even faster search.

### [](https://qdrant.tech/blog/colpali-qdrant-optimization/#try-it-yourself)Try It Yourself

Curious to see this in action? Explore the full codebase and experiment with ColPali optimizations:

- **Demo Notebook:**Â [GitHub Repository](https://github.com/qdrant/demo-colpali-optimized)
- **Webinar Walkthrough:**Â [Watch Here](https://www.youtube.com/live/_h6SN1WwnLs?si=n8gwiIjJ5dnfucXC)ã€‚</description><guid isPermaLink="true">https://winnerineast.github.io/post/Optimizing%20ColPali%20for%20Retrieval%20at%20Scale%2C%2013x%20Faster%20Results.html</guid><pubDate>Sat, 12 Jul 2025 12:03:14 +0000</pubDate></item><item><title>MUVERA-è®©å¤šå‘é‡æ£€ç´¢ä¸å•å‘é‡æœç´¢ä¸€æ ·å¿«</title><link>https://winnerineast.github.io/post/MUVERA--rang-duo-xiang-liang-jian-suo-yu-dan-xiang-liang-sou-suo-yi-yang-kuai.html</link><description>åšRAGçš„æœ‹å‹ä¸€å®šè¦çœ‹çœ‹ Google è¿™ä¸ªæ–°è®ºæ–‡â€”â€”MUVERAï¼šè®©å¤šå‘é‡æ£€ç´¢ä¸å•å‘é‡æœç´¢ä¸€æ ·å¿« å¤§å®¶åœ¨RAGçš„æ£€ç´¢å†…å®¹è¿‡ç¨‹éƒ½ä¼šé‡åˆ°è¿™ç§æƒ…å†µï¼Œå¦‚æœç”¨ä¼ ç»Ÿæœç´¢ï¼ˆä¾‹å¦‚ElasticSearchï¼‰ï¼Œæ–‡æ¡£ = 1 ä¸ªå‘é‡ â†’ å¿«é€Ÿä½†ä¸å‡†ç¡®ã€‚</description><guid isPermaLink="true">https://winnerineast.github.io/post/MUVERA--rang-duo-xiang-liang-jian-suo-yu-dan-xiang-liang-sou-suo-yi-yang-kuai.html</guid><pubDate>Sat, 12 Jul 2025 12:02:17 +0000</pubDate></item><item><title>Deep Research,  WebDancer,  WebSailor</title><link>https://winnerineast.github.io/post/Deep%20Research%2C%20%20WebDancer%2C%20%20WebSailor.html</link><description>
- é—®ï¼šâ€œä¸€éƒ¨çŸ¥åç”µè§†å‰§ï¼šå¥³äºŒ 1993 å¹´å…¥è¡Œï¼›å¥³ä¸€ç°ä»»ä¸ˆå¤«æ˜¯æµ™æ±Ÿæ¹–å·äººï¼›ç”·ä¸€å…­å¹´åç™»ä¸Šæ˜¥æ™šã€‚</description><guid isPermaLink="true">https://winnerineast.github.io/post/Deep%20Research%2C%20%20WebDancer%2C%20%20WebSailor.html</guid><pubDate>Sat, 12 Jul 2025 12:01:43 +0000</pubDate></item><item><title>Building effective agents</title><link>https://winnerineast.github.io/post/Building%20effective%20agents.html</link><description>Over the past year, we've worked with dozens of teams building large language model (LLM) agents across industries. Consistently, the most successful implementations weren't using complex frameworks or specialized libraries. Instead, they were building with simple, composable patterns.

In this post, we share what weâ€™ve learned from working with our customers and building agents ourselves, and give practical advice for developers on building effective agents.

## What are agents?

'Agent' can be defined in several ways. Some customers define agents as fully autonomous systems that operate independently over extended periods, using various tools to accomplish complex tasks. Others use the term to describe more prescriptive implementations that follow predefined workflows. At Anthropic, we categorize all these variations as **agentic systems**, but draw an important architectural distinction between **workflows** and **agents**:

- **Workflows** are systems where LLMs and tools are orchestrated through predefined code paths.
- **Agents**, on the other hand, are systems where LLMs dynamically direct their own processes and tool usage, maintaining control over how they accomplish tasks.

Below, we will explore both types of agentic systems in detail. In Appendix 1 (â€œAgents in Practiceâ€), we describe two domains where customers have found particular value in using these kinds of systems.

## When (and when not) to use agents

When building applications with LLMs, we recommend finding the simplest solution possible, and only increasing complexity when needed. This might mean not building agentic systems at all. Agentic systems often trade latency and cost for better task performance, and you should consider when this tradeoff makes sense.

When more complexity is warranted, workflows offer predictability and consistency for well-defined tasks, whereas agents are the better option when flexibility and model-driven decision-making are needed at scale. For many applications, however, optimizing single LLM calls with retrieval and in-context examples is usually enough.

## When and how to use frameworks

There are many frameworks that make agentic systems easier to implement, including:

- [LangGraph](https://langchain-ai.github.io/langgraph/) from LangChain;
- Amazon Bedrock's [AI Agent framework](https://aws.amazon.com/bedrock/agents/);
- [Rivet](https://rivet.ironcladapp.com/), a drag and drop GUI LLM workflow builder; and
- [Vellum](https://www.vellum.ai/), another GUI tool for building and testing complex workflows.

These frameworks make it easy to get started by simplifying standard low-level tasks like calling LLMs, defining and parsing tools, and chaining calls together. However, they often create extra layers of abstraction that can obscure the underlying prompts â€‹â€‹and responses, making them harder to debug. They can also make it tempting to add complexity when a simpler setup would suffice.

We suggest that developers start by using LLM APIs directly: many patterns can be implemented in a few lines of code. If you do use a framework, ensure you understand the underlying code. Incorrect assumptions about what's under the hood are a common source of customer error.

See our [cookbook](https://github.com/anthropics/anthropic-cookbook/tree/main/patterns/agents) for some sample implementations.

## Building blocks, workflows, and agents

In this section, weâ€™ll explore the common patterns for agentic systems weâ€™ve seen in production. We'll start with our foundational building blockâ€”the augmented LLMâ€”and progressively increase complexity, from simple compositional workflows to autonomous agents.

### Building block: The augmented LLM

The basic building block of agentic systems is an LLM enhanced with augmentations such as retrieval, tools, and memory. Our current models can actively use these capabilitiesâ€”generating their own search queries, selecting appropriate tools, and determining what information to retain.

We recommend focusing on two key aspects of the implementation: tailoring these capabilities to your specific use case and ensuring they provide an easy, well-documented interface for your LLM. While there are many ways to implement these augmentations, one approach is through our recently released [Model Context Protocol](https://www.anthropic.com/news/model-context-protocol), which allows developers to integrate with a growing ecosystem of third-party tools with a simple [client implementation](https://modelcontextprotocol.io/tutorials/building-a-client#building-mcp-clients).

For the remainder of this post, we'll assume each LLM call has access to these augmented capabilities.

### Workflow: Prompt chaining

Prompt chaining decomposes a task into a sequence of steps, where each LLM call processes the output of the previous one. You can add programmatic checks (see 'gateâ€ in the diagram below) on any intermediate steps to ensure that the process is still on track.

**When to use this workflow:** This workflow is ideal for situations where the task can be easily and cleanly decomposed into fixed subtasks. The main goal is to trade off latency for higher accuracy, by making each LLM call an easier task.

**Examples where prompt chaining is useful:**

- Generating Marketing copy, then translating it into a different language.
- Writing an outline of a document, checking that the outline meets certain criteria, then writing the document based on the outline.

### Workflow: Routing

Routing classifies an input and directs it to a specialized followup task. This workflow allows for separation of concerns, and building more specialized prompts. Without this workflow, optimizing for one kind of input can hurt performance on other inputs.

**When to use this workflow:** Routing works well for complex tasks where there are distinct categories that are better handled separately, and where classification can be handled accurately, either by an LLM or a more traditional classification model/algorithm.

**Examples where routing is useful:**

- Directing different types of customer service queries (general questions, refund requests, technical support) into different downstream processes, prompts, and tools.
- Routing easy/common questions to smaller models like Claude 3.5 Haiku and hard/unusual questions to more capable models like Claude 3.5 Sonnet to optimize cost and speed.

### Workflow: Parallelization

LLMs can sometimes work simultaneously on a task and have their outputs aggregated programmatically. This workflow, parallelization, manifests in two key variations:

- **Sectioning**: Breaking a task into independent subtasks run in parallel.
- **Voting:** Running the same task multiple times to get diverse outputs.

**When to use this workflow:** Parallelization is effective when the divided subtasks can be parallelized for speed, or when multiple perspectives or attempts are needed for higher confidence results. For complex tasks with multiple considerations, LLMs generally perform better when each consideration is handled by a separate LLM call, allowing focused attention on each specific aspect.

**Examples where parallelization is useful:**

- **Sectioning**:
- Implementing guardrails where one model instance processes user queries while another screens them for inappropriate content or requests. This tends to perform better than having the same LLM call handle both guardrails and the core response.
- Automating evals for evaluating LLM performance, where each LLM call evaluates a different aspect of the modelâ€™s performance on a given prompt.
- **Voting**:
- Reviewing a piece of code for vulnerabilities, where several different prompts review and flag the code if they find a problem.
- Evaluating whether a given piece of content is inappropriate, with multiple prompts evaluating different aspects or requiring different vote thresholds to balance false positives and negatives.

### Workflow: Orchestrator-workers

In the orchestrator-workers workflow, a central LLM dynamically breaks down tasks, delegates them to worker LLMs, and synthesizes their results.

**When to use this workflow:** This workflow is well-suited for complex tasks where you canâ€™t predict the subtasks needed (in coding, for example, the number of files that need to be changed and the nature of the change in each file likely depend on the task). Whereas itâ€™s topographically similar, the key difference from parallelization is its flexibilityâ€”subtasks aren't pre-defined, but determined by the orchestrator based on the specific input.

**Example where orchestrator-workers is useful:**

- Coding products that make complex changes to multiple files each time.
- Search tasks that involve gathering and analyzing information from multiple sources for possible relevant information.

### Workflow: Evaluator-optimizer

In the evaluator-optimizer workflow, one LLM call generates a response while another provides evaluation and feedback in a loop.

**When to use this workflow:** This workflow is particularly effective when we have clear evaluation criteria, and when iterative refinement provides measurable value. The two signs of good fit are, first, that LLM responses can be demonstrably improved when a human articulates their feedback; and second, that the LLM can provide such feedback. This is analogous to the iterative writing process a human writer might go through when producing a polished document.

**Examples where evaluator-optimizer is useful:**

- Literary translation where there are nuances that the translator LLM might not capture initially, but where an evaluator LLM can provide useful critiques.
- Complex search tasks that require multiple rounds of searching and analysis to gather comprehensive information, where the evaluator decides whether further searches are warranted.

### Agents

Agents are emerging in production as LLMs mature in key capabilitiesâ€”understanding complex inputs, engaging in reasoning and planning, using tools reliably, and recovering from errors. Agents begin their work with either a command from, or interactive discussion with, the human user. Once the task is clear, agents plan and operate independently, potentially returning to the human for further information or judgement. During execution, it's crucial for the agents to gain â€œground truthâ€ from the environment at each step (such as tool call results or code execution) to assess its progress. Agents can then pause for human feedback at checkpoints or when encountering blockers. The task often terminates upon completion, but itâ€™s also common to include stopping conditions (such as a maximum number of iterations) to maintain control.

Agents can handle sophisticated tasks, but their implementation is often straightforward. They are typically just LLMs using tools based on environmental feedback in a loop. It is therefore crucial to design toolsets and their documentation clearly and thoughtfully. We expand on best practices for tool development in Appendix 2 ('Prompt Engineering your Tools').

**When to use agents:** Agents can be used for open-ended problems where itâ€™s difficult or impossible to predict the required number of steps, and where you canâ€™t hardcode a fixed path. The LLM will potentially operate for many turns, and you must have some level of trust in its decision-making. Agents' autonomy makes them ideal for scaling tasks in trusted environments.

The autonomous nature of agents means higher costs, and the potential for compounding errors. We recommend extensive testing in sandboxed environments, along with the appropriate guardrails.

**Examples where agents are useful:**

The following examples are from our own implementations:

- A coding Agent to resolve [SWE-bench tasks](https://www.anthropic.com/research/swe-bench-sonnet), which involve edits to many files based on a task description;
- Our [â€œcomputer useâ€ reference implementation](https://github.com/anthropics/anthropic-quickstarts/tree/main/computer-use-demo), where Claude uses a computer to accomplish tasks.

## Combining and customizing these patterns

These building blocks aren't prescriptive. They're common patterns that developers can shape and combine to fit different use cases. The key to success, as with any LLM features, is measuring performance and iterating on implementations. To repeat: you should consider adding complexity *only* when it demonstrably improves outcomes.

## Summary

Success in the LLM space isn't about building the most sophisticated system. It's about building the *right* system for your needs. Start with simple prompts, optimize them with comprehensive evaluation, and add multi-step agentic systems only when simpler solutions fall short.

When implementing agents, we try to follow three core principles:

1. Maintain **simplicity** in your agent's design.
2. Prioritize **transparency** by explicitly showing the agentâ€™s planning steps.
3. Carefully craft your agent-computer interface (ACI) through thorough tool **documentation and testing**.

Frameworks can help you get started quickly, but don't hesitate to reduce abstraction layers and build with basic components as you move to production. By following these principles, you can create agents that are not only powerful but also reliable, maintainable, and trusted by their users.

### Acknowledgements

Written by Erik Schluntz and Barry Zhang. This work draws upon our experiences building agents at Anthropic and the valuable insights shared by our customers, for which we're deeply grateful.

## Appendix 1: Agents in practice

Our work with customers has revealed two particularly promising applications for AI agents that demonstrate the practical value of the patterns discussed above. Both applications illustrate how agents add the most value for tasks that require both conversation and action, have clear success criteria, enable feedback loops, and integrate meaningful human oversight.

### A. Customer support

Customer support combines familiar chatbot interfaces with enhanced capabilities through tool integration. This is a natural fit for more open-ended agents because:

- Support interactions naturally follow a conversation flow while requiring access to external information and actions;
- Tools can be integrated to pull customer data, order history, and knowledge base articles;
- Actions such as issuing refunds or updating tickets can be handled programmatically; and
- Success can be clearly measured through user-defined resolutions.

Several companies have demonstrated the viability of this approach through usage-based pricing models that charge only for successful resolutions, showing confidence in their agents' effectiveness.

### B. Coding agents

The software development space has shown remarkable potential for LLM features, with capabilities evolving from code completion to autonomous problem-solving. Agents are particularly effective because:

- Code solutions are verifiable through automated tests;
- Agents can iterate on solutions using test results as feedback;
- The problem space is well-defined and structured; and
- Output quality can be measured objectively.

In our own implementation, agents can now solve real GitHub issues in the [SWE-bench Verified](https://www.anthropic.com/research/swe-bench-sonnet) benchmark based on the pull request description alone. However, whereas automated testing helps verify functionality, human review remains crucial for ensuring solutions align with broader system requirements.

## Appendix 2: Prompt engineering your tools

No matter which agentic system you're building, tools will likely be an important part of your agent. [Tools](https://www.anthropic.com/news/tool-use-ga) enable Claude to interact with external services and APIs by specifying their exact structure and definition in our API. When Claude responds, it will include a [tool use block](https://docs.anthropic.com/en/docs/build-with-claude/tool-use#example-api-response-with-a-tool-use-content-block) in the API response if it plans to invoke a tool. Tool definitions and specifications should be given just as much prompt engineering attention as your overall prompts. In this brief appendix, we describe how to prompt engineer your tools.

There are often several ways to specify the same action. For instance, you can specify a file edit by writing a diff, or by rewriting the entire file. For structured output, you can return code inside markdown or inside JSON. In software engineering, differences like these are cosmetic and can be converted losslessly from one to the other. However, some formats are much more difficult for an LLM to write than others. Writing a diff requires knowing how many lines are changing in the chunk header before the new code is written. Writing code inside JSON (compared to markdown) requires extra escaping of newlines and quotes.

Our suggestions for deciding on tool formats are the following:

- Give the model enough tokens to 'think' before it writes itself into a corner.
- Keep the format close to what the model has seen naturally occurring in text on the internet.
- Make sure there's no formatting 'overhead' such as having to keep an accurate count of thousands of lines of code, or string-escaping any code it writes.

One rule of thumb is to think about how much effort goes into human-computer interfaces (HCI), and plan to invest just as much effort in creating good *agent*\-computer interfaces (ACI). Here are some thoughts on how to do so:

- Put yourself in the model's shoes. Is it obvious how to use this tool, based on the description and parameters, or would you need to think carefully about it? If so, then itâ€™s probably also true for the model. A good tool definition often includes example usage, edge cases, input format requirements, and clear boundaries from other tools.
- How can you change parameter names or descriptions to make things more obvious? Think of this as writing a great docstring for a junior developer on your team. This is especially important when using many similar tools.
- Test how the model uses your tools: Run many example inputs in our [workbench](https://console.anthropic.com/workbench) to see what mistakes the model makes, and iterate.
- [Poka-yoke](https://en.wikipedia.org/wiki/Poka-yoke) your tools. Change the arguments so that it is harder to make mistakes.

While building our agent for [SWE-bench](https://www.anthropic.com/research/swe-bench-sonnet), we actually spent more time optimizing our tools than the overall prompt. For example, we found that the model would make mistakes with tools using relative filepaths after the agent had moved out of the root directory. To fix this, we changed the tool to always require absolute filepathsâ€”and we found that the model used this method flawlessly.ã€‚</description><guid isPermaLink="true">https://winnerineast.github.io/post/Building%20effective%20agents.html</guid><pubDate>Sat, 12 Jul 2025 12:01:15 +0000</pubDate></item><item><title>AIæ™ºæ…§ä¸æ£€ç´¢çš„å·…å³°èåˆï¼šRAGè§£é”ä¸‹ä¸€ä»£ä¿¡æ¯ç”Ÿæˆæ–°æ¶æ„</title><link>https://winnerineast.github.io/post/AI-zhi-hui-yu-jian-suo-de-dian-feng-rong-he-%EF%BC%9ARAG-jie-suo-xia-yi-dai-xin-xi-sheng-cheng-xin-jia-gou.html</link><description>**GPTDAOCN-e/acc** @GPTDAOCN [2024-12-11](https://x.com/GPTDAOCN/status/1866947666690838557)

AIæ™ºæ…§ä¸æ£€ç´¢çš„å·…å³°èåˆï¼šRAGè§£é”ä¸‹ä¸€ä»£ä¿¡æ¯ç”Ÿæˆæ–°æ¶æ„

è¿™å¼ å›¾å±•ç¤ºäº†'æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRetrieval-Augmented Generationï¼ŒRAGï¼‰'æŠ€æœ¯çš„ä¸åŒæ¶æ„ï¼Œç”¨æ¥æè¿°å¦‚ä½•ç»“åˆä¿¡æ¯æ£€ç´¢å’Œç”Ÿæˆå¼AIæ¨¡å‹ï¼ˆå¦‚GPTï¼‰æ¥æ›´å¥½åœ°å›ç­”é—®é¢˜æˆ–ç”Ÿæˆå†…å®¹ã€‚</description><guid isPermaLink="true">https://winnerineast.github.io/post/AI-zhi-hui-yu-jian-suo-de-dian-feng-rong-he-%EF%BC%9ARAG-jie-suo-xia-yi-dai-xin-xi-sheng-cheng-xin-jia-gou.html</guid><pubDate>Sat, 12 Jul 2025 12:00:46 +0000</pubDate></item><item><title>ã€è®ºæ–‡æ¨èã€‘DeepResearch</title><link>https://winnerineast.github.io/post/%E3%80%90-lun-wen-tui-jian-%E3%80%91DeepResearch.html</link><description>ï¼ˆå¥½ä¸€äº›çš„ç»¼è¿°ï¼‰[https://arxiv.org/abs/2506.18096](https://t.co/KqENoLuAiv) 
ï¼ˆè¿‡äºæ±‚å…¨åè€Œç¼ºä¹æ´è§çš„ç»¼è¿°ï¼‰ [https://arxiv.org/pdf/2506.12594](https://t.co/xeWo94wXZq) 
ï¼ˆè¯„ä¼°æ–¹æ³•ï¼Œä»æŠ¥å‘Šè´¨é‡å’Œå¼•ç”¨ç²¾åº¦ä¸¤ä¸ªè§’åº¦è¯„ä¼°ï¼‰[https://arxiv.org/pdf/2506.11763](https://t.co/vKR0NO4HLO)

ï¼ˆæ¨èï¼Œå¸¦UIï¼Œå­—èŠ‚å¼€æºçš„åŸºäºLangGraphçš„DeepFlowï¼‰[https://github.com/bytedance/deer-flowâ€¦](https://t.co/EtqKMqlKBf) 
ï¼ˆæ¨èï¼Œå¸¦UIï¼ŒGeminiåŸºäºLangGraphçš„Deep Searchå®ç°ï¼Œæ¯”è¾ƒç®€å•ï¼‰ [https://github.com/google-gemini/gemini-fullstack-langgraph-quickstartâ€¦](https://t.co/VTe064Fsx2) 
ï¼ˆæ²¡æœ‰UIï¼Œä½†æ˜¯åŠŸèƒ½æ¯”è¾ƒä¸°å¯Œï¼‰[https://github.com/foreveryh/mentis/tree/main/super_agents/deep_researchâ€¦](https://t.co/9TJy0juGLG)
[](https://t.co/VTe064Fsx2)
ï¼ˆåŸºäºsmolagentsæ¡†æ¶ï¼Œæ¯”è¾ƒç®€å•ï¼‰[https://github.com/huggingface/smolagents/tree/main/examples/open_deep_researchâ€¦](https://t.co/9wXwCsbxeL) ï¼ˆåŸºäºOpenAI Agents åº“å®ç°ï¼Œæ¯”è¾ƒç®€å•ï¼‰ [https://huggingface.co/spaces/mallocode200/Deep_Research_Assistant/blob/main/research_manager.py](https://t.co/BaoHko5WT9)ã€‚</description><guid isPermaLink="true">https://winnerineast.github.io/post/%E3%80%90-lun-wen-tui-jian-%E3%80%91DeepResearch.html</guid><pubDate>Sat, 12 Jul 2025 10:13:59 +0000</pubDate></item></channel></rss>